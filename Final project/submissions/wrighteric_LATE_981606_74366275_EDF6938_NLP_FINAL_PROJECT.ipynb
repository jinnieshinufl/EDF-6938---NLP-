{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3nrRKc0HIEV"
   },
   "source": [
    "#### Final Proposal EDF 6938: Natural Langauge Processing\n",
    "\n",
    "### Do data ‘suggest’ that data can suggest?\n",
    "> #### Author: Eric Wright\n",
    "> #### Date: 12/6/2022\n",
    "> #### Email: eric@ericwright.org\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXmw_S3MHpPW"
   },
   "source": [
    "#### 1. Introduction \n",
    "\n",
    "This is a study of using natural language processing (NLP) to identify the prevalence of a language pattern in quantitative studies in education research journals. This pattern is one that, from my perspective, communicates the idea that data can speak for themselves, an idea that is counter to both new and old theoretical perspectives that supposedly underlie the practices of quantitative research. My goal is to train an algorithm to successfully identify this pattern while generating as few false positives as possible, thus using quantitative methods to examine the writing/communication aspect of quantitative methodology. This pattern is ‘[these data/results/findings/etc.] _suggest_/_imply_/_indicate_/etc. [a conclusion made by the researchers based on the data],’ and up to now, it has been entirely unaddressed in the literature. From certain theoretical perspectives, the answer to the title is simple: data do not suggest that data can suggest simply because data cannot suggest. The real question is whether the language used in published articles can be inferred to mean that data can suggest.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr6LyhpnH1Km"
   },
   "source": [
    "#### 2. Related Work \n",
    "##### 2.1 Theoretical underpinning\n",
    "Recent critical-theory–based approaches to quantitative research have highlighted the idea that data cannot speak for themselves. These approaches include critical data studies [1], QuantCrit [2], and data feminism [3]. While some refer to ‘numbers’ rather than ‘data,’ the common point is that data, findings, numbers, results, and so on do not have any inherent meaning and that it is the authors or researchers who make meaning from those. In other words, a conclusion does not emerge directly from a result. Instead, a conclusion is proposed by a researcher who interprets the result, or the result is evidence that supports a theory.\n",
    "\n",
    "This position is also supported by the quantitative-associated theoretical perspective of _postpositivism_. This is the theoretical perspective on which modern quantitative practices have supposedly been based following a rejection of positivism [4]. And while the idea that data cannot speak for themselves is not always stated in such clear terms in the postpositivist literature, it follows easily from the standpoint that theories are underdetermined by data, that a set of data can never point to a single theory that explains it [5]. Instead, there are many possible theories that could explain any set of observations. So while newer theoretical perspectives have called specific attention to this, the idea itself is not new and is, in theory, very mainstream.\n",
    "\n",
    "This brings me to a question: why then do we write our quantitative articles as if data do speak for themselves? This question was born from my own observations, particularly of the ways researchers have a habit of discussing results, while reading education research articles and reviewing conference proposals as a quantitative methodologist. The most common phrase I have noticed is of the form ‘[these data/results/findings/etc.] _suggest_ [a conclusion made by the researchers based on the data],’ as if the conclusion sprang forth from the data without any thought or extrapolation from the researchers. Almost always, this type of phrase included only one potential conclusion out of the many that different researchers could draw given the same results.\n",
    "\n",
    "The use of the word ‘suggest’ may seem like a small detail, but even if every quantitative researcher simply used this as shorthand with the best intentions, enough repetition may communicate and instill the belief in readers that data can speak for themselves and suggest conclusions, especially for anyone who is not intimately familiar with quantitative methods and the underlying theoretical perspectives. This is highlighted, in a way, by QuantCrit [2]. Gillborn et al. [2] recommend researchers move away from referring to ‘race’ when talking about results related to a categorization of race. This is because a racial designation is not something that is physically inherent to the individual, but calling it ‘race’ and saying that results are due to race or explained by race gives the impression that any negative quantitative effects related to that categorization are based on some individual quality rather than a likely-systemic force, especially in the United States. Therefore, they recommend at least replacing ‘race’ with ‘race/racism.’ Lipton and Steinhardt [6] also call out the extent of anthropomorphization in machine learning as potentially creating a false impression that algorithms do more and are more human-like than in actuality.\n",
    "\n",
    "##### 2.2 Methodologically related work\n",
    "Based on a search of the literature, I was unable to identify any similar NLP studies. However, there is one line of research that address a very similar topic: unsupported causal inferences in quantitative research studies. These are statements in that imply causation from results in studies where the research designs do not support causal inferences from those results. This has been studied in psychology [7], teaching and learning [8], and professional counseling [9] to name a few, and they all found large percentages of unsupported causal inferences. In fact, I was on the research team for the professional counseling study [9]. That study’s focus on the linguistic and communication aspect of methodology, my long experience of hand-coding articles for it, and my informally-observed prevalence of ‘suggests’ in those articles all contribute to the inspiration for this study and the interest in NLP for a potential solution.\n",
    "\n",
    "##### 2.3 Purpose and research questions\n",
    "No one currently knows the extent of these language patterns. Therefore, I have two primary research questions:\n",
    "\n",
    "(1) In quantitative educational research articles, how prevalent is the use of language that implies data speak for themselves, for this study specifically focusing on sentences that include a form of the word ‘suggest?’\n",
    "\n",
    "(2) Can the naïve Bayes or neural network NLP algorithms successfully classify these sentences as cases of data speaking for themselves, with perfect or near-perfect precision and at least 50% recall?\n",
    "\n",
    "As a secondary purpose, I hope readers will be encouraged by this study to consider theoretical perspectives in quantitative work and to think about how their choice of words may be supporting undesired meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HYXWwxRH7_E"
   },
   "source": [
    "#### 3. Methods \n",
    "All processing and NLP algorithms were done in Python 3 [10]. All web scraping and conversion of HTML to plaintext were done in R and Perl, respectively [11], [12], repurposing code I originally wrote for [9].\n",
    "\n",
    "##### 3.1 Data collection\n",
    "For this study, I identified all research articles published in volume 58 (published in 2021) of the American Educational Research Journal (AERJ) that were empirical and quantitative-only, excluding meta-analyses, measurement studies, and methodological studies. AERJ is a well-considered journal that accepts articles from all educational research subfields, which is advantageous because it means the trained algorithm will be less likely to be specific to a subfield.\n",
    "\n",
    "Volume 58 contained 72 articles (excluding one published correction to an article). I examined the abstracts of those articles for inclusion, and if there were any question at all about inclusion based on the abstract, I looked at the article body. This resulted in 22 quantitative-only articles as my final sample. Article abstracts and bodies for these 22 articles were then converted to plaintext, excluding any notes, references, appendices, and mathematical markup.\n",
    "\n",
    "Both NLP and the manual coding of sentences required some preprocessing beforehand.\n",
    "\n",
    "Phase one of preprocessing, prior to coding, involved removing instances of ‘et al.’ that were not at the end of a sentence because those could confuse the sentence tokenizer, removing content in parentheses or any other type of bracket (this simplifies the entire process, and any loss of ‘suggests’ within brackets is acceptable), as well as some miscellaneous cleanup important for NLP that would not interfere with manual coding. After cleanup, sentences were tokenized using NLTK’s Punkt tokenizer and sentences that did not contain a form of the word suggest were discarded. Full details are presented in the code in section 4.2.1.\n",
    "\n",
    "Next, I manually coded the 307 sentences that resulted for whether ‘suggest’ is used in a way that may imply data speak for themselves. For this study, I operationalized language that implies data speak for themselves as phrases where any type of data, findings, results, studies, etc. ‘suggest’ a conclusion that could not be supported by data. I erred on the side of the data not speaking for themselves for phrases such as ‘this suggests’ that have no clear indication from the isolated sentence of what is doing the suggesting. I also considered studies, research, literature, theory, author citations, etc. as not referring to data. However in practice, I expect many of these examples are likely cases of data speaking for themselves.\n",
    "\n",
    "Phase two of preprocessing was to do some further cleaning such as removal of punctuation (see section 4.2.3 for full details), stem the words using the ILTK Porter stemmer, and remove stop words. The stop words came from NLTK’s English list with several added and removed on a theoretical basis. One corpus of sentences was saved with stop words left in and one corpus saved with stop words excluded.\n",
    "\n",
    "The multinomial naïve Bayes and MLP neural network algorithms in the sklearn package were then run using bag of words for the feature representation and several factors. After some informal testing to help select appropriate settings to keep and factors to vary, the final conditions used here were (1) sentences with stop words or without stop words; (2) n-gram sizes of 1–3 or 1–5 (n-grams of some size greater than one are important theoretically, but a smaller maximum would help reduce dimensionality); (3) the minimum number of times a feature needs to appear to be included, considering 1–4 as cutoffs (higher numbers reduce dimensionality but may lead to important but rare features being removed); (4) the maximum occurrences, over which a feature will be excluded, comparing all included to some excluded to slightly more excluded (again, this relates to dimensionality); (5) the activation function for the neural network of ReLU or logistic (ReLU is newer and generally preferred, but logistic is worth trying because it fits with the binary form of my desired output [13]); (6) six configurations of hidden layers. The first two configurations were simply no hidden layer and a single hidden layer of 100 nodes. The remaining went from one to four hidden layers with a total number of hidden layer nodes equal to two-thirds the number of input features with a maximum number of 500. The number of nodes is partially based on a common rule of thumb plus the recognition that a small training set may require fewer nodes and a mind for training time [14]. Maintaining the same number of nodes, I divided the nodes between layers such that each node has half as many as the previous layer. Altogether, this resulted in 624 conditions.\n",
    "\n",
    "For training and testing, I aimed for 75% of articles in the training set with 25% in the testing set. The dataset was randomly split by article rather than sentence because sentences within an article are likely to have some similarity. The actual split by article was 16 in training and 6 in testing (73%/27%). At the sentence level, the split was 67%/33%, which is close enough to intended to be fine. Furthermore, the proportion of positive cases in each sample was similar (65.4% in training, 60.5% in testing).\n",
    "\n",
    "##### 3.2 Analysis\n",
    "Analysis of results is descriptive. For RQ1, I use the proportion of articles that contain at least one instance, the proportion of sentences coded as a positive case, the mean number of instances per article, and the distribution of numbers of articles by number of instances. For RQ2, I examine the precision and recall with respect to positive cases at both the sentence and article level. I also take what I consider to be the best NLP result and qualitatively examine the misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxhsjBDDIDIH"
   },
   "source": [
    "#### 4. Analysis Demonstration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQAGOovRHjvx"
   },
   "source": [
    "##### 4.1. Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKqprgOWHCOG"
   },
   "outputs": [],
   "source": [
    "#### Import all necesary libraries\n",
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'omw-1.4', 'stopwords'])\n",
    "import sklearn\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odZPjAudIY3P"
   },
   "source": [
    "##### 4.2. Code\n",
    "###### 4.2.1 Cleaning and data management prior to manual coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dd5d-suXHgIs"
   },
   "outputs": [],
   "source": [
    "#### Load and clean full documents and tokenize into sentences\n",
    "\n",
    "# Load data\n",
    "articles = pd.read_csv('articles_td.csv', sep='\\t', header=0)\n",
    "\n",
    "\n",
    "# Function doclean(): Clean articles prior to coding\n",
    "def doc_clean(text):\n",
    "    text = re.sub(r'[^\\.]+https://orcid\\.org/.*?$', '', text) #remove any ORCID links at end\n",
    "    text = re.sub(r' et al\\.(?:\\'s)?( ?[^\\sA-Z])', r'\\1', text) #remove \"et al.('s)\" (unless at end of sentence, which is unlikely anyway)\n",
    "    text = re.sub(r'\\[MATH\\]', '<MATH>', text) #math ML replaced with [MATH] during conversion from html; change to <MATH>\n",
    "    text = re.sub(r'\\s?\\(.*?\\)', r'', text) #remove content in parentheses\n",
    "    text = re.sub(r'\\s?\\[.*?\\]', r'', text) #remove content in brackets\n",
    "    text = re.sub(r'\\s?\\{.*?\\}', r'', text) #remove content in squigglies\n",
    "    text = re.sub(r\"n't\", r' not', text) #n't to not (not likely)\n",
    "    text = re.sub(r\"'ll\", r' will', text) #'ll to will (not likely)\n",
    "    text = re.sub(r\"'m\", r' am', text) #'m to am (not likely)\n",
    "    text = re.sub(r\"(s?he|it)'s\", r'\\1 is', text) #expand contractions involving 's to is (not likely)\n",
    "    text = re.sub(r\"'re\", r' are', text) #'re to are (not likely)\n",
    "    text = re.sub(r\"'ve\", r' have', text) #'ve to have (not likely)\n",
    "    text = re.sub(r'(\\s?)[\\-–−$+#]*\\.?\\d[\\d\\w\\.,:%$\\(\\)*^#]*?(\\.?[\\s\\-–−])', r'\\1<NUMBER>\\2', text) #numbers to <NUMBER>, with logic to account for decimals, periods, signs, and retaining ending punctuation\n",
    "    text = re.sub(r'\\s+', r' ', text) #collapse all instances of more than one whitespace character into a single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text) #remove any spaces at beginning and end\n",
    "    return text\n",
    "\n",
    "# Clean the abstracts and bodies\n",
    "articles['c_abs'] = [doc_clean(text) for text in articles['Abstract']]\n",
    "articles['c_body'] = [doc_clean(text) for text in articles['Body']]\n",
    "\n",
    "\n",
    "# Tokenize sentences for a given article, with abstracts and bodies converted to long format\n",
    "def my_st(article):\n",
    "    sents_a = nltk.sent_tokenize(article['c_abs'])\n",
    "    sents_b = nltk.sent_tokenize(article['c_body'])\n",
    "    return pd.DataFrame({'ID':np.repeat(article['ID'], len(sents_a)+len(sents_b)), #article ID repeated for all sentences\n",
    "                         'Cat':np.concatenate([np.repeat('a', len(sents_a)), np.repeat('b', len(sents_b))]), #abstract and body codings, repeated the necessary number of times each\n",
    "                         'Sent':sents_a+sents_b}) #the cleaned, tokenized sentences\n",
    "\n",
    "all_sent = pd.concat(articles.apply(my_st, axis=1, result_type='reduce').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Send sentences comtaining forms of 'suggest' to Excel for coding (uncomment to store)\n",
    "\n",
    "#all_sent.loc[all_sent['Sent'].str.contains(r'\\bsuggest')].to_excel('sentences_to_code.xlsx', freeze_panes=(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.2.3 Pre-NLP sentence/word-level cleaning, normalization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load manually-coded sentences for recoding, further cleaning, normalization, etc.\n",
    "\n",
    "### Load manually-coded sentences\n",
    "coded_sent = pd.read_excel('sentences_coded.xlsx')\n",
    "\n",
    "\n",
    "### Recode to n/? to 0 and y/~ to 1\n",
    "coded_sent['Recode'] = coded_sent['Code'].str.contains(r'y|~')*1\n",
    "\n",
    "\n",
    "### Cleaning and word tokenization\n",
    "nopunc = [re.sub(r\"'\", r'', sent) for sent in coded_sent['Sent']] #remove apostrophes and replace with nothing (targets possessives)\n",
    "nopunc = [re.sub(r'\\s*[^\\w\\d<>]\\s*', r' ', sent) for sent in nopunc] #remove all other punctuation, replacing with spaces\n",
    "words = [nltk.word_tokenize(i) for i in nopunc]\n",
    "\n",
    "\n",
    "### Normalize (stem) including stopwords\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmed = [[stemmer.stem(token) for token in sent] for sent in words]\n",
    "\n",
    "# Store as a sentence again (mostly for troubleshooting)\n",
    "coded_sent['Withstop'] = [nltk.tokenize.treebank.TreebankWordDetokenizer().detokenize(sent) for sent in stemmed] #store as a sentence again (mostly for troubleshooting)\n",
    "\n",
    "\n",
    "### Normalize (stem) excluding stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Exclude words from list that could be important subjects for 'suggest'\n",
    "stop_words.discard('i')\n",
    "stop_words.discard('we')\n",
    "stop_words.discard('this')\n",
    "stop_words.discard('these')\n",
    "stop_words.discard('those')\n",
    "stop_words.discard('they')\n",
    "\n",
    "# Include other stop words in list based on observations\n",
    "stop_words.add('also')\n",
    "\n",
    "# Remove stopwords\n",
    "nostop = [np.array(sent)[[word not in stop_words for word in sent]].tolist() for sent in words]    \n",
    "\n",
    "# Stem\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmed = [[stemmer.stem(token) for token in sent] for sent in nostop]\n",
    "\n",
    "# Store as a sentence again (mostly for troubleshooting)\n",
    "coded_sent['Nostop'] = [nltk.tokenize.treebank.TreebankWordDetokenizer().detokenize(sent) for sent in stemmed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.2.4 NLP testing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Testing Balance\n",
      "\n",
      "Training proportion of cases: 0.654\n",
      "Testing proportion of cases: 0.605\n",
      "Sentence percentage in testing: 0.329\n",
      "Article percentage in testing: 0.273\n"
     ]
    }
   ],
   "source": [
    "#### NLP setup, including data splitting\n",
    "\n",
    "### Some specific imports for ease of use\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "### Sim conditions\n",
    "corpuses = [coded_sent['Withstop'], coded_sent['Nostop']]\n",
    "ng_ranges = [(1,3), (1,5)] #n-grams\n",
    "min_dfs = [1, 2, 3, 4] #minimum number of occurances required\n",
    "max_dfs = [0.999, 0.5, 0.1] #no maximum occurrences cut (0.999) vs. cutting some of the maximum occurrences (0.5 and 0.1)\n",
    "act_fs = ['relu', 'logistic'] #activation function\n",
    "\n",
    "\n",
    "### Get data splits by ID for testing/training (have to do it manually because train_test_split doesn't like clusters with only one sentence)\n",
    "ids = coded_sent['ID'].unique()\n",
    "testn = math.ceil(0.25*len(ids)) #number in testing\n",
    "ids = sklearn.utils.shuffle(ids, random_state=32607) #randomize ids\n",
    "testids = ids[0:testn] #get testing ids\n",
    "trainids = ids[testn:] #get training ids\n",
    "\n",
    "## Get coded_set indices for each group\n",
    "testind = coded_sent.index[coded_sent['ID'].isin(testids)]\n",
    "trainind = coded_sent.index[coded_sent['ID'].isin(trainids)]\n",
    "\n",
    "### Check group balance\n",
    "print(\"Training/Testing Balance\\n\")\n",
    "print(\"Training proportion of cases:\", round(coded_sent['Recode'][trainind].sum()/len(coded_sent['Recode'][trainind]), 3))\n",
    "print(\"Testing proportion of cases:\", round(coded_sent['Recode'][testind].sum()/len(coded_sent['Recode'][testind]), 3))\n",
    "print(\"Sentence percentage in testing:\", round(len(coded_sent['Recode'][testind]) / len(coded_sent['Recode'][trainind]), 3))\n",
    "print(\"Article percentage in testing:\", round(testn / len(ids), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.2.5 Run NLP algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NLP algorithm testing\n",
    "\n",
    "# Set up DataFrame for results\n",
    "results = pd.DataFrame(columns=['alg', 'settings', 's_precision', 's_recall', 'a_precision', 'a_recall', 'n_features'])\n",
    "\n",
    "### Big loop\n",
    "for sentence_set in corpuses:\n",
    "    for ng in ng_ranges:\n",
    "        for mindf in min_dfs:\n",
    "            for maxdf in max_dfs:\n",
    "                \n",
    "                ## BOW with training/testing split\n",
    "                bow = CountVectorizer(ngram_range=ng, min_df=mindf, max_df=maxdf)\n",
    "                X = bow.fit_transform(sentence_set)\n",
    "\n",
    "                x_train = X[trainind]\n",
    "                x_test = X[testind]\n",
    "                y_train = coded_sent['Recode'][trainind]\n",
    "                y_test = coded_sent['Recode'][testind]\n",
    "\n",
    "\n",
    "                ## Naive Bayes\n",
    "                clf_nb = MultinomialNB()\n",
    "                clf_nb.fit(x_train, y_train)\n",
    "                y_hat = clf_nb.predict(x_test)\n",
    "\n",
    "                # Record NB results\n",
    "                ay_test = y_test.groupby(coded_sent['ID'][testind]).sum().ne(0)\n",
    "                ay_hat = pd.Series(y_hat).groupby(coded_sent['ID'][testind].reset_index()['ID']).sum().ne(0)\n",
    "                outcomes = [round(sum(y_test & y_hat) / sum(y_hat), 3) if sum(y_hat) != 0 else 0, #sentence precision\n",
    "                            round(sum(y_test & y_hat) / sum(y_test), 3), #sentence recall\n",
    "                            round(sum(ay_test & ay_hat) / sum(ay_hat), 3) if sum(ay_hat) != 0 else 0, #article precision\n",
    "                            round(sum(ay_test & ay_hat) / sum(ay_test), 3)] #article recall\n",
    "                \n",
    "                results.loc[len(results)] = ['NB', sentence_set.name+' '+str(ng)+' '+str(mindf)+' '+str(maxdf)] + outcomes + [x_train.shape[1]]\n",
    "                #print(results.loc[len(results)-1].to_list())\n",
    "                \n",
    "                                \n",
    "                ## Neural Network\n",
    "                \n",
    "                # Set up layers, mostly based on dimensionality\n",
    "                n_nodes = min(math.ceil(x_train.shape[1]*2/3), 500) #same as 2/3 input layer or max of 500\n",
    "                layers = [(),                             #no hidden layers\n",
    "                          (100,),                         #sklearn default (one layer)\n",
    "                          (n_nodes,),                     #one hidden layer\n",
    "                          (math.floor(n_nodes*2/3),       #two hidden layers\n",
    "                           n_nodes-math.floor(n_nodes*2/3)),\n",
    "                          (math.floor(n_nodes*4/7),       #three hidden layers\n",
    "                           math.floor(n_nodes*2/7),\n",
    "                           n_nodes-math.floor(n_nodes*4/7)-math.floor(n_nodes*2/7)),\n",
    "                          (math.floor(n_nodes*8/15),      #four hidden layers\n",
    "                           math.floor(n_nodes*4/15),\n",
    "                           math.floor(n_nodes*2/15),\n",
    "                           n_nodes-math.floor(n_nodes*8/15)-math.floor(n_nodes*4/15)-math.floor(n_nodes*2/15))]\n",
    "                          \n",
    "                # Loop it\n",
    "                for act in act_fs:\n",
    "                    for lay in layers:\n",
    "                        # Neural Network\n",
    "                        clf_nn = MLPClassifier(hidden_layer_sizes=lay, random_state=352, max_iter=500, activation=act, solver='lbfgs')\n",
    "                        clf_nn.fit(x_train, y_train)\n",
    "                        y_hat = clf_nn.predict(x_test)\n",
    "\n",
    "                        # Record NN results\n",
    "                        ay_test = y_test.groupby(coded_sent['ID'][testind]).sum().ne(0)\n",
    "                        ay_hat = pd.Series(y_hat).groupby(coded_sent['ID'][testind].reset_index()['ID']).sum().ne(0)\n",
    "                        outcomes = [round(sum(y_test & y_hat) / sum(y_hat), 3) if sum(y_hat) != 0 else 0, #sentence precision\n",
    "                                    round(sum(y_test & y_hat) / sum(y_test), 3), #sentence recall\n",
    "                                    round(sum(ay_test & ay_hat) / sum(ay_hat), 3) if sum(ay_hat) != 0 else 0, #article precision\n",
    "                                    round(sum(ay_test & ay_hat) / sum(ay_test), 3)] #article recall\n",
    "                        \n",
    "                        results.loc[len(results)] = ['NN', sentence_set.name+' '+str(ng)+' '+str(mindf)+' '+str(maxdf)+' '+act+' '+str(lay)] + outcomes + [x_train.shape[1]]\n",
    "                        #print(results.loc[len(results)-1].to_list())\n",
    "\n",
    "### Output results to Excel\n",
    "results.to_excel('results.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.2.6 Partial RQ1 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of articles with a case: 1\n",
      "Proportion of sentences that are cases: 0.642\n",
      "Mean sentence-cases per article: 8.95\n",
      " Standard deviation: 6.1\n"
     ]
    }
   ],
   "source": [
    "#### Some results from manual-coding\n",
    "\n",
    "# Proportions, mean, and sd\n",
    "counts_per_article = coded_sent['Recode'].groupby(coded_sent['ID']).sum()\n",
    "print(\"Proportion of articles with a case:\", statistics.mean(counts_per_article.ne(0)))\n",
    "print(\"Proportion of sentences that are cases:\", round(statistics.mean(coded_sent['Recode']), 3))\n",
    "print(\"Mean sentence-cases per article:\", round(statistics.mean(counts_per_article), 2))\n",
    "print(\" Standard deviation:\", round(statistics.stdev(counts_per_article), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.2.7 Partial RQ2 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get misclassification data for the best NLP result for qualitative analysis\n",
    "#### Best result was Nostop (1, 3) 1 0.1 relu (266, 133, 66, 35)\n",
    "\n",
    "# BOW with training/testing split\n",
    "bow = CountVectorizer(ngram_range=(1,3), min_df=1, max_df=0.1)\n",
    "X = bow.fit_transform(coded_sent['Nostop'])\n",
    "\n",
    "x_train = X[trainind]\n",
    "x_test = X[testind]\n",
    "y_train = coded_sent['Recode'][trainind]\n",
    "y_test = coded_sent['Recode'][testind]\n",
    "\n",
    "\n",
    "# Neural Network\n",
    "clf_nn = MLPClassifier(hidden_layer_sizes=(266, 133, 66, 35), random_state=352, max_iter=500, activation='relu', solver='lbfgs')\n",
    "clf_nn.fit(x_train, y_train)\n",
    "y_hat = clf_nn.predict(x_test)\n",
    "\n",
    "\n",
    "# Record false positives (FP) and false negatives (FN)\n",
    "errors = coded_sent.copy()\n",
    "\n",
    "FP = ((pd.Series(y_hat).eq(1) & y_test.reset_index(drop=True).eq(0))*1)\n",
    "FP.index = y_test.index\n",
    "errors['FP'] = [FP[i] if (i in testind) else 0 for i in range(len(errors))]\n",
    "\n",
    "FN = ((pd.Series(y_hat).eq(0) & y_test.reset_index(drop=True).eq(1))*1)\n",
    "FN.index = y_test.index\n",
    "errors['FN'] = [FN[i] if (i in testind) else 0 for i in range(len(errors))]\n",
    "\n",
    "errors.to_excel('errors.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1e7Gt-BIhWL"
   },
   "source": [
    "#### 4. Results \n",
    "\n",
    "For RQ1, 100% of the manually-coded articles had at least one positive case, and 64.2% of the sentences were a positive case. The average number per article was 8.95 (_sd_ = 6.10). The distribution of articles by numbers of cases is displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Histogram (# of articles per case count)\n",
      "1.0 * * *\n",
      "3.0 * *\n",
      "5.0 * * * *\n",
      "7.0 * * *\n",
      "9.0 * * *\n",
      "11.0 * * *\n",
      "13.0 *\n",
      "15.0 \n",
      "17.0 *\n",
      "19.0 \n",
      "21.0 *\n",
      "23.0 *\n"
     ]
    }
   ],
   "source": [
    "#### Figure: Histogram\n",
    "frequency, bins = np.histogram(coded_sent['Recode'].groupby(coded_sent['ID']).sum(), bins=12, range=[1, 25])\n",
    "print('\\nHistogram (# of articles per case count)')\n",
    "for b, f in zip(bins[0:], frequency):\n",
    "    print(round(b, 1), ' '.join(np.repeat('*', f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution lies mainly from 1 to 12 cases per article with numbers higher than that being rarer.\n",
    "\n",
    "For RQ2, precision matters the most, so I examined the top 20 results sorted by sentence-level precision and and then recall, displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg</th>\n",
       "      <th>settings</th>\n",
       "      <th>s_precision</th>\n",
       "      <th>s_recall</th>\n",
       "      <th>a_precision</th>\n",
       "      <th>a_recall</th>\n",
       "      <th>n_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>Nostop (1, 3) 1 0.1 relu (266, 133, 66, 35)</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833</td>\n",
       "      <td>10997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 3) 1 0.999</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.478</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>11017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 3) 1 0.5</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.478</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>11017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 5) 1 0.999</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.283</td>\n",
       "      <td>1</td>\n",
       "      <td>0.667</td>\n",
       "      <td>21114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 5) 1 0.5</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.283</td>\n",
       "      <td>1</td>\n",
       "      <td>0.667</td>\n",
       "      <td>21114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 3) 2 0.999</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 3) 2 0.5</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NB</td>\n",
       "      <td>Withstop (1, 5) 1 0.1</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.130</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>30908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 5) 1 0.1</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.130</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>21094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NN</td>\n",
       "      <td>Nostop (1, 5) 1 0.1 logistic (100,)</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833</td>\n",
       "      <td>21094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NB</td>\n",
       "      <td>Withstop (1, 3) 1 0.1</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.370</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>14845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 5) 2 0.999</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.370</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 5) 2 0.5</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.370</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NN</td>\n",
       "      <td>Nostop (1, 5) 1 0.1 relu (333, 167)</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.435</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833</td>\n",
       "      <td>21094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 3) 1 0.1</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.326</td>\n",
       "      <td>1</td>\n",
       "      <td>0.667</td>\n",
       "      <td>10997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NB</td>\n",
       "      <td>Withstop (1, 5) 1 0.999</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.217</td>\n",
       "      <td>1</td>\n",
       "      <td>0.667</td>\n",
       "      <td>30955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NB</td>\n",
       "      <td>Withstop (1, 5) 1 0.5</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.217</td>\n",
       "      <td>1</td>\n",
       "      <td>0.667</td>\n",
       "      <td>30949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NN</td>\n",
       "      <td>Withstop (1, 5) 1 0.1 logistic (100,)</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.522</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>30908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 3) 4 0.1</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NB</td>\n",
       "      <td>Nostop (1, 5) 4 0.1</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alg                                     settings  s_precision  s_recall  \\\n",
       "0   NN  Nostop (1, 3) 1 0.1 relu (266, 133, 66, 35)        0.885     0.500   \n",
       "1   NB                        Nostop (1, 3) 1 0.999        0.880     0.478   \n",
       "2   NB                          Nostop (1, 3) 1 0.5        0.880     0.478   \n",
       "3   NB                        Nostop (1, 5) 1 0.999        0.867     0.283   \n",
       "4   NB                          Nostop (1, 5) 1 0.5        0.867     0.283   \n",
       "5   NB                        Nostop (1, 3) 2 0.999        0.864     0.413   \n",
       "6   NB                          Nostop (1, 3) 2 0.5        0.864     0.413   \n",
       "7   NB                        Withstop (1, 5) 1 0.1        0.857     0.130   \n",
       "8   NB                          Nostop (1, 5) 1 0.1        0.857     0.130   \n",
       "9   NN          Nostop (1, 5) 1 0.1 logistic (100,)        0.852     0.500   \n",
       "10  NB                        Withstop (1, 3) 1 0.1        0.850     0.370   \n",
       "11  NB                        Nostop (1, 5) 2 0.999        0.850     0.370   \n",
       "12  NB                          Nostop (1, 5) 2 0.5        0.850     0.370   \n",
       "13  NN          Nostop (1, 5) 1 0.1 relu (333, 167)        0.833     0.435   \n",
       "14  NB                          Nostop (1, 3) 1 0.1        0.833     0.326   \n",
       "15  NB                      Withstop (1, 5) 1 0.999        0.833     0.217   \n",
       "16  NB                        Withstop (1, 5) 1 0.5        0.833     0.217   \n",
       "17  NN        Withstop (1, 5) 1 0.1 logistic (100,)        0.828     0.522   \n",
       "18  NB                          Nostop (1, 3) 4 0.1        0.826     0.413   \n",
       "19  NB                          Nostop (1, 5) 4 0.1        0.826     0.413   \n",
       "\n",
       "    a_precision  a_recall  n_features  \n",
       "0             1     0.833       10997  \n",
       "1             1     1.000       11017  \n",
       "2             1     1.000       11017  \n",
       "3             1     0.667       21114  \n",
       "4             1     0.667       21114  \n",
       "5             1     1.000        1488  \n",
       "6             1     1.000        1488  \n",
       "7             1     0.500       30908  \n",
       "8             1     0.500       21094  \n",
       "9             1     0.833       21094  \n",
       "10            1     1.000       14845  \n",
       "11            1     1.000        1616  \n",
       "12            1     1.000        1616  \n",
       "13            1     0.833       21094  \n",
       "14            1     0.667       10997  \n",
       "15            1     0.667       30955  \n",
       "16            1     0.667       30949  \n",
       "17            1     1.000       30908  \n",
       "18            1     1.000         500  \n",
       "19            1     1.000         502  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Table: Top 20 NLP results according to sentence-level precision and then recall\n",
    "pd.read_excel('results.xlsx').sort_values(['s_precision', 's_recall'], ascending=False).head(20).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHRLxsDAIlCr",
    "tags": []
   },
   "source": [
    "In the above, \"s_\" indicates sentences and \"a_\" indicates articles. The settings column contains a string representing the specific settings used in the order of stop words, n-grams, minimum occurrences, maximum occurrences (0.999 is no maximum while lower numbers place greater restrictions), activation function (NN-only), and hidden layer configuration (NN-only). The highest article precision is .885 for the setting \"Nostop (1, 3) 1 0.1 relu (266, 133, 66, 35).\" This is lower than the ideal of 1.00, but the recall for that just hits the desired threshold at .500. Article precision will always be 1.00 because there were no true negatives for articles. Article recall is .83, meaning one article was not correctly classified as a positive case. The number of features included in each analysis are displayed for additional context.\n",
    "\n",
    "I care more about false positives than negatives, so for further investigation, below are the cleaned and normalized versions of the three negative cases that were predicted to be positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0c6e6_row0_col0, #T_0c6e6_row0_col1, #T_0c6e6_row1_col0, #T_0c6e6_row1_col1, #T_0c6e6_row2_col0, #T_0c6e6_row2_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0c6e6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0c6e6_level0_col0\" class=\"col_heading level0 col0\" >Sent</th>\n",
       "      <th id=\"T_0c6e6_level0_col1\" class=\"col_heading level0 col1\" >Nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0c6e6_level0_row0\" class=\"row_heading level0 row0\" >185</th>\n",
       "      <td id=\"T_0c6e6_row0_col0\" class=\"data row0 col0\" >Academic capitalism suggests that race and ethnicity are also related to the ways in which student markets are segmented.</td>\n",
       "      <td id=\"T_0c6e6_row0_col1\" class=\"data row0 col1\" >academ capit suggest race ethnic relat way student market segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c6e6_level0_row1\" class=\"row_heading level0 row1\" >208</th>\n",
       "      <td id=\"T_0c6e6_row1_col0\" class=\"data row1 col0\" >After examining potential shifts in enrollment, we did find some evidence to suggest that Native American, White, and Asian students who would otherwise have attended two nearby <NUMBER>-year universities instead decided to enroll in TCC due to Tulsa Achieves.</td>\n",
       "      <td id=\"T_0c6e6_row1_col1\" class=\"data row1 col1\" >after examin potenti shift enrol we find evid suggest nativ american white asian student would otherwis attend two nearbi <number> year univers instead decid enrol tcc due tulsa achiev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c6e6_level0_row2\" class=\"row_heading level0 row2\" >231</th>\n",
       "      <td id=\"T_0c6e6_row2_col0\" class=\"data row2 col0\" >Building on the existing studies on AP and DE enrollment as well as the broader literature on racial disparities in educational choices and outcomes, we focus on six sets of factors that theories and existing literature suggest may be correlated with racial disparities in AP and DE participation: student academic preparation prior to high school, family socioeconomic background, racial composition in a district, between-school income segregation and racial segregation, average characteristics of high schools in a district, and state-level AP and DE policies.</td>\n",
       "      <td id=\"T_0c6e6_row2_col1\" class=\"data row2 col1\" >build exist studi ap de enrol well broader literatur racial dispar educ choic outcom we focu six set factor theori exist literatur suggest may correl racial dispar ap de particip student academ prepar prior high school famili socioeconom background racial composit district school incom segreg racial segreg averag characterist high school district state level ap de polici</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13400f56070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Table: Get negative cases that were predicted to be positive\n",
    "with pd.option_context('display.max_colwidth', 0):\n",
    "    display(pd.read_excel('errors.xlsx')[['Sent','Nostop']].loc[errors['FP'].eq(1)].style.set_properties(**{'text-align': 'left'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHRLxsDAIlCr",
    "tags": []
   },
   "source": [
    "A few things to note about the above: The first sentence (#185) contains the subject \"academic capitalism,\" which was not in the training set at all. The second sentence (#208) was an uncertain case for me during coding, so I said it was not example. However, it likely should be. The third sentence (#231) includes \"theories\" and \"literature,\" which were always coded as definite negative cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHRLxsDAIlCr"
   },
   "source": [
    "#### 5. Conclusion and Discussion\n",
    "\n",
    "The results from hand-coding are promising to me as a confirmation of data speaking for themselves because the results show what I consider to be practically-significant percentages of articles (100%) and sentences (64.2% or 197 sentences in 22 articles) as positive cases. And although the distribution of cases does tend towards lower numbers per article, this is with only looking at sentences containing 'suggest' and with coding 'this suggests' as negative. The true number of cases would be larger.\n",
    "\n",
    "None of the NLP results met my standards, but the top result did reach a precision of .885 and a recall of .500. I would want both of those to be much higher before using an algorithm in practice. Naive Bayes dominated the top results by sentence precision but typically had much worse recall than the neural networks at the top. Based on this, neural networks seem more promising to me, especially because there is still room to try other hidden layer structure. However, the performance of naive Bayes could always change with alternative feature representations, other methods for reducing dimensionality, and more data.\n",
    "\n",
    "The good news is that precision and recall may have been underestimated here because one of the false positives likely should have been coded as a positive, but even that would not be enough to bring the results up to a level I would be satisfied with. The other two false positives possibly show something interesting. In one, there is a sentence subject that is unlikely to be used in many papers except from people using an academic capitalism framework. In the other, I cannot identify any words or phrases in particular that may have caused the misprediction, but it _is_ a particularly long sentence with many words that are theoretically irrelevant but potentially misleading for these algorithms, as run.\n",
    "\n",
    "The largest limitation for this study was time. Lack of time led to focusing only on a single year's articles and only on 'suggest,' limiting both the training and testing data in a population where each document frequently has its own subject-specific language. It also meant I was the only coder and was coding while also building my operationalization of data speaking for itself. Flawed input easily leads to flawed performance. The next steps before running more NLP algorithms _should_ be to take the time to properly operationalize data speaking for itself, to collect and code more data for training and testing, and to ideally include two other coders who are well-versed in quantitative methods. Involving others should also increase confidence in the manually-coded results, making for a stronger argument even just using those. A related open question is how to handle coreferences like 'this suggests.' There are NLP algorithms that attempt to resolve these, which could lead to identifying even more cases. However, those algorithms would need detailed evaluation to make sure they do not introduce false positives by incorrectly resolving coreferences. This is especially important because of my focus on high precision / few false positives rather than on optimizing recall.\n",
    "\n",
    "Based on the number of features in the top-performing NLP version just from 307 sentences, I am also still concerned about dimensionality. For this, I may need to try alternative, more-dense feature representations or try other methods for reducing the dimensions of the feature set. And because my corpus has clusters (sentences within articles), it may be worth considering frequency within an article compared to frequency among all articles when discarding features or even to identify article-specific terminology that can be replaced with placeholders or even removed entirely. Different rules for different n-gram sizes could also be considered, because larger n-grams are less likely to occur frequently and may contain important context. In the same vein, rules could be different depending on whether an n-gram contains a word of interest or not (e.g., suggests, results, data, etc.).\n",
    "\n",
    "Finally, this was also not a particularly exhaustive array of the potential options whether in terms of algorithms or even if continuing with basic MLP neural networks and n-grams. Other network configurations with different numbers of nodes could certainly work better, and the possibilities there are seemingly endless with only discrepant rules of thumb as a priori guidance [14]. Resolving this requires systematic evaluation and tuning beyond the current scope.\n",
    "\n",
    "While the results are not quite what I would have liked, whether because of a rushed process or lower-than-desired precision, I am satisfied. The process of this study, which was essentially a pilot study of sorts, will inform future more-rigorous work. And based on these results, I am now more convinced that data speaking for themselves is endemic in our communication of educational research. Identification is the first necessary step in creating a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHRLxsDAIlCr"
   },
   "source": [
    "#### References\n",
    "\n",
    "[1] Dalton, C., & Thatcher, J. (2014). What does a critical data studies look like, and why do we care? Seven points for a critical approach to ‘big data’. Society and Space, 29.\n",
    "\n",
    "[2] Gillborn, D., Warmington, P., & Demack, S. (2018). QuantCrit: education, policy, ‘Big Data’ and principles for a critical race theory of statistics. Race Ethnicity and Education, 21(2), 158-179.\n",
    "\n",
    "[3] D'Ignazio, C., & Klein, L. F. (2020). Data feminism. MIT press.\n",
    "\n",
    "[4] Phillips, D. C. (2004). Two decades after: “After the wake: Postpositivistic educational thought”. Science & Education, 13(1), 67-84.\n",
    "\n",
    "[5] Phillips, D. C., & Burbules, N. C. (2000). Postpositivism and educational research. Rowman & Littlefield.\n",
    "\n",
    "[6] Lipton, Z. C., & Steinhardt, J. (2019). Research for practice: troubling trends in machine-learning scholarship. Communications of the ACM, 62(6), 45-53.\n",
    "\n",
    "[7] Bleske-Rechek, A., Gunseor, M. M., & Maly, J. R. (2018). Does the language fit the evidence? Unwarranted causal language in psychological scientists’ scholarly work. The Behavior Therapist.\n",
    "\n",
    "[8] Robinson, D. H., Levin, J. R., Thomas, G. D., Pituch, K. A., & Vaughn, S. (2007). The incidence of “causal” statements in teaching-and-learning research journals. American Educational Research Journal, 44(2), 400-413.\n",
    "\n",
    "[9] Huggins‐Manley, A. C., Wright, E. A., DePue, M. K., & Oberheim, S. T. (2021). Unsupported causal inferences in the professional counseling literature base. Journal of Counseling & Development, 99(3), 243-251.\n",
    "\n",
    "[10] Van Rossum, G., & Drake, F. L. (2009). Python 3 Reference Manual. Scotts Valley, CA: CreateSpace.\n",
    "\n",
    "[11] R Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing. Vienna, Austria. https://www.R-project.org/.\n",
    "\n",
    "[12] Wall, L., Christiansen, T., & Orwant, J. (2000). Programming Perl. O’Reilly Media, Inc.\n",
    "\n",
    "[13] Goldberg, Y. (2016). A primer on neural network models for natural language processing. Journal of Artificial Intelligence Research, 57, 345-420.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
