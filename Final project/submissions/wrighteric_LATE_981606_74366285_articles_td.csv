ID	JTitle	Link	ATitle	Abstract	Body
10.3102_0002831220924037	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220924037	Can a Positive School Climate Promote Student Attendance? Evidence From New York City	 Nearly 15% of American students are chronically absent from school. To address absenteeism, many states have recently made chronic absence a core component of their school accountability plans. Scholars have theorized that a positive school climate can promote student attendance, but empirical support for this idea is lacking. In this study, the relationship between four student-reported measures of school climate and student attendance are investigated by analyzing two annual school climate surveys (N = 823,753) from New York City. Results indicate small associations among the four measures of perceived school climate and student attendance. Furthermore, school-level changes in perceived school climate between middle and high school were only marginally associated with student attendance.	 National trends indicate that student absenteeism is an endemic problem in the United States. During the 2015–2016 academic year, approximately 1 in 6 students missed 15 or more days of school, amounting to 7.8 million students being chronically absent (U.S. Department of Education, 2019). This striking rate of absenteeism suggests that millions of students are unable to realize the potential benefits of the public education system (Chang et al., 2018; Goodman, 2014). Frequent absenteeism may also have ramifications of even greater concern as chronic absenteeism has been consistently linked to school dropout, criminal behavior, and other adverse life outcomes (Bell et al., 2016; Gottfried & Kirksey, 2017; Sheldon, 2007; Spencer, 2009). As part of attempts to address absenteeism, 36 states have made student attendance a core component of their school accountability systems (U.S. Department of Education, 2019). One of the main assumptions behind holding schools accountable for student attendance is that school personnel will be incentivized to reduce absenteeism (Bauer et al., 2018; Gottfried & Hutt, 2019), but it is unclear what strategies schools might undertake to improve student attendance (Balu et al., 2016; Corrin et al., 2016; Parise et al., 2017; Robinson et al., 2018). Scholars have theorized that fostering a positive school climate may be an effective way to promote student attendance, but little empirical research has tested this claim (Kostyo et al., 2018; L. E. Maxwell, 2016; Van Eck et al., 2017). This study investigates the relationship between four student-reported measures of school climate and student attendance by linking administrative data to 823,753 student responses to New York City's Learning Environment Survey in 2011 and 2012. This survey of the city's public school students in Grades 6 to 12 is the largest school climate survey administered in the United States. It contains well-documented components of school climate, enabling an examination of key dimensions of school climate with a large analytic sample (Berkowitz et al., 2017; Thapa et al., 2013). The analyses first examine the relationship between individual student perceptions of school safety, the relational environment, personal connectedness, and academic engagement and two measures of student attendance (i.e., total absences and chronic absence). By using two consecutive years of data, this study also tests whether school-level changes in measures of perceived school climate between middle and high school are associated with individual students' total absences and chronic absenteeism (n = 61,684). This focus on ninth grade investigates a critical transitional phase when school climate may be consequential for reducing absenteeism (Alspaugh, 1998; Barber & Olsen, 2004; Roderick et al., 2014). While the analyses do not allow for causal claims, this study considerably advances prior research examining the relationship between school climate and student attendance. It also provides suggestive evidence on the potential for school climate to improve attendance rates. A large evidence base links absenteeism to many academic and later-life outcomes (Goodman, 2014; Henry & Huizinga, 2007; Kearney, 2008; Spencer, 2009). Cross-sectional and longitudinal studies have routinely found that student absenteeism is associated with lower academic achievement (Chang & Romero, 2008; Gottfried, 2009; Ready, 2010) while other recent work using instrumental variables analyses offers causal evidence indicating that absenteeism leads to lower achievement (Goodman, 2014). Chronic absence, generally defined as missing 10% of the school year, is also related to grade retention and school dropout (Cortiella & Boundy, 2018; Gottfried, 2014a; Neild & Balfanz, 2006). By analyzing longitudinal data in Chicago, Smerillo et al. (2018) reported that students with 14 or more absences in eighth grade were 18% less likely to graduate from high school within 4 years. Along with underperformance in school, chronic absenteeism may have other harmful consequences. Chronically absent youth exhibit comparatively high rates of substance abuse and criminal activity (Chou et al., 2006; Hallfors & Godette, 2002; Henry & Huizinga, 2007; McConnel & Kubina, 2014; Spencer, 2009). Researchers have also linked frequent school absence to adverse outcomes in later life, including unemployment and incarceration (Henry et al., 2012; Kearney, 2008; U.S. Department of Education, 2019). These findings are concerning since nearly 8 million students in the American public education system are chronically absent from school (U.S. Department of Education, 2019). Many factors underlying absenteeism appear to emanate outside of school (Datar & Strum, 2006; Flaherty et al., 2012; Leventhal & Brooks-Gunn, 2004; Levy et al., 2011). Student health issues, such as long-term illness, asthma, and obesity, influence the number of days that a student is absent during the school year (Daniels, 2006; Datar & Strum, 2006; Geier et al., 2007; Levy et al., 2011; Moricca et al., 2013). Physical and mental health challenges may also increase the likelihood of being absent from school for students with disabilities (Cortiella & Boundy, 2018; Da Costa Nunez et al., 2015). In addition, socioeconomic disadvantage may create conditions that lead to higher absenteeism (Henry & Huizinga, 2007; Leventhal & Brooks-Gunn, 2004). Studies find that homelessness, parental incarceration, and highly adverse household circumstances create daily struggles to meet basic needs that can lead to irregular student attendance (Flaherty et al., 2012; Hjalmarsson, 2008). Misconceptions about school attendance, a lack of transportation, and difficulty monitoring children during the school day have also been linked to chronic absence among students of low-income families (Balfanz & Byrnes, 2012; Henry & Huizinga, 2007; Ready, 2010). In high-poverty neighborhoods with high crime rates, safety concerns may further discourage students from commuting to school when safe transit options are unavailable (Gottfried, 2017; Hamlin, 2019). Although nonschool factors seem to have considerable influence on attendance, schools have been an important setting for launching intervention efforts (Gottfried & Kirksey, 2017). Experimental interventions in which schools deliver alerts and information to parents about their child's attendance show only slight or no improvement in student absenteeism (Balu et al., 2016; Robinson et al., 2018; Rogers et al., 2017). Similarly, other correlational work finds small associations between student attendance and access to school transportation (Cordes et al., 2019), home-school partnerships (Sheldon, 2007), and delayed school start times (Wahlstrom et al., 2014). To generate greater improvements in attendance, schools may require a comprehensive approach that addresses deeper student needs on an ongoing basis (Adams et al., 2016; Loukas & Murphy, 2007; Ryan & Deci, 2000). Positive school climate may conceivably foster conditions responsive to students' academic, social, and emotional needs each school day (Cohen et al., 2009; Gage et al., 2016). Yet the concept of school climate is broad, being described as beliefs, interactions, relationships, and organizational features that shape the experience of school life (Aldridge et al., 2018; Davis & Warner, 2018). Researchers have thus used varying proxy measures when attempting to assess school climate (Berkowitz et al., 2017). While consensus is lacking on what precise factors constitute a school's climate, extensive scholarly reviews underscore safety, interpersonal relationships, connectedness to school, academic engagement, and physical resources as common dimensions of school climate (McConnel & Kubina, 2014; Thapa et al., 2013; Wang & Bergol, 2016). When measuring safety as a dimension of school climate, studies have tended to analyze incidents of school crime, violence, and bullying as well as students' perceptions of safety in classrooms, hallways, and bathrooms (Hamlin & Li, 2020; Lesneski & Block, 2017; Ripski & Gregory, 2009). Others have used reports of school harassment, verbal abuse, and discrimination in school to assess emotional aspects of school safety (Gower et al., 2015). To evaluate a school's relational environment, scholars have investigated whether the nature of interactions and relationships among students, parents, teachers, and staff are characterized by respect, trust, and inclusivity (Adams et al., 2016; Forsyth et al., 2011). In the literature on connectedness, studies have analyzed reports of how comfortable students feel with school staff, how welcome students feel at school, and whether students believe adults and their peers in school know who they are (Lohmeier & Lee, 2011; Osterman, 2000). Analyses of academic engagement use an array of proxy measures that comprise students' perceptions of instructional practices, course offerings, and academic expectations; teachers' reports on professional development, organizational supports, and school leadership; and student grades, test scores, and graduation rates (Berkowitz et al., 2017; Thapa et al., 2013). To gain understanding of physical resources in schools, researchers have tended to examine access to supplies and technological tools as well as the conditions of school facilities and classrooms (Thapa et al., 2013; Van Eck et al., 2017). Across these dimensions of school climate, multilevel analyses suggest that not only is school climate represented by individual experiences but also it is a distinct product of the school as a collective (Adams et al., 2016; S. Maxwell et al., 2017). The theoretical basis for the idea that positive school climate can lead to improved attendance is compelling (Bauer et al., 2018; Freeman et al., 2016; Kostyo et al., 2018). Schools where students feel free from violence, bullying, and harassment may ensure regular attendance by satisfying fundamental needs for safety and security (Forsyth et al., 2011; Hamlin, 2019; Ryan & Deci, 2000). Quality interpersonal relationships may produce positive developmental assets that provide a buffer against risk factors that cause absenteeism (Adams et al., 2016) while relationships characterized by trust, respect, and fairness among staff, teachers, and students may generate social capital, positive social norms, and emotional support systems that help to reduce school avoidance (Bandura, 1997; Ryan & Deci, 2000). Furthermore, feelings of connectedness at school may fulfill developmental needs for social attachment and relatedness with others, conceivably increasing motivation to be in school (Lohmeier & Lee, 2011; Osterman, 2000). In academically engaging environments interest in and sense of purpose for being in school may increase student attendance (Gershenson, 2016; Simons et al., 2010). Different elements of school climate could also provide important social and emotional safeguards that support regular attendance during periods when students are at risk for higher absence (Freeman & Simonsen, 2015; Rocque et al., 2017; Williams & Richman, 2007). For instance, the transition from middle to high school is a critical point when student absenteeism spikes and the likelihood of grade failure is higher than any other academic year (Alspaugh, 1998; Barber & Olsen, 2004; Williams & Richman, 2007). Positive school climate could reduce absenteeism among ninth graders whose struggles with attendance have been attributed to declines in school climate between middle and high school (Berkowitz et al., 2017; Thapa et al., 2013; Van Eck et al., 2017). Even though there is a persuasive theoretical rationale linking positive school climate to regular student attendance, very few empirical studies have tested this relationship. Several descriptive studies report that a positive relational environment, school connectedness, and academic engagement are associated with lower dropout rates (Balfanz et al., 2007; Niehaus et al., 2016; Osterman, 2000). Other research shows positive associations between composite measures of school climate and student attendance. Two studies performed in the late 1980s find a positive relationship between student attendance and school climate, but these analyses were derived from small surveys of teachers (deJung & Duckworth, 1986; Gottfredson & Gottfredson, 1989). In more recent work, L. E. Maxwell (2016) observed a positive correlation between school climate and school-level attendance rates in 236 schools while Van Eck et al. (2017) found that positive school climate was associated with lower school-level chronic absence rates in 106 schools. However, these two cross-sectional analyses offer limited empirical evidence, being based on school-level rates of absenteeism that mask differences among students within schools. This study investigates the relationship between four measures of perceived school climate (i.e., school safety, relational environment, personal connectedness, and academic engagement) and total absences and chronic absence among students in Grades 6 to 12. To estimate these relationships, student administrative data are linked to students' responses (N = 823,753) to two annual waves of New York City's Learning Environment Survey in 2011 and 2012. For the analyses, this study first asks the following question: Research Question 1: Is there a relationship between student perceptions of school climate (i.e., school safety, relational environment, personal connectedness, and academic engagement) and total absences and chronic absence? This study then tests whether school-level changes in perceived school climate between middle and high school are associated with individual absences and chronic absence (Freeman & Simonsen, 2015; Rocque et al., 2017). This analysis examines results for students (n = 61,684) who were in eighth grade in 2011 and who entered a new high school as ninth-grade students in 2012. To examine school-level changes in perceived school climate between middle and high school, student ratings of school climate were aggregated to the school level and students entering a new high school were classified as experiencing a school-level increase, decrease, or minimal change in perceived school climate. For this analysis, the following question was asked, Research Question 2: Are school-level changes in perceived school climate between middle and high school associated with individual absences and chronic absence in ninth grade? New York City maintains the largest public school system in the United States with approximately 1.1 million students attending over 1,700 schools in the city's five boroughs. In spring, the city's Department of Education administers its "Learning Environment Survey" to all students in Grades 6 to 12 enrolled in the city's public and charter schools. The survey was developed through a collaborative partnership among university researchers, community groups, and department officials (Nathanson et al., 2013). To support high response rates, schools set aside a single class period during the school day for students to complete the survey. The survey states that student responses are confidential and a third-party distributes and collects the surveys (Nathanson et al., 2013). On the survey, students are asked a series of questions about their perceptions of school safety, interpersonal relationships, connectedness, academic engagement, and resources. In annual reports, aggregated scores from this survey constitute an environment score on school report cards. For this study, two consecutive annual administrations of New York City's Learning Environment Survey in 2011 and 2012 were linked to administrative data containing information on student absences and sociodemographic background characteristics. Item congruence between the 2011 and 2012 surveys enabled a comparison of school climate measures between the 2 years. Table 1 presents summary statistics for each variable used in the analyses. The survey achieved substantial coverage of students in Grades 6 to 12. Seventy-two percent of 570,945 eligible students completed the survey in 2011, and 72% of 569,860 eligible students completed the survey in 2012. In 2011, 94% of schools (n = 1,045) serving Grades 6 to 12 were represented on the survey and 94% of schools (n = 1,057) were represented in 2012. Despite high coverage rates, nonrespondents seem to be students who are more likely to be absent from school. In 2011, nonrespondents had an average of 38 total absences compared with an average of 14 absences for students who are represented in the sample (p < .001). Nonrespondents in 2012 had an average of 36 total absences compared with an average of 12 absences for students who are represented in the sample (p < .001). While it remains unclear whether these nonrespondents (who may have been absent on the day of the survey) are a subgroup with important unobserved characteristics, a large proportion of students with high levels of absenteeism did participate in the survey. Approximately 22% of students in the sample were chronically absent (i.e., 18 or more absences during the school year). Two measures of attendance were analyzed. Total absences was a count variable indicating the total number of absences for a student during the school year. Chronic absence was a dichotomous variable indicating whether a student had been absent 18 or more times during the school year. This threshold for chronic absenteeism coheres with the (U.S. Department of Education's (2019) classification of chronic absence. From the Learning Environment Survey, four student-reported measures of school climate were constructed from survey items on a 4-point rating scale. School safety was constructed from 10 items inquiring about specific aspects of school safety (e.g., "Students threaten or bully other students at school"), in which response options ranged from "never"; "some of the time"; "most of the time"; and "all of the time." Cronbach's alpha was .87, providing support for combining these items into a single measure.Academic engagement comprised nine items (e.g., "My school helps me develop challenging academic goals"). Cronbach's alpha was .87 for this measure. Personal connectedness comprised seven items inquiring about an individual's perceived sense of belonging and social connections at school (e.g., "I feel welcome at my school"). Cronbach's alpha was .80. Relational environment consisted of eight items (e.g., "Most students in my school treat each other with respect"). Cronbach's alpha was .83. Appendix Table A1 presents individual factor loadings for each item comprising the four measures. Factor loadings indicate that each measure assesses a distinct aspect of school climate. The measures are also closely aligned with those proposed by the U.S. Department of Education (2019) and the National School Climate Council (2019). However, components of school climate related to physical resources, school personnel, and leadership are limited as it may be difficult for students to assess aspects of these school features (Wang & Degol, 2016). Nevertheless, student perceptions of school climate not only provide valuable insight into how students experience school but also are less subject to bias related to job performance that may taint teacher and staff reports of school climate (Koth et al., 2008; Thompson & Goodvin, 2016). In Table 2, between- and within-school variance estimates indicate approximately 30% to 34% in between-school variance for the four measures of school climate. Controls were used for student grade-level, racial background, free/reduced-price lunch, special education, and English language learner status. Controls for school enrollment, grade level, percentage of students eligible for free and reduced-price lunch, and school type (e.g., charter) were included at the school level. The analyses first explore correlations among student-level perceptions of school safety, personal connectedness to school, academic engagement, and the school's relational environment and two dependent variables (i.e., total absences and chronic absence). In subsequent statistical models, negative binomial regression accounting for school-level clustering was performed to examine the relationship between individual student perceptions of the four measures of school climate and total student absences. Negative binomial regression is appropriate for count variables, such as total absences, that are subject to overdispersion (Hilbe, 2011). Logistic regression accounting for school-level clustering was performed to estimate the relationship between individual perceptions of the four measures of school climate and chronic absence. To test the robustness of the main results, separate models for both survey years were performed. Models were also run with chronic absence specified at 15, 20, and 50 total absences to examine whether results vary based on how chronic absence is defined. Separate models were also performed for sixth-, seventh-, and eighth-grade students. As chronic absence rates are lower during earlier grades, these models were used to consider the possible effects of nonresponse among chronically absent students. After estimating relationships for student-reported measures of perceived school climate, associations among school-level changes in perceived school climate and individual attendance were examined. For this phase, eighth-grade students in middle school in 2011 who entered a new high school as ninth graders in 2012 were identified. Then, student-reported perceptions of school safety, personal connectedness, academic engagement, and the relational environment were aggregated to the school level. For each of these measures, ninth-grade students in high school in 2012 who experienced a school-level increase of greater than 0.25 standard deviations between middle and high school were classified as "increased." Students who experienced a school-level decrease of more than 0.25 standard deviations were classified as "decreased." Smaller differences between −0.25 and 0.25 standard deviations were classified as "minimal" change. Since the majority of students in the sample experienced little school-level change in perceived school climate between years, these thresholds enabled meaningful contrasts between students who experienced substantive school-level changes (e.g., increased/decreased) and those who experienced very small or no change on a measure of perceived school climate. In supplementary analyses, varying thresholds ranging from 0.25 to 0.10 standard deviations were tested to determine whether results might be sensitive to different classifications of school-level changes in perceived school climate. Results for chronically absent eighth-grade students in 2011 were also generated to investigate average ratings of perceived school climate for chronically absent students. Correlations among school-level ratings of perceived school safety, personal connectedness, academic engagement, and the relational environment and student attendance were subsequently explored. Then, to examine the relationship between individuals' total absences and school-level change (i.e., increased, decreased, and minimal change) on the four measures of perceived school climate between eighth and ninth grade, negative binomial regression accounting for school-level clustering was performed. Logistic regression was undertaken to estimate these relationships for chronic absence. School-level ratings of perceived school climate may be less affected by issues of endogeneity than individual perceptions, so this school-level analysis may offer additional understanding of the relationship between perceived school climate and attendance. Table 3 presents correlations among individual perceptions of school safety, relational environment, personal connectedness, and academic engagement and total absences and chronic absence. Total absences and chronic absence are positively associated with each other (r = .74, p < .001). However, these two measures of attendance show only small negative correlations with each of the four measures of perceived school climate (r = .03–.09, p < .001). Table 4 presents the results of a negative binomial regression model predicting total absences and a logistic regression model predicting chronic absence (i.e., 18 or more absences during the school year). With controls for student and school characteristics, results show small negative associations between total absences and individual student perceptions of school safety (incidence rate ratio [IRR] = 0.93, p < .001) and academic engagement (IRR = 0.87, p < .001). To interpret these results, when a student's perception of safety increases from one standard deviation below the mean to one standard deviation above the mean, predicted absences decrease from approximately 13 to 11 absences, holding all other variables at their sample means. For academic engagement, predicted absences decrease from 13 to 10 total absences. Given the large student sample, students' perceptions of the relational environment and personal connectedness show statistically significant associations with increased absences, but these associations are small. Analyses using Poisson and zero-inflated negative binomial regression yielded results that were consistent with those using negative binomial regression. For chronic absence, patterns are similar to those observed for total absences. Individual perceptions of school safety (odds ratio [OR] = 0.89, p < .001) and academic engagement (OR = 0.76, p < .001) are associated with less chronic absence, but these relationships are relatively small. For example, when a student's perception of safety increases from one standard deviation below the mean to one standard deviation above the mean, the predicted likelihood of being chronically absent decreases by only 4%, holding all other variables at their sample means. The predicted likelihood of being chronically absent decreases by approximately 8% when a student's perception of academic engagement increases from one standard deviation below the mean to one standard deviation above the mean. The relational environment and personal connectedness are not associated with a lower likelihood of being chronically absent. Results at upper thresholds for chronic absence offer insight into how influential nonresponse might be for students who have absence rates that exceed typical chronic absence thresholds. In supplementary analyses, models using alternative classifications for chronic absence at 15, 20, and 50 total absences exhibited consistent patterns with those of the main analyses. Models performed separately for each survey year also mirrored those of the main analyses. (Results of supplementary analyses available on request.) In addressing this study's first research question, overall results suggest only small associations among individual perceptions of school climate and student attendance. Table 5 presents the percentage of students who experienced increased, decreased, and minimal school-level changes on the four measures of perceived school climate between eighth and ninth grade. Based on descriptive patterns, the majority of ninth-grade students who entered a new high school in 2012 experienced "minimal" school-level change in perceived safety (58%), relational environment (62%), personal connectedness (68%), and academic engagement (59%). However, substantial numbers of students in the sample entered high schools with higher (8%-28%) or lower (14%-33%) school-level ratings of perceived school climate than that of their middle schools in the previous year. Table 5 also presents the percentage of chronically absent students in eighth grade who experienced increased, decreased, and minimal school-level changes on the four measures of perceived school climate on entering a new high school as ninth graders. These percentages for chronically absent students are similar to those of the full sample. Supplementary analyses indicated that school-level ratings of the four school climate measures were not statistically different between students who were chronically absent and those who were not. Table 6 presents correlations among school-level ratings of the four measures of perceived school climate in 2012 and total absences in 2011 and 2012. Total absences in 2012 and in 2011 are positively correlated (r = .68, p < .001) with each other. School-level ratings of perceived school safety (r = −.19, p < .001), relational environment (r = −.15, p < .001), personal connectedness (r = −.07, p < .001), and academic engagement (r = −.09, p < .001) are negatively related to total absences for ninth-grade students. Even though these bivariate associations are slightly larger than those observed at the individual level, they remain small. Table 7 presents the results of a negative binomial regression model predicting total absences and a logistic regression model predicting chronic absence for ninth-grade students who entered new high schools in 2012. For ninth-grade students who entered high schools where the school-level rating of perceived school safety was higher than that of their middle school in the previous year, there are very small statistical associations indicating less absenteeism (IRR = 0.92, p < .001) and chronic absence (OR = 0.87, p < .001). Holding all variables at their sample means, an increase in school-level perceived school safety is associated with a predicted decrease of 11 to 10 absences and a decline of 2% in the likelihood of being chronically absent. A decrease in school-level ratings of perceived school safety is associated with a predicted increase of 10 to 12 absences and a 6% increase in the likelihood of being chronically absent. For school-level increases in ratings of the relational environment, personal connectedness, and academic engagement, no statistical associations are observed for total absences and chronic absence. However, there are small associations for school-level decreases. A school-level decrease in the relational environment is associated with slightly higher absenteeism (IRR = 1.10, p < .001) and chronic absence (OR = 1.31, p < .001). This association amounts to a predicted increase of one absence and a 4% increase in the likelihood of being chronically absent. Personal connectedness was not statistically associated with increased total absences, but it was related to a 3% increase in the likelihood of being chronically absent (OR = 1.20, p < .001). A school-level decline in academic engagement is associated with increased absenteeism (IRR = 1.10, p < .01) and chronic absence (OR = 1.24, p < .001), amounting to a predicted increase of one absence and a 3% increase in the likelihood of being chronically absent. To test the robustness of these results, supplementary analyses examined different specifications of "decreased,""increased," and "minimal" school-level changes in perceived school climate by adjusting the range for minimal change from 0.25 to 0.10 standard deviations. Results remained consistent with those of the main analyses reported in Table 7. Additional checks were done with chronic absence specified at 15, 20, and 50, or more total absences. Statistical patterns across these different models were also consistent with those of the main analyses. (Results of supplementary analyses available on request.) Based on this study's second research question, results indicate that school-level changes in perceived school climate between middle and high school are only marginally associated with student attendance. Student absenteeism is a major concern with 7.8 million students being chronically absent from school (U.S. Department of Education, 2019). These students not only exhibit lower academic achievement than their peers but also are at risk for school dropout, criminal behavior, and other adverse outcomes (Bell et al., 2016; Goodman, 2014; Gottfried & Kirksey, 2017; Spencer, 2009). Most states have recently placed emphasis on absenteeism by making student attendance part of their school accountability systems. Scholars have argued that schools can promote student attendance by fostering a positive school climate, but little empirical research has tested this claim (Kostyo et al., 2018; L. E. Maxwell, 2016). By analyzing student responses to the largest school climate survey administered in the United States, this study found only small associations between student attendance and measures of perceived school climate at individual and school levels. Individual perceptions of school safety and academic engagement were related to slightly less chronic absence and fewer total absences. School-level increases in perceived school safety between eighth and ninth grades were associated with marginally less absenteeism and chronic absence while school-level decreases in perceived school safety, personal connectedness, academic engagement, and the relational environment were associated with a small increase in total absences and chronic absence. These results remained robust across supplementary analyses. The study makes important empirical contributions to the literature. Few studies have tested the relationship between school climate and student attendance. This small body of prior work reports positive associations between perceived school climate and attendance but is limited by small sample sizes, minimal controls for student background characteristics, and use of school-level attendance rates that mask variation among students within schools (deJung & Duckworth, 1986; Gottfredson & Gottfredson, 1989; Van Eck et al., 2017). This study's analyses address these limitations by using two measures of attendance at the student level, multiple controls for student background characteristics, and a very large student sample capable of detecting small associations. Results suggest weaker associations than those of previous studies. Among the four measures of school climate that were investigated, perceived school safety seems to have the strongest consistent relationship with student attendance, but even this relationship was smaller than what has been anticipated (Freeman et al., 2016; Kostyo et al., 2018). In addition to these contributions, this study advances the broader literature on school climate. Estimates were generated from four student-reported measures that scholars have routinely identified as key dimensions of school climate (McConnel & Kubina, 2014; Thapa et al., 2013; Wang & Degol, 2016). The considerable number of survey responses that were analyzed also represents one of the largest scholarly investigations of these four measures of school climate at individual and school levels. The analysis of school-level change in perceived school climate between eighth and ninth grade is also uncommon in the literature. Many prior studies of school climate tend to generate correlational estimates using cross-sectional data (Berkowitz et al., 2017). Furthermore, when students enter ninth grade, the likelihood of absence, grade failure, and school dropout substantially increases (Williams & Richman, 2007). This study's analytical focus on this transitional period is insightful since positive school climate is expected to create conditions that help to reduce absenteeism (Berkowitz et al., 2017; Van Eck et al., 2017). Empirical results, however, suggested only minor associations. This study has several limitations. First, estimates were derived from a study design with known limitations in producing causal inferences. One possible concern with noncasual estimates is reverse causality, whereby strong attendance boosts perceptions of a school's climate. Higher attendance itself could also be indicative of preexisting motivation, social resources, and other unobserved household characteristics (Hamlin, 2019). Household factors that this study was unable to control for, such as family structure, parental monitoring, and parent-child relationships, are thought to have an influence on student attendance (Jeynes, 2010; Sheldon, 2007; Sheldon, & Epstein, 2004). Associations between school climate and attendance could be more of a consequence of these unobserved characteristics rather than strategies that a school undertakes. If this is the case, small associations between positive school climate and student attendance found in this study could become even smaller after accounting for these unobserved characteristics. Future research employing improved causal designs could address these methodological issues. School settings where students are randomly assigned to schools through lotteries or other naturally occurring mechanisms may present opportunities for such analyses. Another limitation is that this study's analyses do not contain controls for community-level services, youth interventions, and programs that may affect student attendance (Gottfried, 2014b). Some community-level absence interventions and youth programs have shown small positive effects on student attendance in the study setting of New York City (Balfanz & Byrnes, 2018; Balu et al., 2016). Without controls for these community-level factors, this study may underestimate the association between perceived school climate and student attendance. In future work, developing a more nuanced understanding of how household, school, neighborhood factors shape student attendance would advance existing literature (Balfanz & Byrnes, 2018; Corcoran et al., 2016; Datar & Strum, 2006; Moricca et al., 2013; Sheldon, 2007). Assessing school climate through student surveys also presents certain limitations. Even though the survey used in this study enabled an analysis of four commonly conceptualized dimensions of school climate (U.S. Department of Education, 2019; National School Climate Council, 2019), it contains only one survey item covering socioemotional safety that some researchers contend is an important part of school climate (Wang & Degol, 2016). Student surveys also lack the perspectives of teachers and parents that may illuminate aspects of the learning environment related to physical resources, school personnel, leadership, and home-school partnerships (Berkowitz et al., 2017). Reference bias may occur in surveys inquiring about respondents' perceptions of school climate (Dee & West, 2011). As a result, differences in household-, neighborhood-, and state-level conditions may lead to varying perceptions across schools of what a positive school climate means. Students living in impoverished neighborhoods with high crime rates may give more positive assessments of a moderately safe school than might students from more affluent areas with lower crime rates. Reference bias of this nature could lead to underestimating the relationship between perceived school climate and student attendance. While this study cannot rule out this limitation, its focus on one large school district may reduce forms of reference bias that are more pronounced in state- or national-level analyses across more diverse geographical contexts. Future research is needed to determine if patterns observed in New York City are consistent in other locales. New York City's school district administers a well-developed school climate survey, but it is in the nation's largest city with many of its own specific circumstances (Nathanson et al, 2013). Previous studies have observed strong links between school climate and academic, behavioral, and other student outcomes, so it is conceivable that school climate might also be associated with large improvements in student attendance (Berkowitz et al., 2017; Hopson & Lee, 2011; Thapa et al., 2013). Yet this study offers evidence that raises questions about this potential. One possible reason for underwhelming results is that outcomes, such as student attendance, may be predominantly driven by factors that are beyond the control of schools. For example, physical health is strongly predictive of student absence (Daniels, 2006; Datar & Strum, 2006; Levy et al., 2011; Moricca et al., 2013). School climate could be inconsequential for a student who must stay home from school for a long duration because of serious illness or injury. Barriers to regular attendance related to transportation and poverty could also require extensive efforts and resources that extend beyond school grounds. In these cases, although school climate surveys may serve as diagnostic tools that inform ground-level improvement strategies, they may have less utility when connected to outcomes largely driven by factors outside of schools' locus of control. Minor associations observed in this study are largely characteristic of empirical literature examining the effects of school strategies on attendance (Balu et al., 2016; Rogers et al, 2017). Access to school transportation, delayed school start times, mentoring programs, home-school communication, and parent-teacher relationship-building initiatives indicate only small associations with student attendance (Cordes et al., 2011; Corrin et al., 2016; Parise et al., 2017; Robinson et al., 2017; Sheldon, 2007; Wahlstrom et al., 2014). At the same time, while it remains unclear what school-led approaches might produce large gains in student attendance, states are increasingly seeking action from schools by making chronic absence a part of their school accountability systems (Schanzenbach et al., 2016). To generate greater improvements in attendance rates, it is possible that a combination of home, school, and neighborhood supports may be needed. Yet determining what feasible mix of strategies could work well across diverse school contexts is likely to remain a vexing challenge. The results of this study should not be interpreted as a rejection of the value of positive school climate. Attendance is only one potential benefit of a positive school climate. School climate seems to have stronger positive effects on academic achievement, student behavior, and the efficacy of school improvement strategies (Hopson & Lee, 2011; Thapa et al., 2013). As survey assessments of the climate in schools may be beneficial for giving voice to students, it may also be argued that school climate is an indicator of worth in its own right. If students report feeling unsafe, disconnected from others, and unengaged in class, responding to how students feel may matter if schools are to provide reasonably enriching learning environments. Daniel Hamlin https://orcid.org/0000-0002-8595-2558
10.3102_0002831220929638	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220929638	Putting PjBL to the Test: The Impact of Project-Based Learning on Second Graders' Social Studies and Literacy Learning and Motivation in Low-SES School Settings	 This cluster randomized controlled trial investigated the impact of project-based learning with professional development supports on social studies and literacy achievement and motivation of second-grade students from low–socioeconomic status school districts. At random in within-school pairs, 48 teachers were assigned to the experimental or comparison group. Experimental group teachers were asked to teach four project-based learning units designed to address nearly all social studies and some literacy standards. Comparison group teachers were asked to teach social studies as they normally would except to teach a target number of lessons. The experimental group showed higher growth in social studies and informational reading but not writing or motivation. Greater consistency with project-based learning session plans was associated with higher growth in writing, motivation, and reading.	 The ideas driving project-based learning (PjBL) have a long history in American education dating back to the early 20th century during the Progressive Era (Kliebard, 2004). The progressive educator John Dewey helped popularize, at least in theory, approaches to education that were student-centered, had practical meaning and application, and, in his view, promoted democracy by providing students with more educational opportunities and by teaching citizenship (Dewey, 1902)—all characteristics associated with PjBL. Another progressive educator, sociologist David Snedden, advocated the use of practical projects to engage students in learning by doing in the field of vocational education (Snedden, 1916). William Kilpatrick (1918) encouraged the use of projects, such as designing a kite or presenting a play, in which students developed knowledge and skills and engaged in activities that, he argued, prepared them for life. Progressive educators disagreed on essential aspects of a project-based approach, but they all viewed projects as a compelling alternative to traditional instructional approaches they considered to be dry, fact-based, disconnected from students' lives, and teacher-centered. Throughout the 20th and into the 21st century, PjBL has been a presence in the educational literature. Much of the existing research on PjBL shows promise for the approach, yet there has been relatively little research testing its impact, particularly at the elementary level. Reviews of research on PjBL reveal that the majority of studies have taken place in middle and high school classrooms and have not been designed in such a way as to draw clear causal conclusions about the impact of PjBL—although they have certainly made other important contributions to our understanding of the approach (Condliffe, 2016; Holm, 2011; Kokotsaki et al., 2016; Thomas, 2000). There is a particular need to study the effects of PjBL on social studies and informational reading and writing achievement and for students from underrepresented racial-ethnic groups and students living in economic poverty. In the United States, social studies and informational reading and writing are neglected in the primary-grade school settings, particularly in low-socioeconomic status (SES) classrooms (Duke, 2000b; Fitchett & Heafner, 2010; Jeong et al., 2010; McGuire, 2007; Pace, 2012; VanFossen, 2005; Vogler et. al, 2007). Although there is no research on the degree to which students in low- versus high-SES settings experience PjBL, as detailed later, research has found that some key practices related to PjBL are less likely to occur in low-SES school settings. Given the longevity of PjBL, the promise of the approach, the lack of efficacy studies with young learners, and the need to investigate strategies for addressing inequity in certain educational opportunities, we set out to study the impact of PjBL for second graders in low-SES schools in the United States. We did so by conducting a cluster randomized controlled trial comparing social studies and literacy (in particular, informational reading and writing) achievement and motivation of students engaged in PjBL to that of students whose teachers taught social studies and literacy as they normally would except with a promise to teach a target number of social studies lessons. This study is grounded in a view of learning as driven by the desire for human connection. We learn largely in order to understand the social world, to enable our interactions with others, and to show others that we have learned. As such, the social and cultural context around our learning is paramount to whether and how learning occurs (National Academies of Sciences, Engineering, and Medicine, 2018). As would be expected, a wide-ranging body of research points to the impact of teacher-student relationships on student learning (Howard et al., 2020). Instructional approaches that endeavor to capitalize on the need for human connection are often found to be more effective than instruction that is less intentional in that regard. For example, research continues, as it has for decades, to document the benefits of cooperative structures for learning (Kyndt et al., 2013). Learners do not rest passively as the social context washes over them, taking them where it may. Rather, they are actively constructing understandings within their social contexts (Vygotsky, 1978). Indeed, our approach to social studies and literacy education is, in part, consistent with traditional premises of constructivism. However, like Fitzgerald and Palincsar (2019), we ascribe a much greater role of the social and cultural context of learning than is traditionally the case in constructivist theory. These scholars' concept of sensemaking captures our thinking well: "Sensemaking entails being active, self-conscious, motivated, and purposeful in the world. It is an activity that is always situated within the cultural and historical contexts in which we interact with others and with the aid of tools" (p. 227). They document that a common implication of instructional studies on individuals' sensemaking is the need to create the social conditions in which sensemaking is fostered, including opportunities for students to inquire, interpret, elaborate, and link new learning to their prior knowledge and experiences. Put another way, our view of learning involves elements of both sociocultural and cognitive theoretical perspectives (e.g., Danish & Gresalfi, 2018; Purcell-Gates et al., 2004). It is not simply constructivism in context, however. We are compelled by the common finding in research regarding the central role that more knowledgeable others play in fostering learners' sensemaking. For example, in a meta-analysis of 37 studies examining the impact of inquiry-based science education, Furtak et al. (2012) found that although inquiry-based pedagogy is effective in general, with an overall mean effect size (ES) of 0.50, approaches that included teacher-led activities had mean ESs approximately 0.40 higher. Similarly, in our approach to social studies and literacy education, each session includes a teacher-led component, often with explicit teaching of concepts and/or skills, in addition to a substantial block of time for primarily student-led activity, with the teacher serving as a "guide on the side." Project-based learning is an instructional approach designed to capitalize on students' drive for sensemaking (National Academies of Sciences, Engineering, and Medicine, 2018). Educational researchers have defined PjBL in varying ways (e.g., Barron et al., 1998; Krajcik et al., 1998; Thomas, 2000). In recent years, a large international consortium of education leaders and stakeholders, including researchers, have sought to define high-quality PjBL using a consensus process. They have identified six characteristics of high-quality PjBL: intellectual challenge and accomplishment, authenticity, a public product, collaboration, project management, and reflection (HQPBL, 2018). There is considerable overlap between these characteristics of high-quality PjBL and the conceptualization of learning presented earlier. Intellectual challenge and reflection foster sensemaking, and collaboration and a public product capitalize on the drive for human connection. Authenticity calls for establishing the relevance of school learning by reflecting what happens in the social world outside of school and by connecting to that world through the project's impact on other people and communities. Our conceptualization of PjBL foregrounds authenticity in PjBL because it both invites active and purposeful sensemaking and allows human connection not only within but also beyond the classroom, into other social and cultural worlds of the learner (e.g., Newmann et al., 1996; Purcell-Gates et al., 2007). We designed each of the project-based units tested in this study to have an overarching authentic purpose, such as to improve a local park or to raise money for a valued cause. Students' work then has a purpose beyond satisfying school requirements or expectations: addressing an authentic problem, need, or opportunity in their community or the larger world. We also viewed PjBL as requiring that the project be developed over an extended period of time and be the primary driver of learning throughout a unit. These features increase the likelihood that the social purpose of the project will drive learning throughout the unit and that there will be sufficient time and space for students to engage with the learning. Each activity in which students engaged during each of our project-based units is carried out not for its own sake, or because the teacher told them to do it, but rather to contribute to meeting the project's goals either directly or indirectly by developing knowledge and skills needed to carry out the project. Although we foreground authenticity in our conceptualization of PjBL, we do address other elements of high-quality PjBL (HQPBL, 2018): intellectual challenge and accomplishment, collaboration, reflection, and a public product. We seek to incorporate intellectual challenge and accomplishment through our attention to grade-level standards and complex tasks in our projects (e.g., learning about the history of a site in the community by examining photographs and conducting interviews). The structure of our daily sessions sets aside a time for collaboration as well as a time for reflection. Each of our projects involves development of one or more public products (see the Experimental Group section for further explanation). The only element from HQPBL that we do not address is project management, which did not occur to us as a priority for students this age, although it is possible that it could be. To illustrate how students learn within the project-based approach we have outlined, take the case of students learning about the map key. (As we describe this case, we use parentheses to point out the five elements of high-quality PjBL and key aspects of the view of learning we presented earlier.) In one of our project-based units, students are engaged in writing brochures about their local community to try to persuade people to move to or visit their community (authenticity). The class shares their brochures with prospective visitors to or residents of their community, for example, via a local realtor (public product, human connection). Each student's brochure persuades in part by including descriptions of the student's favorite natural and human characteristics in the community (cultural context) and is to include a map and map key (cultural and historical tools) showing where those sites are located. The teacher provides explicit instruction in what a map key is and how it works (teacher-led activity, more knowledgeable other). Students then reread an informational text about map keys in small groups. They answer questions on a handout asking them to identify particular symbols used in the maps in the text and interpret maps in the text using the map keys (collaboration, social conditions for sensemaking). They then use this experience to imagine a characteristic they might include on their own map and draw the symbol they might use for that characteristic. That process launches students in working individually or with a small group to make a full map key for their brochure and to place the key symbols in the appropriate locations on their brochure's map (collaboration, student-led activity). Students also engage in whole-group review and reflection, which can include the sharing of student work on their map keys and discussion about the progress the students have made toward the public product (reflection). At the end of this series of experiences, the second-grade students have a strong grasp of how to not only use but also create map keys (intellectual challenge and accomplishment). Fitzgerald and Palincsar (2019) pointed to the particular role that curriculum materials can play in fostering sensemaking, provided those materials are "rich enough to support this complex work" (p. 244), and curriculum materials are implicated throughout accounts of effective contexts for student learning (National Academy of Sciences, Engineering, and Medicine, 2018). In many contemporary implementations of PjBL, there is detailed guidance for teachers available, including unit descriptions and individual lesson plans. For example, Learning Reviews (2018) recommended 18 resources for "Project Based Learning Lesson Plans and Examples," such as the Buck Institute for Education (now PjBLWorks) and Edutopia. PjBL may also involve use of curriculum materials not originally designed for a PjBL context, such as textbooks in Parker et al.'s (2018) PjBL model or children's trade and school market books in ours. Scholars have long established that there is a complicated relationship between what is written in curriculum materials and what actually happens in the classroom. In a review of research, Remillard (2005) presents a framework in which a participatory relationship between curriculum materials and the teacher (e.g., teacher knowledge, teacher identity), influenced by context, leads to a planned curriculum. Context exerts its influence again, as do the students themselves, to lead to the enacted curriculum. Davis et al. (2017) identified several factors that influence how teachers enact curriculum, including the strengths and limitations of the materials, their relevant content knowledge (in their case, science knowledge), the students in the classroom, and time. Teachers adapt curriculum materials to meet their needs. Indeed, in the present study we focus on teachers' consistency with the primary ideas within lesson plans, rather than a rigid notion of "fidelity" to a lesson script, in recognition of the reality that quality instruction involves curricular adaptation. In sum, students engage in sensemaking both driven and shaped by the social and cultural context around them. PjBL seems well-suited to capitalizing on these forces in learning. Curriculum materials have the potential to support teachers in fostering students' sensemaking through PjBL, but there is a complex relationship between curriculum materials and teachers' actual enactment of curriculum, suggesting that scholars should consider the degree to which curriculum materials are taken up when examining the impact of providing project-based curriculum materials and professional development (PD). In the following section, we discuss the degree to which opportunities to learn in the ways described in the Theoretical Framework section are equally available to students across SES groups. We then discuss efforts to institute such instruction through project-based approaches, in particular in the domain of social studies education. Finally, we turn to the question of what the field currently knows about the impact of PjBL approaches on young children. Like any approach to enacting curriculum, PjBL positions teachers and learners in specific ways and conveys particular cultural values (e.g., valuing inquiry and local contexts, constraining transmission and passivity; Au, 2012; Eisner, 1985). There is no research to indicate whether this approach is more, less, or equally common in low- as compared to high-SES settings. However, there are practices associated with PjBL that have been shown to be less common in primary-grade classrooms in low-SES school settings. In over 10,000 minutes of observation in second-grade classrooms, Billman (2008) found 0 minutes devoted to inquiry in social studies in low-SES classrooms but 82 minutes of inquiry activities observed in high-SES classrooms. Similarly, Anyon's (1981) classic work found fewer opportunities for inquiry in lower SES schools. Additionally, she found that those schools positioned social studies more as a matter of facts to be remembered than conceptual understandings to be built. Strachan (2016) found that during social studies instruction, students in primary-grade classrooms in low-SES settings were less likely than those in high-SES settings to engage in student-led activities, to read or write extended text, or to write for an audience other than the teacher. As in social studies education, in literacy education there is evidence of less frequent use of practices consistent with PjBL. For example, Duke (2000a) found that first-grade students in low-SES school districts were less likely than students in high-SES districts to have opportunities to engage in literacy in the content areas, to make choices in their reading, to exert a high degree of authorship in their writing, or to read or write for audiences beyond the teacher alone. Research on teachers who are unusually effective at fostering literacy achievement in students of poverty and students of color often find that such teachers emphasize meaning making, higher order questioning, and higher order discussion to a greater extent than typical teachers (e.g., Taylor et al., 2000; Turner, 2005). Teale et al. (2007) argue that there is a literacy curriculum gap by SES such that students in low-SES settings experience more emphasis on basic reading and math skills and less attention to content building, conceptual understanding, comprehension, and writing. Like others (e.g., Center on Education Policy, 2006), they attribute this gap in part to the policies of the No Child Left Behind Act. Opportunity gaps observed in social studies and literacy curriculum and instruction are set in the context of larger opportunity gaps by race and SES. These gaps are evident not only in the practices but also in the systems, processes, structures, and policies that shape U.S. schooling (Milner, 2012). For example, research has also long indicated that educators have, on average, lower expectations for students of lower SES and students of color (Dusek & Joseph, 1983). Those lower expectations likely explain, at least in part, the relatively less intellectually ambitious instruction in school settings with high proportions of students of low-SES and students from underrepresented racial and ethnic groups. Social studies is a multidisciplinary field comprising many disciplines from the social sciences and the arts and humanities. The disciplines in the projects tested in this study, which are the disciplines most frequently taught at the elementary and secondary level (National Council for the Social Studies [NCSS], 2013), are economics, geography, history, and political science (which is called civics and government, or civics, at the school level). Collectively, social studies as a school subject is highly compatible with PjBL because of the subject's foci on helping students (1) recognize and work to address societal problems, needs, or opportunities; (2) conduct inquiry, in particular, asking and exploring complex questions about the world around us; (3) use critical thinking, problem solving, and collaborative skills; (4) explore authentic issues and problems; and (5) take informed action (NCSS, 2013). Each social studies discipline, as described in the C3 Framework (NCSS, 2013), is also compatible with PjBL. The central goals of civics and government are helping students develop (1) knowledge about government and (2) civic efficacy, or the capacity and willingness to take on the roles of citizenship. PjBL, with its attention to authentic issues and concerns, is a natural fit with civics and government. Economics deals with people's choices and reasoning regarding resources and how to make informed decisions. Projects entail complex decision making, often with limited resources such as time and materials; thus, economic decision making is often used in the process of PjBL. Geography focuses on knowledge of the Earth's human and natural characteristics—both the local and the global. Geographic thinking involves understanding the Earth's many interdependent relationships. PjBL's focus on societal problems and needs is compatible with geography, as many of today's most pressing needs are geographic (e.g., climate change; food and water security). History, with its focus on events, developments, movements, and individuals from the past (and not from students' daily lives), may seem an unlikely fit with PjBL's focus on addressing societal needs and opportunities, which tend to be contemporary in nature. However, these contemporary problems are deeply rooted in events of the past: History education provides contextual understanding. PjBL, with its focus on sustained inquiry or exploration, when used in history education, allows students to develop their skills in perspective taking, cause and effect, chronology, contextualization, and evaluation of sources, among others, to understand the causes and conditions of current issues (e.g., MacArthur et al., 2002). Each of these disciplines cannot be enacted without literacy skills, most notably informational text reading and writing. The dominant national framework for social studies education (NCSS, 2013) asks teachers to engage students in developing disciplinary literacy skills along with content knowledge via an inquiry arc. The PjBL approach that is the subject of this study involves informational text reading and writing as well as attention social studies content. Research with older students has demonstrated the suitability and promise of project-based approaches for social studies disciplines. For example, Parker et al. (2011) found, in their mixed-methods design experiment with 314 students across 12 classrooms in three high schools (eight classrooms used an alternative approach to Advanced Placement (AP) U.S. Government and Politics and four classes used a traditionally taught AP approach), that students in the alternative approach classes earned the same or higher scores on the AP exam and performed better on a complex-scenario test of deep conceptual learning, compared to students in a traditionally taught AP course. Parker et al. (2013) found, in their mixed-methods design experiment with 289 students in 12 classrooms across four schools, that students in the classrooms using an alternative approach to AP U.S. Government and Politics were more likely to earn a high pass on the AP exam than students in classrooms using traditionally taught AP approaches, and they also scored higher on a complex-scenario test, an open-ended assessment designed to assess students' abilities to apply knowledge from the course to investigating real-world problems in politics and government. The approach to PjBL employed by Parker et al. (2011; Parker et al., 2013; Parker et al., 2018) had five key principles: "rigorous projects as the spine of the course, quasi-repetitive project cycles (looping), engagement first, teachers as co-designers, and an eye for scalability" (Parker et al., 2011, p. 538). The projects included in their PjBL curriculum also followed an inquiry-based learning approach; a "master question" unified all the projects, and as students progressed through the projects, they revisited and attempted to answer the master question (Parker et al., 2013). As in our approach to PjBL, students worked both collaboratively and independently through an extended engagement with authentic issues. Problem-based learning, an approach related to though distinct from PjBL, has also been applied to social studies disciplines. For example, Saye and Brush (2007) describe the results of their 9-year research program on problem-based historical inquiry, used with a series of small samples of high school students. Across studies, the researchers concluded that their approach to problem-based learning increased engagement, empathy, and attention to sources of knowledge but did not always improve deep knowledge or critical reasoning. They attributed the success of their work to "authenticity of tasks and deliberate support for active learning" in addition to the digital tools they built to support inquiry (p. 210). Economics has also been addressed through a problem-based approach, yielding positive results (Mergendoller et al., 2006). Although these and other studies have focused on older students, there is no evidence to suggest that younger students are incapable of engaging with the real-world problems, needs, or opportunities that drive learning within a project-based framework. For example, in a justice-oriented economics unit (although not one best characterized as project-based), Sylvester (1994) found that a third-grade class could grapple with authentic social and economic issues: homelessness, entrepreneurship, economic competition, and unemployment. Mitra and Serriere (2012) found that fifth graders in a socioeconomically diverse school who learned the ABCDEs of youth development—agency, belonging, competence, discourse, and (civic) efficacy—could engage successfully in school life and civic life by identifying and addressing a local issue. Indeed, as Chi et al. (2006) argue, "In many ways, the elementary level is an ideal time to create a strong and meaningful foundation for the civic knowledge, skills, and dispositions needed to prepare and engage students as active citizens . . ." (p. 24). We provide further evidence that even young children are able to engage with authentic problems, needs, or opportunities that drive PjBL in the following section. Although a number of studies have examined the causal impact of PjBL in middle, high school, and postsecondary contexts (e.g., Boaler, 1997; Geier et al., 2008; Harris et al., 2014; Parker et al., 2011; Parker et al., 2013), finding positive impacts on learning and motivation, few studies have been conducted with younger students, particularly those in the preprimary and primary grades. Some studies of PjBL with young children have focused on effects on overall development (e.g., Habok, 2015), but most have focused on PjBL in relation to specific domains. Aral et al. (2010) examined Turkish children's acquisition of basic concepts (e.g., colors, shapes) in one classroom that used the typical preschool curriculum and another classroom in the same school in which teachers taught the concepts using PjBL (SES unspecified). PjBL was employed once per week for 12 weeks. Few other details were provided. In contrast to the other studies reviewed in this section, in this study there was no evidence of an advantage for a project-based approach. Focusing on science content knowledge, Robinson et al. (2014) randomly assigned teachers in 70 classrooms in low-income schools in the United States, Grades 2 through 5, to an experimental group who experienced a PjBL curriculum along with more than 100 hours of PD over 2 years (including a summer institute and weekly coaching) or to a comparison group who taught science as usual for the year. Although results for the full sample have not been published, Robinson et al. compared the learning gains of students labeled as gifted in both groups, concluding that those students who participated in the PjBL condition made statistically significantly greater learning gains in science process, concepts, and content knowledge than the comparison group. The randomized design allowed a strong causal inference regarding the relative efficacy of the experimental and control conditions; however, within the experimental condition it is difficult to parse out the effects of PjBL as compared to the large number of hours focused on PD that sometimes dealt with science content, technology, and differentiation as opposed to only PjBL. Also focused on science learning was a study by Dresden and Lee (2007) involving first-grade students in a low-SES school in the United States. Science learning was examined in one classroom before and after participating in a teacher-directed unit on different types of animals and then again after participating in a PjBL unit on chicks. Assessments asked students to discuss an animal of their choice—or specifically a chick following the PjBL unit—and to provide facts about that animal, as well as draw and label a picture of the animal. The researchers found that students used statistically significantly more words to describe their animal following the PjBL unit and had higher levels of detail and accuracy in their writing at that point. However, the improvements might have stemmed from the fact that the PjBL unit on chicks followed a unit on different types of animals in which important conceptual groundwork may have been set. Chicks is also a narrower topic than animals, which may have contributed to the findings. Motivation, as well as science content learning, was the focus of the Kaldi et al. (2011) study, involving students in ethnically diverse classrooms (SES unspecified) in Greece just above the primary grades (Year 4; ages 9 and 10). Using a single-group pretest-posttest design, the researchers examined students' knowledge of sea animals as well as motivation and attitude toward environmental studies following participation in a PjBL intervention lasting between 2 and 3 months in six classrooms. Interviews with teachers and students showed statistically significant pre- and posttest differences for science content learning as well as motivation in this learning domain. They concluded that the students in the study "found [PjBL] amusing and more motivational in comparison to traditional teaching methods (direct instruction, teacher talk, studying from their own textbooks)" (p. 43). Also focused on science motivation as well as learning was the Karaçalli and Korur (2014) study. In this study, which the researchers identified as quasi-experimental, 143 fourth-grade students in Turkey (SES unspecified) experienced 4 weeks of 1-hour daily experience learning about electricity in daily life. The experimental and comparison groups experienced the same presentation materials and explanations. The experimental group applied their learning in the form of an ongoing project, whereas the control group answered questions about material and prepared questions to ask of their friends. Students in the PjBL group had better achievement and retention of the material taught, but unlike in the Kaldi et al. (2011) study, did not display effects on motivation (a measure of attitudes toward science and technology). We were able to locate only two studies examining the effects of PjBL in relation to social studies learning in the primary grades. In one study, seven students ages 6 to 7 from a special education class in Turkey (SES unspecified) participated in a project-based unit for 1 to 2 weeks (Guven & Duman, 2007). Students improved in their understanding of bakeries (which could be considered social studies content) following the unit and field trip. In a second study, second-grade students in low-SES schools in the United States made statistically significant gains in social studies knowledge and informational reading and writing following engagement in two project-based units, one focused on economics and the other on civics and government (Halvorsen et al., 2012). In addition, students' postscores were statistically the same as postscores of students in high-SES schools who had not experienced the units, suggesting that PjBL may help to narrow the achievement gap. However, as in nearly all of the studies discussed in this review, this study did not use a randomized controlled trial (RCT) design that would afford a strong causal inference. Some additional studies of PjBL in the preprimary or primary grades examine teacher, student, and/or parent perceptions of the approach (e.g., Beneke & Ostrosky, 2009; Chu et al., 2011; Tretten & Zachariou, 1995) or teacher implementation. For example, Chu et al. (2011) examined teachers', parents', and students' perceptions of the impact of PjBL in science and social studies over 19 weeks on students' information technology or informational literacy (e.g., internet searching) skills on four classes of P4 (9- to 10-year olds, just outside of the primary-grade age range) students in Hong Kong. All groups thought that students' skills were improved, and students expressed that the skills were important to their work. The relatively small number of studies that have examined effects of PjBL in the primary grades have, with one exception, found evidence of promise of the approach for general development and content learning and mixed evidence of promise with respect to motivation. However, only one of the studies, focused on science, has employed an RCT design, which is best suited to drawing causal conclusions. Four reviews of research on PjBL (Condliffe, 2016; Holm, 2011; Kokotsaki, et al., 2016; Thomas, 2000) have also noted the dearth of studies with an RCT design. Our theoretical framework calls for opportunities for students to engage in sensemaking driven by their social contexts, particularly by extended engagement with authentic problems, needs, or opportunities in their community or the larger world. Our review of literature documents that such opportunities are less common in school settings with high proportions of students of color and students of low SES. PjBL can, at least theoretically, provide such opportunities and is well suited to social studies education. Studies with older students engaged in PjBL in social studies have had promising results, as have studies involving young children in PjBL largely within other domains. A causal study of PjBL in social studies in the primary grades would provide insights into whether PjBL can fulfill this promise. The present study addressed these gaps by examining the impact of PjBL on social studies and literacy achievement and motivation in the primary grades in low-SES school settings using a cluster randomized controlled trial design. The study was carried out with a sample of teachers during their first and only year of implementation who had, except for one, never carried out PjBL—among the most challenging contexts in which PjBL has ever been tested. The research questions were the following: Research Question 1: What is the impact of being in classrooms of teachers randomly assigned to implement, with some PD support, an integrated, project-based approach, as compared to business-as-usual (but with a promise to teach a target number of lessons) instruction, on the (a) social studies learning, (b) informational reading, (c) informational writing, and (d) motivation of second-grade students in low-SES school settings? Research Question 2: Among teachers randomly assigned to implement integrated, project-based units, is greater consistency with unit session plans associated with greater student learning and motivation? This study was a cluster randomized experiment in which 48 teachers were assigned randomly to an experimental (n = 24) or a comparison (n = 24) group within second grade in each school. Teachers in the experimental group were provided with one initial professional learning workshop, three follow-up recorded webinars, coaching, and detailed session plans for 80 sessions within four project-based units, one each for economics, geography, history, and civics and government. Comparison teachers were asked to teach their regular social studies curriculum (which in no case involved PjBL). They were asked, and agreed, to teach 80 lessons over the course of the year so that the amount of social studies instruction could be held constant across conditions. Teachers in both groups were systematically observed. To measure student growth, near the beginning and end of the school year, we administered pre and post standards-aligned measures of social studies, informational reading, and informational writing, and a Likert-type scale motivation survey about social studies, literacy, and integrated instruction. Participants were second-grade teachers (N = 48) and their students (N = 684; comparison group = 289, experimental group = 395) from 20 elementary schools (16 schools with two participating second-grade classrooms and 4 schools with 4 participating second-grade classrooms) in 11 school districts. Classrooms were drawn from schools that met the following criteria: (1) at least 65% of the student population qualified for free or reduced-priced lunch; (2) below state average student performance on state exams in social studies (assessed in Grade 6 in this state), reading (assessed in Grade 3), and writing (assessed in Grade 4); and (3) location within an hour's drive of either of the university sites where the principal investigators were located. The reported free or reduced-priced lunch rates of participating schools ranged from 65% to 100%, with a mean of 80.350%. All second-grade teachers within qualifying schools were invited to participate; at least two teachers in each school needed to agree to participate in order to be included in the study. Teachers were paired within second grade in each school; one member of each pair was randomly assigned to implement four units of our integrated, PjBL approach to teaching social studies and informational reading and writing (the experimental [E] group) whereas the other was asked to teach social studies using the approach they normally would during any other school year (the comparison [C] group). As detailed later in this section, for 15 of the 24 comparison group teachers, this involved using a (non–project-based) curriculum developed by two state education organizations, and for nine of the 24 comparison group teachers, this involved using a national social studies textbook series. The remaining two comparison group teachers used self-designed (non–project-based) lessons. Comparison group teachers were asked to promise to teach at least 80 social studies lessons over the course of the year, considerably more than they likely would normally have taught. In that sense, they too were participating in an intervention—to increase the amount of social studies instruction provided in schools. Table 1 provides information for evaluating initial experimental versus comparison group comparability. There were no statistically significant differences between experimental and comparison group teachers in years of teaching experience or having received PD in PjBL. Even among those reporting having received prior PD in PjBL, there was no indication from observations and questionnaires that comparison group teachers actually used a PjBL approach to teach social studies, nor, from interviews, that any but one experimental group teacher did so prior to the study year. All students within participating classrooms were invited to participate through a parent/guardian consent form. The two whole class–administered assessments were collected from all students whose parents provided consent. The two individually administered assessments were given to only a randomly selected subset of students due to budgetary and thus personnel constraints. Despite this, sample sizes at posttest for each assessment were adequate: social studies: E = 305, C = 257; reading: E = 307, C = 252; writing: E = 358, C = 270; motivation: E = 343, C = 265. A total of 47.937% percent of students had a mother or guardian with higher than a high school education. Among these students, 17.120% of students had a mother or guardian with an associate's degree. The majority of participating students, 57.048%, were from underrepresented racial-ethnic groups. Additional demographic information about students and participating teachers, as well as students' baseline/preassessment scores, can be found in Table 1. As this is a cluster randomized experiment, with teachers randomly assigned to condition, data reported in Table 1 are all aggregated and t tests conducted at the teacher level (the unit of random assignment). Measures are after attrition took place. As the independent samples t tests show in the last column of Table 1, the experimental and comparison groups were comparable on average in terms of demographic variables and preassessments. Variances on preassessments in the two groups were also statistically equivalent for all measures as determined by F tests. Thus, we can assume that the randomization of our study was realized as intended. It is noteworthy that there was no attrition at the teacher level. In terms of student attrition rate, the overall attrition rate was 7.895%. The differential attrition rate for the experimental group was 9.367%, and 5.882% for comparison group—a difference of less than 4 percentage points. Although the attrition rate for the experimental group was higher than that of the comparison group, the descriptive analysis and baseline equivalence of covariates from before attrition were very similar to those reported in Table 1. That is, overall the sample of participating students was similar to the sample of students initially assigned to experimental or comparison conditions. Combined with the low overall and differential attrition, we find no evidence that attrition had any influence on our estimation of the treatment effect. The four project-based units used in this study were designed to involve students in PjBL as described earlier in this article. We used a design-based research approach to develop the units, field testing and obtaining feedback from teachers (not involved in the present study) throughout the process (see Halvorsen et al., 2012; Halvorsen et al., 2018, for a description of the methodology). The four PjBL units, taught in the following order, were (1) Producers and Producing in Our Community (economics), (2) Brochure About the Local Community (geography), (3) Postcards About the Community's Past (history), and (4) The Park/Public Space Proposal Project (civics and government). Each unit involved a project that had an authentic purpose with a public product developed in part through collaboration on intellectually challenging tasks with regular opportunities for reflection—all characteristics of high-quality PjBL discussed earlier in the article (HQPBL, 2018). Although the unit and session plans were premade and the same for all classrooms, they were unlike traditional lesson plans in that they were written to embed opportunities for connections to the local community in which the unit was taught and for teacher and student voice and choice. Every project involved teacher and student voice and choice and opportunities for extended informational text reading and writing. The project for the economics unit involved creating an informational flier about a local business for that business' use and creating and selling their own good or service to raise money for a cause. The business chosen, the good or service created and sold, and the cause were all decided by each class. The geography project involved developing a brochure to persuade people visiting or considering settling in the local community that it has compelling natural and human characteristics. The local community varied by district, and the natural and human characteristics were chosen by each child—for example, one child might choose to feature the local athletic center, whereas another might choose to feature the local public library. In the history unit, the project involved students developing postcards about the history of the local community to display or sell in a local institution, such as a library or historical society, with the teacher and/or students deciding which historical sites to feature, whether to sell or display their postcards, and the location(s) where postcards were shared. The civics and government project involved developing a proposal, conveyed in letters and in a group presentation, to persuade the local city government to make improvements to a local park or other public space selected by the teacher and/or teacher and students collectively. See Supplemental Appendix A for abstracts of each project (in the online version of the journal). In addition to characteristics of PjBL described earlier in the article, projects included explicit instruction threaded throughout the units but always presented in the service of the project rather than as material to be learned for the sake of satisfying school requirements. Projects also involved domain-specific, research-supported instructional practices and were closely aligned to standards. Specifically, units addressed nearly all social studies standards for the state, which were largely aligned with national standards (the C3 Framework; NCSS, 2013) and a subset of standards from a national English Language Arts and Literacy standards document (National Governors Association Center for Best Practices & Council of Chief State School Officers [CCSSO], 2010), particularly those involving informational reading and writing. However, it was understood that, unlike the social studies standards, the literacy standards should also be addressed in other parts of the day/outside our units, including in reading, writing, and science instruction. Each of the four units comprised 20 sessions designed to take approximately 45 minutes of instructional time each. (We use the term sessions rather than lessons because only a portion of each session is what might traditionally be considered a lesson, much of the session time involved small-group and individual work on the projects.) We designed session plans to clearly indicate learning objective(s) and standards addressed, any materials required, key vocabulary terms and definitions critical to the sessions, instructional steps of the session, and additional notes for the teacher (e.g., potential pitfalls to avoid). With few exceptions, each session followed a similar format: (1) whole-group instruction and discussion to generate and sustain student interest and excitement about the project as well as to provide explicit instruction (~10 minutes); (2) guided small-group or individual instruction in which students have opportunities to work individually, in pairs, or in small groups (~20–30 minutes); and (3) whole-class review and reflection, which included clarifying any confusions and reviewing key terms (~10 minutes). For example, a session might involve the teacher reading aloud a text related to the unit project, with instruction in social studies content as well as literacy skills, such as how to use an index. In small groups, students might then use information learned from the text and other materials to complete portions of a graphic organizer that would guide their writing of the unit's final product. Then students might then come back together to share their graphic organizers and review with the teacher key content from the beginning of the session. In addition to unit plans, teachers were provided with any texts, artifacts, or other materials, beyond typical school supplies, that were needed to carry out each unit. Although we recognize that PjBL is initially challenging to implement (e.g., Marx et al., 2004), we were cognizant of the limited amount of support many districts or schools are likely to provide when introducing a new curriculum when a research team and grants are not involved. In an attempt to maintain a high level of ecological validity, we were relatively austere about the amount of outside-the-classroom PD provided with the PjBL units: (1) 3 hours of initial PD that introduced participants to PjBL, to our research initiative, and to the first project-based unit; (2) three recorded webinars ranging between 22 and 40 minutes introducing the next three units; and (3) added for a subset of the classrooms, a brief five-minute video of several experimental teachers discussing strategies for addressing some common challenges with units. In contrast, inside the classroom we did provide substantial support in the form of, on average, 11 visits from research assistants (RAs) who provided coaching for unit implementation after the session they observed, with additional communications, as necessary, by phone and/or e-mail. We believed that coaching support had a high degree of ecological validity given the prevalence of instructional coaches in high-poverty school districts. Coaches interacted with teachers only after sessions (during sessions they were taking observation notes, as explained later in this section) and were instructed to restrict their interaction with teachers to implementation of what was in the unit or session plans, rather than larger issues of instruction or classroom management that may affect PjBL implementation. Finally, the project unit and session plans that we developed had a high degree of detail regarding the structure and content of the project sessions and included educative curriculum features, such as child-friendly definitions of key terms. Scholarship has demonstrated that curriculum materials have the potential to serve as a form of PD in their own right (Davis & Krajcik, 2005; Drake, et al., 2014). Teachers signed a letter of consent in which they committed to teaching 80 lessons over the course of the year, but the mean number of lessons/sessions taught by experimental group teachers was 65.917, with a standard deviation of 9.184 and a range of 48 to 86. Four teachers, two in the experimental group and two in the control group, taught the full 80 lessons/sessions. In general, experimental group teachers who did not teach a full 80 lessons/sessions did not teach the civics and government unit (n = 6), taught an abbreviated version of that unit (n = 13), or taught an abbreviated version of the history unit (n = 13), but did teach up to four review sessions we provided. As indicated previously, teachers in the comparison group were asked to teach social studies as they normally would during any other school year except to increase their instruction to a goal of teaching 80 social studies lessons over the course of the year. Of the 24 total teachers in the comparison group, 15 teachers taught social studies using a curriculum developed through two state education organizations by educators from school districts and subject area consultants and aligned to the state social studies standards. Typical units in this curriculum comprised several open-ended questions to guide inquiry during the course of study, key vocabulary concepts, and a series of one to nine lesson plans. Common activities included reading aloud children's literature, writing anchor charts, class discussion, small-group activities, analyzing maps or timelines, video clips, vocabulary work, worksheets, and assessments. None of the units was project-based. Two teachers using these units supplemented them with magazines (Social Studies Weekly; Scholastic News), two teachers added an extended teacher-created unit at one point in the year, and two other teachers improvised all text-based lessons because they were not provided the texts called for in the unit plans. Seven of the remaining nine teachers not using the curriculum described in the previous paragraph utilized district-created lessons or social studies textbooks as the primary mode of instruction, including TCI (Social Studies Alive!), MacMillan/McGraw Hill, and Scott Foresman. The social studies textbooks were not specifically aligned with this state's standards, but there appeared to be considerable overlap with state expectations. Lessons consisted of discussing content vocabulary, reading the textbook, watching videos, completing worksheets or written assignments, whole-class discussion, and small-group work. The remaining two comparison teachers taught self-designed lessons as their schools did not provide any social studies curriculum or materials. Much like the lessons designed by the two state organizations, teacher-created lessons typically consisted of vocabulary instruction, whole-class discussion, read-alouds, independent reading, graphic organizers, visual aids, group work, and written activities. Neither the textbook-based instruction nor the teacher-designed instruction was project-based. Teachers signed a letter of consent in which they committed to teaching 80 social studies lessons over the course of the year. As noted earlier, two experimental group teachers and two comparison group teachers actually taught that number of lessons. The mean number of lessons taught by comparison group teachers was 51.375, with a standard deviation of 17.118 and a range of 30 to 85. This is statistically significantly fewer lessons than taught by the experimental group teachers (M = 65.917 lessons, t = −15.217, p < .001). However, as explained in the Discussion section, dosage analyses indicate that the 14.5-lesson difference in mean number of lessons taught is not sufficient to explain the advantage of the experimental group over the comparison group in study results. Our four outcome measures were (1) a standards-aligned social studies assessment administered one-on-one, (2) a standards-aligned informational reading assessment administered one-on-one, (3) a writing assessment comprised of a group-administered paper-and-pencil persuasive-writing assessment and informative/explanatory writing assessment; and (4) a group-administered paper-and-pencil motivation survey. All measures were developed by our team due to the lack of social studies or informational reading and writing assessments aligned with state standards and the lack of a motivation measure that addressed social studies, informational reading and writing, or integrated instruction. Sample items from each assessment are provided in the paragraphs that follow (space limitations preclude appending the instruments, but they are available upon request from the first author). Validity and reliability of each assessment are reported in the paragraphs that follow. Students were assessed near the beginning and end of the school year. Items from all assessments were piloted and refined before administration. The social studies assessment was aligned with state content expectations and the C3 Framework (NCSS, 2013). Ten items with multiple subparts measured student achievement in economics, geography, history, civics and government, and public discourse, decision making, and citizen involvement. Some questions were open-ended, such as "What services does the local government provide?" and "Why do we use time lines?" Others were close-ended, such as showing a map with a key and asking, "Tell me which direction you would go to get from the child's house to the park?" and a question that required students to sort pictures of items involved in the production of pizza into the categories of natural, human, and capital resources. Each item corresponded to all or part of a state standard for social studies for second grade. Without knowledge of whether a given assessment came from a child in the experimental or comparison classrooms (i.e., blind to condition), we scored the responses of the 11 questions on scale of 0 to 3, with a score of 3 indicating fully meeting the standard, for a total possible score of 30 (two questions measured the same standard and were thus averaged for one score for the standard, for a total of 10 items). To examine assessment validity, five reviewers with expertise in social studies were asked to identify the question(s) that best aligned with each content expectation; they had 96% agreement with our determination of the alignment of standards and assessment questions. With regard to reliability, project members established a high inter-rater reliability at Fleiss's kappa = 0.883 for scoring the assessment, and the 10 social studies items had an acceptable internal consistency (α = .715). This assessment comprised a total of 31 items that measured student achievement of six of the 10 second-grade CCSS for Reading Informational Text (Standards 4–9). Sample questions included "What are reasons the author gives to support her point?" (CCSS for Reading Informational Text 8) and "What is the writing under a picture called?" (CCSS for Reading Informational Text 5). The research team scored questions blind to condition on a scale of 0 to 3 with a score of 3 meaning fully meeting that CCSS expectation. This provided a total possible score of 87 (not 93 because one trio of questions all dealt with one text feature and therefore were scored together on the 0–3 scale). To examine validity, five experts in the field of early literacy reviewed the assessment and were asked to identify which CCSS in Reading Informational Texts corresponded with each assessment item. There was 95.5% agreement between these experts' reviews and our own identification of which CCSS best addressed each assessment item. With regard to reliability, research team members established a high interrater reliability of Fleiss's kappa = 0.874 when scoring this assessment, and items had high internal consistency (α = .863). This assessment measured student achievement of writing for two distinct purposes detailed in the CCSS: to opine or persuade (Writing Standard 1) and to inform or explain (Writing Standard 2). This prompt asked students to write independently for 30 minutes about "something you think people should change and why." Students were given a purpose and audience for the writing: "My friends and I will read what you write to get ideas about things we should try to change" and were provided with a list of potential areas of change. Responses were scored blind to condition using a rubric aligned to expectations in CCSS Writing Standard 1 for second grade as follows: introduction (on a scale of 0–2), opinion (0–2), reasons (0–3), linking words (0–1), and concluding statement (0–2), for a total possible score of 10. This prompt asked students to write an article for up to 30 minutes about a community job (e.g., firefighter) for a class magazine. This topic was chosen because it was not addressed in the project-based units so would not inappropriately advantage students in the experimental group and because students would likely to be able to draw on considerable background knowledge/information in responding (thus it would serve as a test of informational writing skill, not knowledge/information). Students were provided with a list of potential jobs. Their responses were scored blind to condition using a rubric aligned to expectations in CCSS Writing Standard 2 for second grade: introduction (0–2), information (0–3), definition (0–1), and concluding statement (0–2), for a total possible score of 8. An overall informational writing achievement score was created by combining scores for responses to the persuasive and informative/explanatory prompts for a total score of 18. With regard to validity, an interrater reliability of Fleiss's kappa = 0.734, which is considered high, was established by project members for scoring of this assessment. Internal consistency reliability was borderline (not surprising in prompted writing assessment) at .661. To investigate motivation, we developed a survey modeled after, but distinct in all items from, a validated reading motivation survey (e.g., McKenna & Kear, 1990). We surveyed students about their attitudes toward engaging and participating in (1) social studies learning, (2) literacy learning, and (3) integrated social studies and literacy learning (there were also items on PjBL, but those were not included in analyses given that students in the comparison group did not participate in PjBL). Students were read 24 statements such as, "When I use maps to learn new things, I feel . . ." and "When our class learns about social studies and reading at the same time, I feel . . ." After each statement, they were asked to circle one of four images of a character, depicting an emotional state ranging from "very happy" to "very upset." Responses were scored on a scale of 1 (very happy) to 4 (very upset). Cronbach's alpha reliability for the assessment was .884. Other data collected include students' demographic/background information (underrepresented racial-ethnic group, gender, and mothers'/guardians' education level), teacher background characteristics (years of teaching experience and whether they received PD in PjBL), and interviews with experimental group teachers (with the interviews not included in this article except with respect to teachers' responses regarding their experience with PjBL prior to the data collection year and number of sessions taught; see Revelle, 2019). In order to most meaningfully address the first research question, we needed to ascertain that the experimental group was using a project-based approach and the comparison group was not. Thus, classrooms in both conditions were observed with a protocol that included the item "Degree to which the lesson appears to be set in project-based context" on a scale of 1 to 3 (from does not appear to be set in project-based context to appears to be set in project-based context). To address the second research question, we needed to know the degree to which experimental group classrooms were implementing PjBL as intended. For that purpose, our observation protocol had three items for observations in experimental classrooms requiring "Ratings for consistency with session plans," one each for whole-group instruction and discussion, guided small-group or individual instruction, and whole-group review and reflection. Each item was rated on a scale of 1 to 3, with 1 = follows fewer than 50% of the steps in the session plan for that section of the session, 2 = follows 50% to 80% of the steps, and 3 = follows 80% or more of the steps for that section of the session. "Follows" meant that the teacher engaged in the primary action identified in the step (e.g., explaining a concept, providing information about or communication from the target audience; allocating time for students to carry out research). Raters were directed to count steps as partially followed if part of a complex step was followed and not to give a lower rating for an altered order or addition of steps but only for steps being missed entirely during that session (e.g., not inviting students to reflect on the process of producing the good or service). The raters did not take into account the degree to which teachers adhered to specific suggested wordings in the plans or the like. As explained earlier, our session plans were unlike traditional lesson plans in that they were written to embed opportunities for teacher and student voice and choice and connections to the local community in which the unit was taught. For example, one session plan calls for giving students an opportunity to generate questions they want to include in their survey about the park or other public space that is the focus of the project. Given that the park or other public space will vary and the questions students want to ask in the survey will vary, it makes sense for our ratings to focus on consistency with the general instructional move called for rather than wording or other details of enactment of that move. The observation protocol was used by RAs, who observed full sessions (their coaching conversations with teachers occurred after the sessions; teachers perceived them as having the dual roles of observing instruction and providing [only] project-related coaching support as needed). RAs were trained in using the observation protocol using videos. Live observations of sessions by two raters achieved a mean interrater reliability of .658 in Fleiss' Kappa, which indicates substantial agreement. In total, RAs carried out an average of 11.208 and 5.458 visits to experimental and comparison classrooms respectively. We used descriptive statistics to examine student achievement and motivation in the experimental and comparison groups and inferential statistics (t tests) to determine any significant differences in raw scores on preassessments of student achievement and motivation between students in the experimental and comparison groups. We also generated descriptive statistics regarding consistency with unit session plans in the experimental group. To take into account the nested relationships in the study (i.e., students nested within teachers), we used hierarchical linear models (Bryk & Raudenbush, 1992). Using a two-level hierarchical linear model (Level 1: student and Level 2: teacher), we explored the effects of the intervention (controlling for female status, underrepresented racial-ethnic group, mother's/guardian's education, and preassessment) on social studies achievement, informational reading, informational writing, and motivation and, for the experimental teachers, the relationship between consistency with unit session plans and social studies achievement, informational reading, informational writing, and motivation. The two-level model matches the research design and is appropriate for the data. This analytic strategy and the detailed data we collected about instruction in the experimental classrooms meant that analyses could examine not only the impact of the project-based units by condition but also whether students showed greater gains in social studies achievement in classrooms in which the teacher implemented project sessions with a higher degree of consistency with unit session plans. First, we examined the treatment on treated effects of the intervention (i.e., using the analytic sample of students). The first-level model for student i of teacher j is[MATH] where Yij represents four outcomes of interest (i.e., social studies learning, informational reading, informational writing, and motivation) for student i of teacher j. FEMALEij is a dummy variable for gender, and UNDERREP is a dummy variable for underrepresented racial-ethnic group status. MOTHER EDUij is equal to 1 if a student's mother/guardian has higher than a high school diploma. PRE_Yij is the preassessments of the outcome. A student-specific residual is εij. At the second level the teacher-specific intercepts are modeled as β0j = γ00+γ01(EXPERIMENTAL)j+µ0j in which γ00 is the average outcome of students in the comparison group and µ0j is a teacher-specific random effect. The variance of µ captures the nesting of students within teachers. EXPERIMENTAL is a dummy variable equal to 1 if a student was in the experimental group. The coefficient γ01 represents the average difference in the outcome between the two groups (adjusted for covariates). Second, we examined the relationship between consistency with unit session plans and the outcomes. As explained earlier, each major component of each session observed was rated on a scale of 1 to 3, for a total score of 9 for a session that was quite consistent with key components of the session plan and a total score of 3 for a session that was not. The model used for the analysis was the same as the previous one shown in the previous section except that (1) we dropped the EXPERIMENTAL variable, (2) included only experimental group students for the analysis, and (3) added the CONSISTENCY variable at the teacher level (i.e., the second level). Our report of results is organized into two sections. The first addresses the first research question (about impact) and the second addresses the second research question (about relationship between consistency with session plans and student growth). All experimental group teachers attended initial PD, received the project unit and lesson plans, and accepted coaching. All teachers implemented at least three out of four of our project-based units; all but six teachers implemented all four units to at least some degree. As intended, none of comparison group teachers did any of these things. Observation data indicated that teachers randomly assigned to the comparison group did not implement project-based-learning (mean score of 1.1 on the 1–3 scale described earlier). Collectively, these data reflect several types of fidelity identified by Hill and Erickson (2019). Put another way, the experiment tested what it was designed to test. Descriptive statistics for experimental and comparison group students for all variables used in the multilevel analysis are reported in Table 2 at student level. Results of the multilevel analyses are reported in the paragraphs that follow. Controlling for female status, underrepresented racial-ethnic group status, mother's/guardian's education, and preassessment, the experimental group scored statistically significantly higher than the comparison group on the social studies measure (ES = 0. 482, p < .001, two-tailed here and throughout). That is, the mean difference between experimental and comparison groups in social studies was 0.482 standard deviations even after controlling for baseline scores. In line with relevant methodological research (e.g., Ho et al., 2007), the Institute of Education Sciences What Works Clearinghouse (2014) considers an ES of 0.25 or higher to be "substantively important" (p. 23). See Table 3. Controlling for female status, underrepresented racial-ethnic group status, mother's/guardian's education, and preassessment, the experimental group scored statistically significantly higher than the comparison group on the informational reading measure (ES = 0.182, p = .083). That is, the mean difference between experimental and comparison groups in informational reading was 0.182 standard deviations even after controlling for their baseline scores. By itself (without considering potential cumulative effects of PjBL also being used for informational reading in other parts of the school day), this ES is lower than the 0.25 threshold. See Table 3. Controlling for female status, underrepresented racial-ethnic group status, mother's/guardian's education, and preassessment, the experimental group did not score statistically significantly higher than the comparison group on the writing measure (ES = −0.047, p = .571). See Table 3. Controlling for female status, underrepresented racial-ethnic group status, mother's/guardian's education, and preassessment, differences between the experimental group and the comparison group did not reach a level of statistical significance (ES = 0.135, p = .193). See Table 3. Descriptive statistics for teachers' consistency with unit session plans are provided in Table 5. Higher ratings mean that instruction was more consistent with key components of the session plans. In classrooms with the lowest average consistency with unit session plans, there was significant reduction or elimination of one or more session components (and recall that each session contributed to students' enactment of the project), sometimes because of consistent disruptions to instruction due to off-task classroom behavior. Whole-class teaching was often substituted for the requested guided small-group or partner instruction, perhaps in part due to struggles with classroom management. There was often little to no time at the close of a session for whole-group review and reflection. In contrast, in classrooms with the highest average consistency with unit session plans, instruction was well paced and offered students time with whole-group instruction, regular participation in collaborative work with partners and small groups, and time for collective review and reflection as suggested in the session plans. Higher consistency with unit session plans was associated with higher scores on all measures (see Table 4), with the following p values and ESs—all above the previously cited Institute of Education Sciences 0.25 threshold for substantive importance: social studies (ES = 0.270, p = .301), reading (ES = 0.583, p = .030), writing (ES = 0.239, p = .065), and motivation (ES = 0.287, p = .016). Results of this study suggest that PjBL can be an effective way to bolster student achievement in social studies and informational reading. PjBL as tested in this study led to a 63% gain in social studies as compared to the comparison group. Translated into months of a school year (63% multiplied by a 9-month school year), that represents 5 to 6 months of greater learning. PjBL also led to a 23% gain in informational reading, representing approximately 2 months of greater learning (this smaller impact for informational reading was expected given that, as compared to social studies, informational reading and writing were addressed in fewer sessions and should be addressed in other parts of the day as well). An important implication of the study is that curriculum developers and practitioners should not shy away from using a project-based approach, at least as enacted in this study. There were benefits of using the approach even in teachers' first year of implementation and even as compared to comparison classrooms using either state-developed or national curriculum materials. The efficacy of PjBL in this study is consistent with theoretical perspectives reviewed earlier in this article. Learning is driven by a desire for human connection (National Academies of Sciences, Engineering, and Medicine, 2018), so we can catalyze learning through a curricular approach characterized by attention to social problems, needs, and opportunities; frequent collaboration with peers and others; and development of products with a public audience. Children are active and purposeful sensemakers but not in a vacuum; their sensemaking is shaped by the social and cultural context and by interactions with more knowledgeable others, particularly the teacher (Fitzgerald & Palincsar, 2019; Vygotsky, 1978). As in the example of learning about map keys, presented early in this article, the teacher provides teacher-led activities, including explicit instruction, to support student learning. The teacher also creates considerable space for student-led experiences and collaboration and ensures that there is an authentic purpose and resulting public product of students' intellectual work. The teacher also facilitates students' whole-group reflection about both their progress toward the public product and the ways in which the skills they are learning are relevant in achieving their goal. In other words, young children learned in our approach to PjBL through a combination of teacher-led and student-led activities all driven by an authentic purpose. Much empirical research would also lead us to expect positive effects of PjBL. As detailed earlier in the article, most studies of PjBL have shown promising effects, including in both science and social studies (e.g., Marx et al., 2004; Parker et al., 2011; Parker et al., 2013) and with young children (e.g., Robinson et al., 2014). As explained earlier in the article, conceptualization of the specific disciplines within social studies reveals their affinity with PjBL. Even history, although not as obviously suited to PjBL, provides opportunities to consider the historical roots of contemporary life and engage in sustained inquiry with a range of social artifacts (e.g., MacArthur et al., 2002). A subset of studies of the impact of PjBL has involved students in low-SES school settings with a high proportion of students from underrepresented racial and ethnic groups (e.g., Geier et al., 2008). Nonetheless, our study makes an important contribution to the literature in several respects. First, our study examines the effects of PjBL at the intersection of social studies and primary-grade education, which only two prior studies have done. The first of those studies examined only learning related to bakeries (Guven & Duman, 2007), and the second addressed only two of the four core social studies disciplines for the elementary years (Halvorsen et al., 2012). Neither study used a strong causal design, which points to a second contribution of our study: the degree to which it enables causal inference. Our study employed a cluster randomized controlled design using a large sample of classrooms. There was no attrition at the cluster level, and attrition at the student level was low. Our post hoc tests for baseline equivalence of observed covariates using the analytic sample suggested that random assignment was successful by and large and in agreement with the intention of the research design. In addition, attrition was not a threat to the internal validity of the results because its rate was low and because the students, teachers, and schools that eventually participated in the experiment in either the treatment group or the comparison group were very similar to those who initially participated in the random assignment process. The ecological validity of our study enhances its methodological contribution. As would be true in many school settings, participating teachers had little to no experience with PjBL. Teachers received limited amount of outside-of-classroom support, with a 3-hour initial PD workshop and minimal subsequent webinar-based PD (~100 minutes total). This is ecologically valid as group PD time is relatively limited in high-poverty districts, and social studies is likely to be a low PD priority. Teachers were provided with more in-classroom support, with an average of 11 visits from a coach. This support is also ecologically valid in that high-poverty districts often have a cadre of instructional coaches. However, in order to ensure that we were testing implementation of PjBL and not a general effect of coaching support, coaches played a limited role. They did not coach (e.g., model instructional practices) during sessions and were instructed to restrict their postobservation conversations with teachers to implementation of what was in the session-by-session unit plans, rather than larger issues of instruction or classroom management. In other words, we aimed to maximize ecological validity and minimize confounding factors. Still, it is important to recognize that what we tested was PjBL with PD supports and not simply providing PjBL unit or session plans alone. A fourth contribution of our study is that it was carried out in high-poverty, low-performing school districts with a sample that included many students whose mother or guardian had no more than a high school education and in which the majority of students were from underrepresented racial-ethnic groups. Gaps—or chasms—in educational opportunities (Milner, 2012) for students living in economic poverty and students from underrepresented racial and ethnic groups extend to many practices consistent with a PjBL approach. Studies discussed earlier in the article suggest that students in low-SES settings have fewer opportunities to engage in inquiry, to engage in student-led activities, to experience higher ordering questioning or discussion, to read or write extended text, to make choices in their reading, to exert a high degree of authorship in their writing, or to write for an audience other than the teacher (e.g., Anyon, 1981; Billman, 2008; Taylor et al., 2000; Turner, 2005). In fact, they are less likely to experience content area instruction or reading and writing therein (e.g., Center on Education Policy, 2006; Duke, 2000a; Pace, 2012). We have demonstrated, in a causal design, that when students in low-SES school settings do have the opportunity to engage in such practices, significant learning in multiple domains occurs. The study adds to the work drawing into serious question the No Child Left Behind Era emphasis on basic reading and math skills at the expense of content building, conceptual understanding, comprehension, and writing (Center on Education Policy, 2006; Teale et al., 2007). Policymakers and practitioners have further reason to address the discrepancies in educational practices in low- versus high-SES settings documented in previous studies. Our study also speaks to the complexity of the relationship between curriculum materials and teacher practice (Remillard, 2005). As in some past PjBL research, such as that of Geier et al. (2008) in science, we predetermined the focus of each unit (in that study, e.g., one unit was, "What Is the Quality of Air in My Community?) and provided considerable teacher support via detailed curriculum materials. We also made use of materials aimed directly at students, such as books for a school market, as in some past PjBL research (Parker et al., 2018). Although curriculum materials provided considerable scaffolding, we were able to design them such that they allowed practitioners to tailor aspects of projects to their local community and to teacher and student interests. An implication of the study is that curriculum designers and researchers could emulate this approach to curriculum materials. Although all teachers in the study were provided with the same curriculum materials, we documented that the consistency of their practice with those materials varied considerably. For example, the plans might call for the teacher to provide information about or communication from the target audience (though what that audience was would vary by classroom), but some teachers might skip that part of the session. Or the plans might call for the teacher to engage students in reviewing key points from an earlier session, but the teacher chose not to do so. Curriculum research discussed earlier would lead us to expect this, as many factors influence the enacted curriculum (e.g., Davis et al., 2017). In our research, the degree of consistency between teachers' practice and the curriculum materials turned out to be consequential for students' learning and motivation, as has been found in many past studies (see Hill & Erickson, 2019, for a review). Overall, our intervention did not have a statistically significant impact on students' informational writing. However, implementing more of the steps in the project-based unit session plans was associated with higher year end informational writing achievement, controlling for preassessments and other factors. Similarly, although we did not find a statistically significant overall effect on the study's motivation survey—a result that may be seen as surprising in light of claims and some prior evidence about the positive motivational benefits of PjBL—the more consistent implementation was with unit session plans, the more positive the associated change in students' motivation, at a level of statistical significance. Further research should investigate factors that enable implementation of PjBL curriculum materials in a manner that best fosters informational writing and motivation development. Given the findings in this study regarding consistency with unit session plans, future research might examine factors that enable and constrain teachers to greater or lesser enactment of key features of the design of project-based units or, more broadly, factors that characterize the practice of teachers whose students experience higher and lower growth within a project-based approach. Qualitative data collected as part of the project reported in this article are analyzed in relation to these issues in Toledo et al. (2018) and Revelle (2019). A related implication of this research is that policymakers and administrators should consider how to provide appropriate PD support around PjBL. This test of PjBL occurred with limited workshop-based PD: just 3 hours of initial PD and ~100 minutes of subsequent webinar-based PD. However, it did involve an average of 11 visits from instructional coaches (although, as explained earlier, their coaching was considerably constrained compared to typical coaching support). We do not know whether PjBL would have been successful without these supports or whether it would have been more successful with additional supports; future research could examine these questions, as well as the impact of our approach to PjBL at other grade levels. Although internal validity of the study is strong in many respects (see previous section), a potential threat to the internal validity in the study was the fact that the comparison teachers taught, on average, 14.5 fewer social studies lessons/sessions than experimental group teachers despite requests from the researchers and promises by the teachers to teach the same number requested of the experimental group teachers. Although this difference is statistically significant, it does not appear that it could explain the results of the study. The relationship between the number of lessons/sessions taught and social studies growth was 0.011 and the relationship for reading was 0.008. In contrast, the ESs for achievement in each of these areas were 0.482 and 0.182, respectively. Within the range of number of social studies lessons/sessions taught in this study, it does not appear that the number of sessions is an influential variable. The measures employed in the study might also be seen as a limitation in that they were researcher-developed. As noted, using researcher-developed measures for social studies and informational reading and writing was necessary at the time the study was conducted because no standardized tests were available that were aligned with standards we were using. For motivation, there were also no extant measures that addressed social studies, informational reading and writing, or integrated instruction. To help mitigate the use of researcher-developed measures, we employed a number of mechanisms to establish validity and reliability, described in the Measures section earlier in the article. A potential limitation related to the external validity of our results regards the sample involved in the study. The 11 school districts and 20 elementary schools in our sample were selected using convenience sampling, which does not define a target population. That is, our data do not represent the entire population of schools, teachers, or students in our geographic area and thus our results may not indicate an accurate depiction of the total population of teachers and students in second grade. We did not sample randomly from among all districts and schools in the geographic area or even among the subset of districts and schools meeting our selection criteria (high levels of poverty and a history of low achievement on state assessments). For example, we did not consider small, rural school districts that may have had only one qualifying school. Schools, teachers, and students who participated in our experiment may be different from other schools in the same area. As a result, there is reason to be cautious about generalizing our results beyond the schools, teachers, and students who were part of our experiment. Another limitation of the study is that we tested one specific version of PjBL, described in detail earlier in the article. This instantiation may differ in important ways from others' visions of PjBL. For example, our projects involved addressing specific standards, including explicit instruction, and making use of domain-specific, research-supported instructional practices (Graham et al., 2012), all characteristics that are typically not emphasized in the PjBL literature. We also provided PD support directly related to the units and provided detailed unit and session plans, which is not the case in all enactments of PjBL. However, nearly all comparison group teachers also had the support of instructional materials—either a (non–project-based) curriculum developed by two state education organizations or a national social studies textbook series—and had experience in using them in previous years (which the experimental group teachers did not). Still, it is possible to conclude from this study not that PjBL is always an effective instructional approach but rather than it can be effective and was, with regard to social studies learning and informational reading, effective in the manner in which we operationalized it. Furthermore, we cannot be sure which aspects—or all aspects—of our version of PjBL were responsible for the positive effects found. Finally, results of this study might have been different had we examined PjBL under less demanding conditions. We conducted the study in districts and schools facing many challenges, and we collected data in teachers' first year of implementation (as compared to comparison classrooms in which teachers had prior experience with the instructional materials and approach that they were using). Teachers' first year teaching any approach is likely to be less effective than subsequent years, and certainly in the case of an instructional approach as complex as PjBL. Indeed, Marx et al. (2004) found that the effects of a project-based approach to science education that was implemented over a 3-year period increased over time. Had we carried out random assignment when we did but waited a year, or two or three, to actually collect pre- and posttest data from students, we might have obtained larger effects. Our project-based units centered curriculum on authentic social problems, needs, or opportunities; valued collaboration and reflection; and engaged students in challenging intellectual work toward a public product. Students of low SES less often experience curricular opportunities such as these. Yet when provided with such opportunities, as in this study, results are positive, even in the first year of teacher implementation and a rigorous randomized evaluation. Implementation more consistent with the project-based units as designed showed particular promise. There is sufficient evidence to continue implementation and investigation of PjBL in the primary grades in low-SES settings as a means to address the often-neglected domains of social studies and informational reading and writing and foster learning by tapping into students' drive to connect with and make sense of their social world.
10.3102_0002831220930199	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220930199	What Teachers Want: School Factors Predicting Teachers' Decisions to Work in Low-Performing Schools	 Attracting and retaining teachers can be an important ingredient in improving low-performing schools. In this study, we estimate the expressed preferences for teachers who have worked in low-performing schools in Tennessee. Using adaptive conjoint analysis survey design, we examine three types of school attributes that may influence teachers' employment decisions: fixed school characteristics, structural features of employment, and malleable school processes. We find that teachers express a strong preference for two malleable school processes, administrative support and discipline enforcement, along with a higher salary, a structural feature. Estimates indicate these attributes are 2 to 3 times more important to teachers than fixed school characteristics like prior achievement. We validate our results using administrative data on teachers' revealed preferences.	 Along with compelling research that finds teachers influence student achievement gains more than any other school-based factor (Chetty et al., 2014; Jackson, 2012; Koedel & Betts, 2007; Rivkin et al., 2005; Rockoff, 2004; Jackson et al., 2014), recent research suggests that recruiting and retaining effective teachers to serve in the lowest performing schools is integral to the success of reforming those schools (Dee, 2012; Henry et al., 2020; Papay & Hannon, 2018; Strunk et al., 2016; Sun et al., 2017). After the disappointing results from initial federal investments in comprehensive school reform, studies in California, Tennessee, Massachusetts, and Ohio found positive effects of school turnaround reforms that required substantial teacher replacements (Carlson & Lavertu, 2018; Dee, 2012; Johnson & Heal, 2017; Papay & Hannon, 2018; Player & Katz, 2016; Strunk et al., 2016; Sun et al., 2017; Zimmer et al., 2017). Moreover, several studies suggest that schools replacing more staff also produced larger positive effects than schools replacing fewer staff (Dee, 2012; Strunk et al., 2016; Sun et al., 2017). In addition, a study of Tennessee's district turnaround networks, known as Innovation Zones, found that nearly 40% of their initial positive effect can be explained by hiring more effective teachers (Henry et al., 2020). The same study found that after initially hiring effective teachers, high rates of teacher turnover suppressed potential positive effects in schools taken over by Tennessee's statewide turnaround model, called the Achievement School District (ASD). These studies highlight the importance of teacher recruitment and retention in turnaround schools; however, prior studies have also shown that low-performing schools are hard to staff. Several studies, many of which rely on administrative data, show that teachers leave high-poverty, high-minority, and low-performing schools at much higher rates than other schools (Guarino et al., 2006; Hanushek et al., 2004; Hughes, 2012; Redding & Henry, 2018). In addition, teacher turnover itself has been shown to have overall negative effects on student achievement (Hanushek et al., 2016; Ronfeldt et al., 2013), effects that are larger and more consistent for within-year turnover (Henry & Redding, 2020). However, the effects of overall teacher turnover should be considered in light of studies that find that positive effects can be produced by intentional efforts to replace existing staff with more effective teachers (Dee, 2012; Strunk et al., 2016; Sun et al., 2017). In the current study, we use an innovative survey design known as adaptive conjoint analysis to provide insights into school attributes that can affect teachers' labor market decisions focusing on the expressed preferences of a sample of teachers who have chosen to work in low-performing schools targeted for a school turnaround effort in Tennessee known as the ASD. More specifically, we examine expressed school attribute preferences using a sample of three different groups of teachers—(1) teachers who, during the year of our survey, worked at an ASD school, (2) teachers who previously worked at an ASD school prior to our survey year, and (3) teachers who worked in a school that was eventually taken over by the ASD, but left the school before takeover. Since only the first two groups of teachers have experience working in a school operated by the ASD, we refer to these three groups in our sample as ASD-affiliated teachers. We argue that research utilizing our sample of ASD-affiliated teachers has implications for the preferences of teachers in other states' low-performing and turnaround schools because Tennessee's turnaround efforts have significant overlap with a number of other states' reforms—both in the types of schools targeted for reform and the turnaround interventions. For instance, like most other states' turnaround efforts, Tennessee identifies the 5% of its lowest performing schools (Aragon & Workman, 2015). As of 2016, Tennessee and 28 other states employed a turnaround strategy where schools can be removed from the local district and placed in a state-run district (Jochim, 2016). Similar to reforms in Florida, Illinois, Louisiana, Massachusetts, Michigan, and North Carolina, these schools are then managed outside of the local district, either through an entity like the ASD or authorized external operators, which in Tennessee are authorized charter management organizations or CMOs (Henry et al., 2014; U.S. Department of Education, 2012). The ASD model combines these two management structures by selecting some schools to be directly managed by the ASD and others to be managed by a CMO. In addition, Tennessee's approach aligns with multiple states that have implemented reforms under the School Improvement Grant (SIG) program. The SIG program, a signature initiative under the Obama administration, is one of the largest federal investments in educational grants in U.S. history (Dragoset et al., 2017). The SIG program awards grants to states to implement one of four reform models (transformation, turnaround, restart, or closure) in their lowest performing schools. Consistent with SIG requirements, Tennessee's approach utilizes strategic staffing practices, which includes replacing the school leader and teachers (U.S. Department of Education, 2018). Moreover, like Tennessee, many other states implementing SIGs emphasize hiring and retaining high-quality teachers and principals, due to the compelling evidence supporting their influence on student achievement (Chetty et al., 2014; Jackson, 2012; Koedel & Betts, 2007; Rivkin et al., 2005; Rockoff, 2004; Jackson et al., 2014). Since our results are drawn from a sample of teachers who have worked in low-performing schools, both before and after the ASD turnaround interventions began, they are most relevant to the ASD context. However, given the meaningful parallels we have described between Tennessee's ASD and turnaround efforts in other states, we believe our results also have implications for other low-performing schools that will or have already undergone similar turnaround reforms. Henceforth, we refer to our results as relevant to low-performing school settings targeted for turnaround, but note that they are most applicable to turnaround models similar to the ASD (described in more detail below). While a robust literature has examined teacher preferences more generally (e.g., Horng, 2009; Ladd, 2011), few studies have explicitly examined teachers with experience in low-performing school settings. Teachers who have shown they are willing to work in these schools may have distinctly different concerns from those who do not have experience in these environments. We contribute to this sparse literature with more information on the specific population of teachers who are willing to work in low-performing and turnaround schools. To do so, we use adaptive conjoint analysis, an innovative survey method that has not previously been utilized to examine expressed preferences of teachers in low-performing schools. Our method helps to overcome the limitation of other survey methodologies in that it requires the respondent to rank order their preferences. A rank order of preferences is useful for teacher recruitment and retention because it not only shows which attributes matter but also clearly distinguishes which attributes matter more than others. Therefore, we are able to identify which school attributes could be most useful in recruiting ASD-affiliated teachers to a new school or in retaining ASD-affiliated teachers in their current school. Supported by SIGs and Race to the Top funding, and authorized through an NCLB (No Child Left Behind) waiver, Tennessee began in 2012 to place some of the state's chronically lowest performing 5% of schools into a statewide district called the ASD (Henry et al., 2014; Glazer et al., 2019). Under the ASD model, these turnaround schools were removed from their local districts and placed under the management of either the ASD itself or authorized CMOs that were granted autonomy over school operations and given significant additional resources from federal and philanthropic sources. ASD schools also replaced the principal and most of the existing staff (Henry et al., 2014). We note that during the time period of this study, all but one ASD school was located in Memphis with the remaining school in Nashville. Since teacher replacement is integral to the ASD intervention model, it is important to clarify teacher recruitment and retention in this context. Our sample is composed of teachers who have experience working in a school that would be part of the ASD either before or after ASD interventions began. Inferences made from our results apply only to turnover among teachers in low-performing schools targeted for turnaround. Using this sample of teachers, we refer to turnover as teachers leaving their current school and retention as these same teachers staying in their current school. Teacher recruitment refers to attracting teachers with experience in low-performing and/or turnaround school settings and not the process of bringing new teachers into the profession or the recruitment of teachers from non-low-performing schools. That is, our analysis examines school attributes that may be useful for recruiting or retaining teachers who are in a similar setting/context as ASD-affiliated teachers. We propose a conceptual framework that defines three types of school factors that may attract or repel teachers from certain schools or teaching positions: (1) fixed school characteristics, (2) structural features of teachers' employment, and (3) malleable school processes. Fixed school characteristics (e.g., student composition or commute times) include less readily-altered features that can only be changed over a longer time frame by changing attendance zones or altering student and parent choice mechanisms, including converting a school into a magnet or charter. Structural features include salary, tenure, and performance-based pay and are generally set for longer periods. These are often subject to regulations and are likely applied to all schools managed by the same organization (e.g., all schools in the same school district). Malleable school processes are those for which the locus of control is expected to be within the school and under the control of school administrators. Malleable factors can be changed by school administrators in the short term (e.g., consistent enforcement of student discipline policies). Prior research has also shown that personal factors are highly predictive of teacher mobility, in particular age, gender, and family characteristics (Grissom et al., 2016), but this study seeks to address school conditions that influence teachers' mobility decisions. Prior studies of factors correlated with teacher mobility heavily influenced our typology. Multiple studies agree that certain fixed school characteristics like neighborhood characteristics, student race, socioeconomic status, and achievement predict teacher mobility patterns (Borman & Dowling, 2008; Boyd, Lankford, et al., 2011; Grissom, 2011; Guarino et al., 2006; Hughes, 2012; Johnson et al., 2012). These studies found that schools with larger populations of minority and/or lower socioeconomic students with lower average student achievement experience higher rates of teacher turnover or teachers were less likely to transfer to these schools. Research in current and former turnaround schools in Massachusetts found that teacher recruitment materials described the students as having "significant challenges," in order to target teachers committed to working in this type of school context (Simon et al., 2015; Simon et al., 2019). Another characteristic, which we categorize as a fixed school factor, that is correlated with teacher preferences for a school placement is commute time. While many studies lacked data to correlate teacher turnover with commute time, studies of job applications in Chicago and job preferences among California teachers both found that teachers prefer shorter commutes (Engel et al., 2014; Horng, 2009). Among structural features, salary has garnered the most attention. Correspondingly, many studies have found that higher salaries are associated with lower probabilities of teacher turnover (Clotfelter et al., 2008; Hanushek et al., 2004; Hendricks, 2014; Podgursky et al., 2004). Relatedly, some studies have also linked performance-based pay with improved retention (Springer et al., 2016; Swain et al., 2019) but others report mixed effects (Dee & Wyckoff, 2013). Tenure is another contested structural feature of employment that many researchers hypothesize can be used to attract and retain teachers (Rothstein, 2014; Vergara v. State of California, 2015). A recent study on the effect of tenure reform in Louisiana found that the removal of tenure protection is associated with increased teacher turnover, especially concentrated among teachers in the lowest-performing schools (Strunk et al., 2017). Research on malleable school processes has shown that teachers' turnover decisions are highly responsive to the day-to-day working conditions in a school. For example, prior research found that teacher collaboration and collegiality, student disciplinary policies, professional development (PD) quality, expectations for working outside of the school day, and support from school administrators were correlated with teachers' employment decisions and job satisfaction (Johnson & Birkeland, 2003; Johnson et al., 2012; Johnson et al., 2016; Simon et al., 2019). The positive association between higher levels of administrator support and lower rates of teacher turnover has been examined in a number of studies including a meta-analysis of 34 teacher mobility studies (Borman & Dowling, 2008). Administrator support might also be particularly salient in the ASD context, which included principal replacement. Other studies have also confirmed that expectations for working outside of the school day are correlated with both intended and actual teacher turnover decisions (Ladd, 2011). One study in particular noted that school administrators clearly communicated long work hours to job candidates in order to find teachers who were prepared to work under those expectations (Simon et al., 2019). Another malleable school process that has been shown to be correlated with lower teacher turnover is classroom autonomy, including autonomy to choose instructional materials, methods, and assessments (Achinstein et al., 2010; Guarino et al., 2006; Johnson, 2006). Previous research also suggests that class size and school safety are malleable factors that are either important to teachers or correlated with teacher turnover (Horng, 2009; Loeb et al., 2005). These kinds of malleable school processes might be even more influential in low-performing schools. Prior research from Massachusetts suggests that higher teacher turnover rates in schools serving high proportions of low-income and minority students could be explained by lower levels of administrative support, poorer teacher relationships, and weaker school cultures (Johnson et al., 2012). In the current study, we extract 16 attributes of schools from the research cited above and examine preferences among teachers who have shown a willingness to work in low-performing and turnaround schools, a subset of teachers for which only a nascent literature base examines preferences (e.g., Springer et al., 2016; Swain et al., 2019). Focusing on this understudied sample of teachers, we specifically address the following research questions: Research Question 1: What are the preferred school attributes expressed by teachers who have experience in low-performing or turnaround schools affiliated with the ASD? Research Question 2: To what extent do teachers with experience in low-performing or turnaround schools affiliated with the ASD say that they prioritize fixed school characteristics, structural conditions, and malleable school processes? The factors that predict teacher mobility decisions are traditionally studied using either qualitative methods or surveys asking teachers to rate their preferences. These traditional survey methods can result in less distinct differences between factors if teachers can rate multiple, if not all, factors as highly important. Adaptive conjoint analysis (henceforth ACA) is a survey design that addresses this concern. Originally developed for marketing research, several recent studies of teacher mobility decision-making utilized ACA because of multiple distinct advantages (Horng, 2009; Robinson, 2012). First, the ACA format asks respondents to choose between different attributes of a school profile such that they must express relative preferences. This process clarifies how teachers weigh different trade-offs between teaching positions and shows which factors appear to be more important than others. Second, ACA has been frequently tested by researchers within the marketing community, showing that this method can reliably differentiate between respondents' preferences (Green et al., 1991; Tumbusch, 1991). Third, this analysis quantifies the likelihood that teachers will choose schools with particular attributes or sets of attributes by calculating likelihood measures that are easily interpretable, making them attractive metrics to researchers and practitioners interested in more refined data on the choices teachers make between different schools. Before outlining the study methods, we note that this study does not aim to advance the ACA methodology, generally, or to explicitly show how our version of the ACA survey increases validity or reliability over prior uses of ACA to assess teacher preferences. Rather, since prior studies have utilized samples of traditional public school teachers in nonturnaround settings (Horng, 2009) and preservice music teachers (Robinson, 2012), our study extends these prior works to a novel sample: teachers working in low-performing schools prior to and during turnaround reforms. Below, we describe our process for designing and implementing our survey along with the procedures for data analysis. For brevity, we describe the most important information on design, analysis, and methodological decisions in the article. We include greater detail and illustrative examples of the survey analysis and measures in Supplemental Appendix A in the online version of the journal. We utilized Sawtooth software (https://www.sawtoothsoftware.com/) to construct an online ACA survey that comprises 16 school attributes, or school characteristics that may affect teachers' decision to work there. For each attribute, we included two or three attribute levels. For example, the PD attribute has two levels: either the school has opportunities or does not have opportunities for high quality PD. Table 1 displays each attribute, attribute type, attribute levels, and the research base that supports its inclusion in the survey. We chose these attributes using an iterative process that included reviewing the literature on predictors of teacher mobility (see Table 1); considering school attributes that are relevant to teachers in a low-performing school context; accounting for the reasons provided by ASD leadership regarding why teachers seek employment in ASD schools (e.g., classroom autonomy, involvement in establishing a school); and conducting cognitive interviews with teachers who have worked in low-performing schools similar to the ASD schools in our sample (details on the cognitive interviews are available in Supplemental Appendix A in the online version of the journal). While our review of the literature suggests that more than 16 attributes are needed to comprehensively assess teacher preferences, we had to limit the number of attributes to make the survey short enough for us to obtain an acceptable response rate. The study by Horng (2009) heavily influenced our ACA survey design. We included eight of the 10 attributes that were part of the Horng (2009) study. Enhanced software allowed us to include more attributes than the prior study, allowing for more nuance and complexity in the ACA survey than was present in the Horng study. Following the advice of Horng (2009) and Sawtooth, we limited each attribute to two or three levels. While we would have preferred greater nuance between the attribute levels, we simplified the levels to maintain a reasonable survey length. Similar to our process for selecting the 16 attributes, we chose the attribute levels based on prior literature, contextual relevance, and advice from teachers in our cognitive interviews. For many attributes, we chose levels that represent direct contrasts between two logical extremes. For example, the administrator support attribute was separated into two levels: consistent and inconsistent support. Other attributes with directly contrasting levels include family income, involvement in establishing a school, tenure, school safety, classroom autonomy, student discipline policy, PD opportunities, time, and teacher relationships. For a few attributes, we chose levels that were not necessarily direct contrasts to help shorten the survey and present realistic options available to teachers in Memphis. For example, most Memphis schools tend to be either very low or very high performing. Thus, for the student achievement attribute, we chose two levels: less than 20% or greater than 50% of students scoring proficient. We excluded the level for 20% to 50% proficient because these schools are rare in Memphis, and when directly asked, none of the teachers in our cognitive interviews suggested the middle level was necessary to accurately capture their preferences. Likewise, racial segregation in Memphis schools led us to choose the attribute levels of less than 10% or more than 50% White students. Finally, we chose the numerical values of the attribute levels for commute time, salary, and performance-based pay to reflect realistic expectations for teachers in our sample (often in consultation with teachers during the cognitive interviews). For example, $5,000 performance pay bonuses are potentially available for teachers moving into ASD schools (Springer et al., 2016; Swain et al., 2019). Using a unique link to the survey on Sawtooth software, respondents filled out the ACA survey in four stages: (1) rank the attribute levels in order of desirability; (2) indicate the importance of one attribute level over another; (3) select along a continuum between two different school profiles; (4) enter a number between 0 and 100, indicating their likelihood of choosing to work at a school with given attributes. In the first stage, respondents answered the prompt "Please rate the following in terms of their desirability," for each attribute level on a 7-point scale from Not Desirable to Extremely Desirable. See Figure 1 for an example of what an item on the first section of the survey looks like for the two levels of the teacher relationships attribute. This first section established which level of each attribute respondents preferred (e.g., whether they preferred to work in a high-performing school or a low-performing school). The respondent was only asked to rate the desirability of the attribute levels where the preference for each level cannot be assumed. We assumed that, all else equal, the vast majority of teachers would prefer a shorter commute, higher salary, a safer school, consistent discipline enforcement, smaller class size, consistent administrative support, and high-quality PD. Therefore, we did not include these attributes in the first section. Making these assumptions and not including these attributes in the first section greatly reduced the length of the survey without compromising the validity of the results because the second section of the survey asked respondents about all attributes. In the second section, the respondent answered the prompt "If two teaching jobs were identical in all other ways, how important would this difference be to you?" using a 7-point scale ranging from Not Important to Extremely Important. See Figure 1 for an example of an item from the second section of the survey on school safety. By comparing the attribute levels relative to each other for all 16 attributes, the respondent provided information on which attributes are important to her decision-making process. Using this information, the software was more likely to present attributes in the third and fourth stages that respondents identified as more important in the second stage. In the third stage, the paired trade-off section, respondents were asked to choose between two job profiles. Two job profiles were placed on the page such that one job profile was listed on the left while the other was on the right side of the page. These two job profiles were designed by the software to include both desirable and undesirable attribute levels based on the respondents' answers in the first two sections. Respondents answered the prompt, "If these teaching jobs were identical in all other ways, which option are you more likely to choose?" using a 9-point scale ranging from Strongly Prefer Left to Strongly Prefer Right with Indifferent in the middle of the scale. For an example of what the two job profiles might include, see Figure 1. By asking respondents to choose between two profiles with both desirable and undesirable attribute levels, the software obtained more refined information on the trade-offs that the respondent was willing to make. The software continuously updated the estimated relative preference of each attribute and attribute level after each paired trade-off item, choosing the next job profile based on the prior answers. Respondents were asked 18 items in this section of the survey: 10 items with two attributes on each school profile and eight items with three attributes on the school profiles. This stage allowed the software to obtain a more refined measure of the respondent's relative preferences for each attribute and attribute level. In the last section of the survey, respondents were given a job profile (created by the software) with six attribute levels listed and given the prompt, "How likely would you be to choose this teaching job?" To answer the question, the respondent is instructed to "Please type a number between 0 and 100 where 0 means Definitely would NOT choose and 100 means Definitely WOULD choose." They were asked five of this type of item to calibrate the likelihood the respondent would choose a particular job profile (Orme, 2014). See Figure 1 for an example of a job profile that a respondent might have be given. The calibration information was combined with the rank order preferences from the first three sections to produce a measure called the respondent's utilities. Below, we define utilities and describe how the utilities produced by Sawtooth are used in our analysis. We use the ACA results to obtain three measures we will discuss in the results and validation sections: importance scores, first choice preferences, and shares of preference. We include a broad overview of each of the measures below including how they are calculated and interpreted. We have provided a simplified example of how each of these measures is calculated in the Supplemental Appendix A in the online version of the journal. First, the Sawtooth software used survey responses to output part-worth utilities for each respondent. Part-worth utilities are interval measures that rank the attribute levels within each attribute. Within each attribute, a higher part-worth utility is more preferred. Part-worth utility values were calculated by proprietary Sawtooth software using a hierarchical Bayesian model (see Supplemental Appendix A in the online version of the journal for more information) and while not easily interpretable, they are used to calculate the three measures we discuss in this study. The first measure we present in this study's findings are importance scores. Importance scores are calculated using the following formula:[MATH](1) Range refers to the typical calculation of the distance between the highest and lowest values. The numerator represents the range of part-worth utilities across the attribute levels within one attribute. The denominator is the sum of the ranges of all 16 attributes. The intuition behind importance scores is that attributes with a larger range of part-worth utilities are more important to the respondent than attributes with a smaller range of part-worth utilities (see the example in Supplemental Appendix A, for more information about why the range would be related to relative importance). The importance scores are interpreted as the difference that each attribute could make in the total utility of a job. That is, importance values can be used to directly rank and statistically compare different attributes in the study. To obtain aggregate importance scores for each attribute, we average the importance scores across all respondents. To include information about uncertainty of importance scores, we report 95% confidence intervals of each importance score. The 95% confidence intervals are calculated using the full distribution of importance scores for each individual to calculate standard errors robust to nonconstant residual variance. We compare importance scores and 95% confidence intervals graphically to examine the relative rank of importance scores compared to each other. The average importance scores can be used to compare attributes (e.g., salary vs. commute time), but they do not describe the preference for the levels within an attribute (e.g., the student race attribute will receive an importance score, but this number does not indicate whether the preference is for a school that is 10% or less White vs. a school that is more than 50% White). In order to quantify and compare teacher preferences for each attribute level, we use first choice market simulations (FCMS) to ascertain teachers'first choice preference. Market simulations are designed to predict respondents' choices when they are in real-world conditions. In a market simulation, we set each attribute to a specific attribute level. After creating at least two complete job profiles, FCMS predicts the percentage of respondents who would select each job profile. In this study, we utilize FCMS by setting 15 of the attributes at the same level with the variation between job profiles focusing on the difference in levels in one attribute. The FCMS are conducted for each attribute separately. For example, to estimate the first choice preference of a school that is 10% or less White versus a school that is more than 50% White, we create a FCMS with two job profiles that have exactly the same levels set for 15 of the attributes, so the two job profiles only differ on the student race attribute. The two job profiles are then compared by summing the part-worth utilities for each of the 16 attribute levels separately for the two job profiles. The respondent is predicted to select as the first choice the job profile with the larger sum of part-worth utilities (i.e., the job profile that maximizes the respondent's utility). After obtaining first choice preferences for each respondent from the FCMS, we calculated the percentage of respondents who would choose a particular attribute level as their first choice. For example, we would report an aggregated first choice preference value of 70% for a school with less than 10% White students if seven out of 10 respondents are predicted to have this school as their first choice. We also utilize the FCMS to compare job profiles with differences in multiple attributes, predicting what percentage of respondents would select one job profile over another. For this type of market simulation, we utilized data gathered from school performance reviews (SPRs) conducted at all ASD schools during the 2014–2015 school year to construct the relevant job profiles (more information in the Results section below). As part of our validation analysis conducted after reviewing the main results, we utilize results from another type of market simulation: share of preference (SOP). Unlike FCMS, which assumes the respondent would choose the job profile that maximizes their utility, SOP indicates which job profile is preferred over the others and the relative desirability of the other job profiles. SOP is calculated using the sum of the part-worth utilities, just like with the FCMS. For SOP, the summed part-worth utilities for each job profile are exponentiated (the part-worth utilities are created using the logit rule as part of the hierarchical Bayesian analysis). The SOP is then calculated for each job profile just like a proportion. For instance, when comparing two job profiles, A and B, we calculate the SOP using the following formula:[MATH](2) In this formula, α represents the part-worth utility for each attribute level of job profile A. All 16 selected levels' part-worth utilities are summed and exponentiated. The SOP compares the value for job profile A versus the total of values for job profiles A and B. Respondents that show a very high preference of a certain attribute will have SOP that are close to 1, showing that the alternate job profiles have little utility to that respondent. Our target population for the survey includes current teachers who can be categorized into one of three groups: (1) teachers at an ASD school in the 2014–2015 school year (the time of survey administration), (2) teachers who previously worked at an ASD school prior to the 2014–2015 school year, and (3) teachers who worked in a school that was eventually taken over by the ASD but left the school before takeover. Therefore, each set of teachers has intimate knowledge of teaching in a low-performing school. We identified our target population of teachers using a combination of personnel data provided directly by the ASD and statewide longitudinal data compiled by the Tennessee Education Research Alliance. The final sample included 811 teachers: all 2014–2015 ASD teachers (N = 565) and a random sample of the other two groups of teachers (N = 246; comparisons between those randomly selected and not selected indicate the randomization process produced balanced samples as shown in Supplemental Appendix Table B1 in the online version of the journal). To reflect the sampling strategy, all estimates in this paper include a probability selection weight where the responses from the 2014–2015 ASD teachers are given a weight of one, because they were all included in the sample. The other two groups of teachers' responses are given a probability weight of approximately 1.62 in order to weight the sample to be representative of the target population. Teachers were given approximately 6 weeks to take the online survey in the spring of 2015, and teachers who completed the survey were sent a gift card. Survey reminders were emailed on a weekly basis to nonresponders. The final response rate was 63.5% or 515 teachers, with current ASD teachers having a higher response rate (68.8%) than the response rate of teachers who no longer worked at ASD schools (52.5%). When comparing respondents and nonrespondents, we noted that teachers with more years of experience (6 or more years) were less likely to respond to the survey than teachers with fewer years of experience (1 or 2 years). To address this pattern of nonresponse, we created a nonresponse weight to down-weight respondents with 1 to 2 years of experience and up-weight respondents with 6 or more years of experience such that the weight creates proportions that are representative of the original sample. The weights are approximately 0.86 for 1 to 2 years of experience, 1 for 3 to 5 years of experience, and 1.15 for 6 or more years of experience. We found no other significant differences between other observable characteristics of the sample (see Supplemental Appendix Table B2 in the online version of the journal). The probabilities of selection weight and nonresponse weight were multiplied to create the final weight. The overall attribute importance scores are shown in Figure 2. The vertical black line indicates what the attribute importance score would be if all attributes were equally important to the respondents (100/16 = 6.25). Among teachers who were surveyed, the most important attribute out of the 16 we tested is enforcement of the student discipline policy followed by salary, administrator support, and school safety, as shown on Figure 2. The high relative importance of administrative support and school safety is consistent with Horng (2009) who found clean and safe school facilities, administrator support, and class size to be the most important attributes. The least important attribute in our study is student race followed by student income, involvement in establishing a school, student achievement, time spent working outside of the school day, and commute time. Since importance percentages are ratio measures, we can observe that the most important attributes in this study are two to three times more important to our sample of teachers than the least important attributes. For instance, the importance percentage for commute time is 4.76, and the importance percentage for discipline is 9.47. Therefore, discipline is almost two times (9.47/4.76 = 1.99) as important as commute time. Generally, attributes categorized as fixed school characteristics are clustered at the lower end of importance percentages while attributes categorized as structural and malleable characteristics are dispersed throughout the ranking of important characteristics. Next, we conducted FCMS for all 16 attributes separately. We conducted 16 FCMS with two to three school profiles in each simulation (the number of school profiles matches the number of attribute levels within each attribute) where the school profiles are identical on 15 of the attributes, only differing on the level of one attribute. The results reflect the percentage of respondents who are predicted to select each school profile. The results are displayed in Figure 3 with each bar (i.e., attribute) representing the results from one simulation. For six of the malleable attributes, the respondents are highly sensitive to the difference between levels such that 95% to 100% of respondents would select a school with consistent administrator support, consistently enforced student discipline, safety as a minor concern, small class size, supportive teacher relationships, and available high-quality PD over a school with the alternative for each attribute. For the structural conditions attributes, the only one that had over 95% of the first choices was the ability to make $8,000 more per year (which had 100% of first choices). The first choice percentages are less stark for fixed school characteristics, with commute time as the exception. The vast majority, 94%, of respondents were predicted to select a school with tenure available. Moreover, 92% of respondents were predicted to select a school with some sort of performance-based pay (combining those preferring small incentives of $1,000 or less to larger incentives of $5,000 or more). Respondents are less resolute on which level of the other fixed school characteristics attributes (student race, student income, establishing a school, and prior achievement) they would prefer when deciding to work at a certain school. We were interested in how ASD schools would fare if teachers were hypothetically choosing between existing ASD schools to mirror labor market conditions. In the fall of 2014, the ASD and CMO staff conducted a SPR at each ASD school, and the summaries were made available to the research team. The SPR summaries were created using the same template, each addressing five of the attributes included in our ACA survey. In particular, each SPR summary allowed us to ascertain if (1) high-quality PD was available, (2) teacher relationships were supportive, (3) the discipline policy was consistently enforced, (4) safety was a minor or serious concern, and (5) administrators were consistently supportive of teachers. Given the nature of the SPR, it only included malleable attributes that school administrators can directly influence. We coded each SPR to determine the levels of these five attributes for each school. In particular, we found that many schools had the preferable level (according to the FCMS) on all of the attributes except for one. To see how teachers would pick between four schools that had the more-preferred level on four of the five attributes and the least-preferred level on the fifth attribute, we conducted a market simulation. Among these four malleable factors, 58% of teachers are predicted to select a school that did not have high-quality PD but had consistent discipline, safety is a minor concern, and supportive administration. The attribute that was strongest in deterring teachers from choosing a school as their first choice was inconsistent administrative support with only 5% choosing these schools, followed by inconsistently enforced discipline (7%) and safety as a serious concern (30%). These results help order the attributes in terms of their impact on the expressed preferences of ASD-affiliated teachers and allow more precise distinctions to be made among these malleable attributes than the original importance percentages from Figure 2. While the ACA design theoretically offers a substantial improvement over traditional survey designs when assessing teacher preferences for school conditions, it is possible that teachers' expressed preferences do not match the attributes of schools where they will choose to work. The ACA survey results are most useful if they predict the actual behavior of teachers in the labor market, but there are potential reasons why teachers' expressed preferences could differ from their revealed preferences. For example, teachers who express a desire to teach in schools with a large proportion of non-White students may be influenced by social desirability, and they may choose an assignment with fewer underserved minorities when given the opportunity. Therefore, it is useful to examine the extent to which teachers' expressed preferences match their revealed choices. To this end, we perform a series of validity checks using administrative data to assess if the expressed preferences from the survey are indicative of the teachers' revealed preferences when they choose to either transfer or stay in their schools between 2014–2015 (when the ACA survey was administered) and 2015–2016. Note, we are somewhat limited in our ability to do a comprehensive set of validity checks for several reasons. First, out of our 16 attributes, we are only able to find 11 approximate matches using administrative data. The five attributes which we did not have administrative measures to use for validation include commute time, involvement in establishing a school, opportunities for tenure, performance-based pay, and expectations to work outside of school hours. Although the lack of administrative measures for these five attributes are a limitation for our validity checks, they also highlight why our survey provides insight into teacher preferences with measures that are typically unavailable in administrative data. Another limitation is that the school characteristics we obtain from administrative data do not always directly correspond to the ACA survey measures, because the administrative data is not structured in the same manner as the survey. Moreover, we cannot assess revealed preferences of teachers who decide to teach in another state or a private school because our administrative data only includes teachers in Tennessee's public schools, and we only were able to match 364 of the 515 respondents to a school in both the 2014–2015 and 2015–2016 school years. In order to assess whether the sample used to validate expressed preferences is representative of the full sample of respondents, we used t tests to compare importance values for each attribute between the 364 respondents matched to administrative data and the 151 respondents not matched (see Supplemental Appendix Table B4 in the online version of the journal). We compare importance values instead of demographic characteristics, because these demographic variables are not available for respondents who were not matched to the administrative data. Overall, the t tests show that matched and nonmatched respondents reported similar importance values, with only one attribute that is significantly different: class size. Even though the importance values for class size were statistically different, the values are substantively similar (6.7 for matched respondents vs. 7.2 for nonmatched respondents). This analysis provides some evidence that the 364 respondents used to validate expressed preferences provided similar survey responses to the respondents that could be not matched with the administrative data. We created 11 measures of school characteristics based on information on school demographics, class size, teacher salary, student discipline, and student achievement. We also have results from the Tennessee Educator Survey, which contains several scales and items on classroom autonomy, administrative support, discipline policy, PD, and teacher collegiality. More information on these data sources are included in Supplemental Appendix C in the online version of the journal. When possible, we calculated the attribute levels for each school using the same values stated on the ACA survey (i.e., for demographics, salary, test scores, and class size). For example, since the attribute levels for student race on our survey is either schools with less than 10% or more than 50% White students, we directly aligned the survey and administrative data and identified whether teachers' schools had less than 10% or more than 50% White students. When no exact values were stated on the ACA survey, we use the administrative data to identify whether a school is above or below the mean value of that attribute relative to the rest of the state. For example, the ACA survey attribute levels for administrator support is either consistent or inconsistent support. Since this dichotomy is not on the Tennessee Educator Survey, we identify a school as having more or less consistent support if average teacher ratings of administrative support in that school are above or below the state mean, respectively. We have several options for categorizing expressed preferences from the ACA survey including importance values and results from the market simulations. Our preferred strategy is to use results from the market simulations that indicate the respondents' SOP for one attribute level over the other. This value represents the probability a respondent will choose a school with one attribute level over the other attribute level. In the spirit of only including respondents who show a strong expressed preference (i.e., excluding respondents with only a slight preference who might not actually make decisions based on that attribute), we include respondents who express a SOP of at least 90%. We also include other tests using importance values (in their top five or their number one attribute according to the importance values) and looking only at higher versus lower SOP. The detailed results are in Supplemental Appendix C in the online version of the journal and are generally consistent with the results discussed here. A summary of the results of this exercise exploring revealed and expressed preferences are listed in Table 2. The first column lists the 11 attributes we were able to test. The second column indicates the number (and percent) of respondents where the actual conditions of the teachers' schools in the 2015–2016 school year matched the preferences they expressed on the ACA survey. The third column is the number (and percent) of respondents whose revealed preferences (the conditions at their school in the 2015–2016 school year) did not match their expressed preferences on the survey. For eight of the 11 attributes, the majority of teachers were at schools with attributes that matched their expressed preferences. A particularly high rate of match between expressed and revealed preference occurs for schools with less than 10% White students. 95% of the sample expressing a strong preference for majority non-White schools worked in a school that matched that description. A skeptic may have assumed that respondents' expressed preferences for minority White schools was driven by social desirability, but the validity check would suggest otherwise, at least for the teachers in our sample. For three attributes, teacher revealed preferences do not match their expressed preferences: salary, safety, and flexible autonomy. Of these, we might expect that teachers have less flexibility on salary when they are moving within the same state's public school system. Moreover, teachers in the sample tended to be paid more between 2014–2015 and 2015–2016 (see Supplemental Appendix Table C4 in the online version of the journal). However, the values in Table 2 only counts matches as cases where teachers receive a pay increase of $8,000 or more and not cases where the salary increase is less than $8,000. Since the average salary difference between 2014–2015 and 2015–2016 in our sample is $1,611, an $8,000 pay increase is relatively large and less often observed. If we match teachers who express a preference for higher salary (regardless of amount) with teachers who do receive a higher salary between the 2 years, the match between expressed and revealed preferences is much higher. For example, Supplemental Appendix Table C4 in the online version of the journal shows that among those who were working in a new school in the 2015–2016 school year who had a strong expressed preference for $8,000 of additional salary, two thirds received a higher salary in their new school. Moreover, our measure of the safety attribute might not be an accurate representation of the actual safety of the school because we use reports of safety-related disciplinary incidents. Having a higher number of disciplinary incidents does not necessarily mean the school is less safe, because schools may not accurately report these numbers or may not discipline these behaviors (e.g., teacher harassment). Finally, our results on classroom autonomy might be reflective of utilizing a measure from the Tennessee Educator Survey that is not equivalent to how we measure classroom autonomy in the ACA survey. The wording of the items is relatively similar, but the Tennessee Educator Survey asked about classroom autonomy on a Likert-type scale (the item was worded, "Teachers have autonomy to make decisions about instruction, e.g., pacing, materials, and pedagogy.") while we asked about classroom autonomy with more specific examples of three different levels of autonomy (see Table 1). We define schools with "flexible" autonomy as those with responses on the Likert-type scale that were in the middle third of the distribution of all schools. This middle tier could be teachers who have what we call "flexible" autonomy, or they could be teachers with very little autonomy. Across the board, teachers' ability to work in a school with the attributes they prefer will depend on whether their desirable characteristics are available within the geographic area in which they consider working and inconsistencies in matches may be related to differences in how school characteristics are measured on the ACA survey versus the administrative data. Given these limitations, a conservative approach would be to examine only cases where there is a nearly perfect match rate between expressed and revealed preferences (i.e., nearly 100%). Attributes with the highest match rates are on student race, student income, consistent discipline enforcement, class size, and high-quality PD opportunities. Many of these attributes with the highest match rates are relatively important to teachers (e.g., consistent discipline, class size, and opportunities for high-quality PD). Given high match rates for a number of the more important attributes, we can be more confident in the expressed preferences in our survey indicating revealed preferences. Though there is still potential for bias between expressed and revealed preferences, overall, we find that the revealed preferences of teachers are consistent with their expressed preferences from the ACA survey, evidence that the survey results are showing actual preferences for school working conditions. Our findings suggest that five malleable school processes are likely to have the greatest influence on teachers' employment decisions, at least for those with experience in pre- and posttakeover of ASD schools. These five malleable processes include consistent enforcement of discipline, consistent administrative support, school safety, small class sizes, and availability of high-quality PD. These results are encouraging because the attributes most important to teachers in the sample may be more directly influenced by school administrators. Therefore, these findings could be used to recruit and retain teachers who have shown a willingness to teach in lower performing and turnaround schools. In light of recent research finding that overall teacher turnover harms student achievement (Hanushek et al., 2016; Henry & Redding, 2020; Ronfeldt et al., 2013), these five malleable processes should be considered important for the management, and ultimately, reduction of teacher turnover in low-performing and turnaround schools. Teachers in our study sample, who have had experience teaching in high-poverty, low-performing schools, are unlikely to choose moving into or staying in schools without all of these attributes in place. The validity check shows that these teachers are indeed able to realize their preferences for most of these attributes. Districts and states might want to consider how to use accountability and measurement systems already in place to encourage the types of behaviors associated with desirable attributes. Many states have administrator evaluation systems and/or surveys that measure many of these attributes. These systems can be used to monitor schools and see which schools need more support on important attributes like consistent discipline and administrative support. At the same time, low-performing schools often struggle to consistently have the desired level of these types of attributes because of their high turnover rates. While a school might have a strong discipline system in one year with small class sizes and a strong safety record, leadership turnover could undermine the attainment of these attributes. School districts might be especially interested in helping to protect schools from the deleterious effects of teacher and leader turnover on these areas of importance to teacher preferences. Perhaps surprising in this analysis is the relatively low importance of structural features or even fixed school characteristics with the notable exception of salary. The responses of individuals when asked about preferred fixed school characteristics might be biased by social desirability, but we are less concerned about this threat with this sample and the ACA survey design. This is especially the case for those preferences that passed the validity check such as racial composition of the schools. Teachers who work at high-poverty, low-achieving, or turnaround schools might feel pressure to express a preference for those school attributes, but the results indicate that these teachers, on average, do not place a high importance on these kinds of school attributes. The survey design forces teachers to go beyond their initial responses on the desirability of working in certain settings to ascertain a rank order of the relative importance of many factors when selecting a school. At the same time, all teachers in this sample have experience in very similar school settings which might indicate that they are more flexible in where they are willing to work than the broader population of teachers, leading to fixed school characteristics taking on less importance. Teachers in our sample seem to value malleable school factors more highly when deciding where to work. This finding cautions against reliance on administrative data alone to investigate how fixed school characteristics affect teacher mobility, because administrative data often contain the correlates rather than actual measures of the characteristics that seem to be most important to teachers willing to work in the lowest performing schools. For instance, relying on suspension rates to explain teacher turnover patterns could be misleading if higher suspension rates are indicative of consistent discipline enforcement. Teachers might be more likely to stay in a school with higher suspension rates because of the consistent discipline enforcement, not because of the suspension rate itself. Understanding teacher preferences for their school and position is a commonly explored topic within the education research literature. We advance the research in this area by continuing the use of an innovative survey technique, ACA, with a policy-relevant group of teachers who have taught in some of the lowest performing schools in the nation. That is, our study contributes novel information on the preferences of teachers who have experience working in low-performing and turnaround school settings. Using this sample, we confirm several findings from the work of Horng (2009) who was the pioneer in using ACA with teachers to gauge their preferences. Like Horng, we found teachers place little importance on school demographics and performance, a finding we were able to validate using administrative data on teacher labor market decisions. We also confirmed the importance of administrative support, school safety, and class size, attributes Horng found to be important in her sample. We were able to extend the work of Horng and advance the ACA methodology for use with teachers by incorporating other attributes which we found to be very important to teachers: student discipline enforcement, performance-based pay, relationships among teachers, and high-quality PD. This study provides motivation for the continued use of ACA to gauge teacher preferences. Unlike the output of a traditional survey, using ACA allowed us to rank school attributes by importance on a ratio scale. For instance, we found discipline enforcement and salary were almost two times more important than commute time. ACA also allowed us to conduct market simulations, predicting the likelihood of a teacher preferring specific schools over others. We offer evidence on the validity of ACA to measure revealed preferences on many of the attributes we include in this study. We also have developed a list of attributes and levels that could be useful for future ACA surveys of teachers. For those who are interested in learning more about teacher preferences for their school and position, utilizing an ACA survey could be a particularly useful method to do so. Schools and districts might be particularly interested in attracting and retaining certain subpopulations of teachers like teachers with more experience, high evaluation scores, high value-added scores, or teachers of color. As was noted earlier, some evidence indicates a successful school improvement strategy involves replacing staff with more effective teachers (e.g., Dee, 2012; Strunk et al., 2016; Sun et al., 2017). We conducted these subgroup analyses based on race, teaching experience, and teaching effectiveness to see if certain groups showed a stronger preference for certain attributes than others (full results in Supplemental Appendix Table B3 in the online version of the journal), but our samples sizes were too small to have adequate precision. In these exploratory, descriptive analyses, we appear to see that Black and more experienced teachers show a stronger expressed preference for tenure and longer work hours than White and less experienced teachers, but these results were no longer significant after correcting for multiple comparisons. Future research could explore if schools and districts can leverage certain aspects of the position and school to attract specific subgroups of teachers. Our results may not generalize to all teachers, including those who do not have experience in the lowest performing schools or teachers unaffiliated with ASD schools. We also note that the preferences of teachers who have not shown a willingness to work in low-performing schools could also be important to schools as they seek to recruit high-quality teachers. While it might be theoretically easier for low-performing schools to recruit teachers who have shown a willingness to work in that setting, administrators might seek candidates with experience in higher performing schools. Our results do not necessarily generalize to teachers with the potential to work in low-performing schools who have not yet been recruited to do so, and this is a potentially fruitful route for future research. Our results also might not generalize to teachers choosing their first school who may not be aware of these variations. Structural elements such as salaries, tenure, and bonuses could be more important than the malleable school processes earlier in teachers' careers, especially when they choose to enter the teaching profession. Future work should examine how structural features and fixed school characteristics influence decisions to entertain and/or accept an offer to teach with a sample of eligible individuals, perhaps among teacher preparation program graduates, since many of them do not enter teaching (UNC Educator Quality Dashboard, n.d.). Our findings suggest that preferences may change with experience or, alternatively, as a function of selection into higher levels of experience (i.e., for teachers who choose to continue teaching for more years). Exploring this issue further may have serious implications for the retention of more experienced teachers in the lowest-performing schools and provide solutions for stabilizing the teaching workforce. Samantha Viano https://orcid.org/0000-0002-9229-3597 Lam D. Pham https://orcid.org/0000-0001-8031-7777
10.3102_0002831220937285	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220937285	School's Out: The Role of Summers in Understanding Achievement Disparities	 Summer learning loss (SLL) is a familiar and much-studied phenomenon, yet new concerns that measurement artifacts may have distorted canonical SLL findings create a need to revisit basic research on SLL. Though race/ethnicity and socioeconomic status only account for about 4% of the variance in SLL, nearly all prior work focuses on these factors. We zoom out to the full spread of differential SLL and its contribution to students' positions in the eighth-grade achievement distribution. Using a large, longitudinal NWEA data set, we document dramatic variability in SLL. While some students actually maintain their school-year learning rate, others lose nearly all their school-year progress. Moreover, decrements are not randomly distributed—52% of students lose ground in all 5 consecutive years (English language arts).	 Children experience vastly different home environments prior to formal schooling (Gilkerson & Richards, 2009; Kaushal et al., 2011; Kornrich & Furstenberg, 2013) and thus arrive at kindergarten with a wide range of starting skills (Lee & Burkam, 2002; Magnuson et al., 2004). Yet once they begin school, children continue to spend a significant portion of their school-age years outside the school setting. That out-of-school time is concentrated in the summer months—a time when schools play little to no direct role in children's lives. Instead, children return to the full-time care of their families, with vastly different options and preferences for how children spend this time (Gershenson, 2013). Student achievement disparities may grow dramatically during the summer, when child experiences appear the most diverse. We use a novel data set with more than 200 million test scores for students across the United States to explore whether the "fanning out" of achievement from Grades 1 to 8 occurs while students are in school or during the intervening summers. The field is generally aware of the phenomenon called summer learning loss (SLL)—that student learning slows during the summer. Less apparent, however, is how little consensus actually exists on basic questions about SLL. Moreover, many of the canonical findings on SLL have recently been called into question based on measurement concerns that apply to data used in most prior SLL research (von Hippel & Hamrock, 2019). At a time when even fundamental questions in the SLL literature need to be revisited, our analyses also contribute a unique focus on the total variability in SLL—a surprisingly understudied phenomenon. Nearly all prior SLL work focuses on how summers contribute to race/ethnic or socioeconomic status (SES) gaps. However, these factors together only account for about 4% of the variance in summer learning rates (von Hippel et al., 2018). These gaps deserve our attention, yet a sole focus on these gaps misses important big-picture questions about the SLL landscape. Herein, we zoom out to explore the full spread of SLL experiences and examine how differential SLL contributes to where students end up in the achievement distribution at the end of eighth grade. Even before concerns arose about possible measurement artifacts in SLL, surprisingly few aspects of SLL have been well established. For instance, do students, on average, actually lose ground during the summer or just exhibit no gain (i.e., flat)? What proportion of a student's school-year gain tends to be lost in the summer that immediately follows? Is the magnitude of SLL similar across students, or do some students exhibit gains while others actually lose ground? Does this vary by grade level? Do summer losses accrue to the same students year after year? We tackle these questions using a set of achievement scores that are potentially less susceptible to the measurement concerns raised by von Hippel and Hamrock (2019). These foundational questions have theoretical implications for the production of outcome inequality, as well as practical implications for where researchers and policymakers look for opportunities to disrupt this stratification process. We focus on estimating the total variability in SLL across students, relative to school-year gains. Describing this total (or unconditional) variance is important for at least four reasons. (1) Summers will only contribute to widening achievement disparities if students exhibit meaningful variation around the typical summer pattern. We find that SLL does vary dramatically across students. (2) Because of this wide variability, mean SLL patterns— those that most researchers, policymakers, and practitioners are familiar with—do not characterize most students' summer experiences very well. (3) We find evidence that the same students are likely to lose ground from summer to summer, suggesting a nonrandom accumulation of summer decrements. (4) Prior work has found that even a full vector of student demographics, home characteristics, prior achievement, and a list of summer activities accounts for only 13% of the variation in SLL (Burkam et al., 2004). In other words, SLL appears to vary greatly, but race and class— which have been the main focus of prior SLL research—are an important but limited part of the story. Data provided by the NWEA allow us to estimate means and variances in SLL across eight grade levels, using a data set with more than 200 million test scores for nearly 18 million students in 7,500 districts across all 50 states in a very recent time period (2008 through 2016). NWEA's Measures of Academic Progress (MAP) scores are item response theory (IRT) based and computer adaptive in all grades, and cover a broader range of content than scores used in prior SLL research. The use of MAP scores, in and of itself, represents a timely contribution to the field of SLL because, as von Hippel and Hamrock (2019) have recently shown, newer data sources and scaling practices can dampen and sometimes even reverse some of the long-standing inferences about SLL gaps. They also argue that the above features of NWEA's test scores can make achievement gain inferences less susceptible to measurement artifacts. Their work has raised troubling questions about the robustness of what we thought we knew about SLL. The current study is among a new wave of SLL research to revisit our foundational knowledge about SLL, and our findings reaffirm the existence and importance of this phenomenon. We use this powerful data set in a hierarchical student growth–modeling framework to characterize the contribution of SLL to end-of-school achievement disparities. Specifically, we address the following four questions:1. On average, how do learning gains during the school year compare with gains/losses during the summer across grade levels?2. Of more relevance to the current investigation, how much do students vary in terms of how much they gain or lose?3. Do the same students tend to exhibit SLL year after year, or are these gains/losses randomly distributed?4. How large is the role of summer in producing end-of-school outcome disparities? With respect to the questions posed above, we do find that some students maintain their school-year learning rate throughout the summer, while others can lose almost as much ground as they had gained in the preceding school year. We show that even if all the inequality in school-year learning rates could be entirely eliminated, students would still end up with very different achievement levels due to SLL alone. Our findings also suggest that negative summer decrements tend to accumulate for the same students over time: We find that more than twice as many students exhibit 5 years of consecutive summer losses (as opposed to no change or gains) than one would expect if summer losses were independently distributed across students and grades. Furthermore, these consecutive losses add up to a sizeable impact on where students end up in the achievement distribution: In a 5-year period, the average student in this group ultimately loses nearly 40% of their total school-year gains during the intervening summers. In what follows, we first situate the contributions of the current study within existing SLL literature. Next, we introduce this unique data set and show how it compares with the broader U.S. public school population. We also describe a significant primary data collection activity undertaken to address a methodological concern in SLL research about the dates on which tests are taken (more on this below). In the Methods section, we present our multilevel model and key parameters. The Results section is organized by the four research questions previously described. The Conclusion provides a reflection on our results relative to prior SLL findings, the study limitations, and implications for future research. There are logistical challenges to studying SLL: The data provided by the annual end-of-school-year statewide testing systems, which are most often used by researchers, lack the fall data point needed to separate learning gains between the school year and the summer. Opportunities to investigate SLL have necessarily been limited to idiosyncratic samples (e.g., one city), specific years, or particular grades (e.g., only after grade K/1). Table 1 provides an overview of the data used across 17 key SLL studies, including whether each one focuses on seasonal patterns in White-Black achievement gaps, SES gaps, and/or unconditional variance in achievement—the latter of which is our focus and is relatively unique. Table 1 also highlights some advantageous features of the current data set in terms of size, number of grades included, and recency. Much has been written about SLL (see, e.g., Gershenson, 2013, for a particularly thorough recent overview; Cooper et al., 1996, for a meta-analysis of early studies). Today, there is a common understanding among policymakers, researchers, and practitioners that during the summer students lose some of the knowledge and skills acquired during the school year. The seminal SLL research comes from two key studies: Heyns's (1978) study of the summer after fifth grade for about 3,000 students in 42 Atlanta schools from 1970 to 1972 and Entwisle and Alexander's study of the summers after Grades 1 to 4 for about 750 students in 20 Baltimore schools from 1982 to 1987 (Alexander et al., 2001, 2007; Entwisle & Alexander, 1992). These studies documented that, on average, students' learning rates slow during the summer. Heyns (1978) found that average fifth- and sixth-grade school-year gains in Atlanta were positive (about 60% of the national norm for 1 year of achievement gains) while the summer after fifth-grade gains were either flat or very modestly negative, depending on the cohort. Alexander et al. (2001) used a multilevel, quadratic individual growth curve model to document slower summer (vs. school-year) learning. The authors have continued to follow their Baltimore sample through adulthood and have found that early differences in summer learning are predictive of later life outcomes such as high school completion and college going (Alexander et al., 2007). The findings from these studies became the definitive word on summer setback, raising awareness of the phenomenon and the role it plays in growing educational inequality. More recently, researchers have used the Early Childhood Longitudinal Study: Kindergarten (ECLS-K) 1998–1999 and 2010–2011 cohorts to study SLL (Benson & Borman, 2010; Burkam et al., 2004; Downey et al., 2004; Downey et al., 2008; Quinn, 2014; Quinn et al., 2016; Quinn & Le, 2018; von Hippel et al., 2018; von Hippel & Hamrock, 2019). The advantage of the ECLS-K is that the samples are nationally representative (which NWEA is not). This constituted a major step forward for the SLL literature. The ECLS-K data have a few limitations: While the current study includes summers in Grades 1 to 8, ECLS-K only covers the summer after K or Grade 1, which limits one's view of how SLL accumulates as students move through the grades. In addition, because of the sampling methods used for ECLS-K (e.g., on average, only 3.2 students per K school have SLL estimates), clustered analyses seeking to estimate the variability in SLL are not straightforward. The current NWEA data are therefore a useful complement to the ECLS-K data, since the weakness of each one is a strength of the other. One of these ECLS-K studies—by von Hippel et al. (2018)—has a unique analysis that is particularly relevant to the current study. These authors also examine the unconditional variance in SLL at the student level (like the current study) through the summer after Grade 1. Interestingly, they find that the variation in achievement shrinks over that time. They also find that the variation in achievement arises more in summers than in school years. The current study extends these analyses through Grade 8, and we consider the results from the two alongside one another. Another recent study by von Hippel and Hamrock (2019), which compares SLL racial and SES gap findings across three data sets, warrants more detailed discussion. This article has raised some important questions about SLL since the authors show that measurement artifacts can lead to quite different conclusions about how much these gaps grow over time. For instance, when they use two different scalings of math achievement scores available in ECLS-K 1998–1999, one indicates that student SES gaps grow by 83% from Grades 1 through 8 while their preferred scaling suggests that these gaps decrease by 27%. When von Hippel and Hamrock conduct the same analysis using BSS achievement scores—which the authors posit have several undesirable measurement properties—they find that SES gaps appear to grow 369%. The question of whether SES gaps grow more in the summer versus the school year, however, appears to be less sensitive to variations in data sources and scalings: In most permutations, they confirm the finding from the original BSS data that SES gaps grow faster in the summer versus the school year. The von Hippel and Hamrock (2019) study is also particularly relevant to our current analysis because the authors use a subsample from NWEA's Growth Research Database (GRD). The full NWEA data that we use may not necessarily be comparable with the GRD subsample: The GRD is much smaller (e.g., 25 school districts vs. 7,500) and has a shorter panel (2 vs. 8 years). For the current analysis, the key point from their study is the authors' argument that the features of the NWEA/GRD data (e.g., IRT scaling, computer adaptive in all grades, broader content) make achievement gain inferences less susceptible to measurement artifacts. Their exploration of how measurement properties affect the study of SLL would bolster confidence in our results. Finally, though both of these studies use similar data, they focus on different questions: Whereas the current study describes the degree of total variation in SLL, von Hippel and Hamrock (2019) focus on racial and SES gaps (although due to data limitations one cannot look at student-level SES gaps using the GRD data). As mentioned above, race and SES appear to play an important but modest part in explaining variability in SLL. We are aware of one other peer-reviewed study that uses a subsample of NWEA data to explore SLL. Rambo-Hernandez and McCoach (2015) juxtapose the school-year and summer growth trajectories of initially high- and average-achieving students. Their results suggest that high-achieving students exhibit steadier growth throughout the panel while average-achieving students actually grow faster during the school year but lose more during the summer. In sum, the extant research on SLL took an important leap forward in the late 20th century, and it now seems to be experiencing a resurgence of interest, particularly spurred by the availability of the ECLS-K data. This new work improves on the methods used in prior work (e.g., by taking into account test timing, considering measurement artifacts), updates the evidence to a more recent period, and covers a nationally representative sample (in Grades K and 1). The current study continues in this tradition, building off the various methodological advances in this domain. First, NWEA's MAP tests are designed to be vertically scaled assessments of math and reading achievement, which facilitates an examination of student growth across grades (Quinn, 2014; von Hippel & Hamrock, 2019). In addition, we undertook a substantial primary data collection effort to recover more than 44,000 district-year calendar dates for the start and end of the school year, allowing us to make crucial adjustments to SLL estimates on a large scale. We also implement a set of multilevel models that we think connect more clearly to the central research questions in this domain: The coefficients ("fixed effects" in the language of hierarchical linear modeling) correspond to school-year gains and summer losses, while the variance components allow us to characterize a plausible range of gains/losses one should expect across students during those periods. These variance components connect directly to our primary research question: The larger the variation in summer losses across students, relative to the school-year gains, the more summers are the time when end-of-school achievement disparities arise. Table 1 compares key aspects of the current study with prior work. The defining feature of the current study is our unique focus on documenting the scope and seasonality of the total variation in achievement across U.S. students. The current data set also provides data on more than 18 million students across a wider range of grades than was possible in prior work. In addition, the NWEA data set comes from the 2008 through 2016 postaccountability era—a time in which it is at least conceivable that the dynamics of access to quality schooling have changed. The current study primarily uses data from the NWEA's MAP assessment. The data set contains math and reading scores based on a computer-adaptive test designed to serve as part of a formative, benchmarking data system, used in about 32,000 schools located in 7,500 districts across all 50 states in the United States. The MAP assessment is used as a supplementary tool to aid schools in improving their instruction and meeting students' needs, not as a high-stakes test record. Because the MAP assessment is intended to monitor students' progress throughout the school year, it is administered in both the fall and the spring. NWEA's MAP test is designed so that its scores can be expressed on a vertical scale (which the NWEA calls the RIT [Rasch unit] scale), with the intent that the scale can be used to support equal-interval interpretations. In theory, the vertical scale allows comparisons of student learning across grades and over time, while the equal-interval property of the scale ensures that a unit increase in a student's score represents the same learning gain across the entire distribution. It is worth noting that there are many different ways of designing and calibrating a vertical scale, and there is little consensus with regard to the best methods for evaluating the properties of the scale (Briggs, 2013; Briggs & Dadey, 2015; Briggs & Domingue, 2013; Briggs & Weeks, 2009). Therefore, our findings regarding changes across grades assume that NWEA's vertical scale is valid. However, much of the article concerns itself with comparing learning gains in the same grade (i.e., a given school year relative to the subsequent summer). The full data set used for the current study comes from 7,685 U.S. school districts that administered the MAP assessment during the 9 years between 2008 and 2016. Different districts administer MAP in different grades; the NWEA full data set includes 203,234,153 test scores for 17,955,222 students who took a test between Grades K and 11. The data set includes students' race, gender, and math and reading MAP scores, and the number of items attempted and correctly answered, duration of the test, grade of enrollment, and date of test administration. The file does not include indicators for whether the student is an English language learner, is eligible for the federal free/reduced-price lunch (FRPL) program, or receives special education services. For this reason, the current data set is not well suited to studying achievement gaps along these dimensions. Students do not take MAP tests exactly on the first and last days of school but rather typically 3 to 6 weeks before/after the school year starts/ends, respectively. As a result, some of the time between the spring and fall administrations of the test—what one would mislabel as summer time—is actually spent in school. While the NWEA data set does include the test date, crucially it does not include school-year start or end dates. We therefore conducted a large-scale data collection effort to find the start and end dates in every district in a subset of 11 states with the greatest use of MAP assessments. We found 23,223 school-year start dates and 20,807 school-year end dates—about 77% of the district-year calendar dates in those 11 states from 2008 to 2015. In later years, NWEA also began to collect school-year start and end dates. Together, these efforts allowed us to collect actual calendar start/end dates for 50.3% of the observed school years for the entire NWEA data set. Based on these data, we also extrapolate likely dates for other districts. Following the practices in prior SLL studies, we then use these calendar data to make a linear projection of each student's score on the first and last days of the school year. For more information about this process, including a description of our approach to collecting these data, the percentage of actual dates recovered, our extrapolation process, our score projection process, and similarity of study results when using observed scores instead of projected scores, see Appendix A in the online version of the journal. For fall ELA (English language arts) scores, the correlation between the observed and projected RIT scores is .996, with a root mean square error of 2.3 points. Figure 1 illustrates how even small changes in estimated scores using projection methods could have a large impact on estimating summer learning rates. Figure 1 presents two anonymous students as they progress through school between January 2008 and January 2012. Student 1's observed scores—and their test dates—are shown in orange. In dashed green, we project Student 1's achievement scores linearly based on their school-year learning rate. The green line connects the student's projected achievement on the last day of school to the projected achievement on the first day of school after that summer. In some grades, the summer learning gains estimated in the absence of school calendar information would be positive but would instead appear negative once the projections are used. The results are similar for Student 2 (red solid = observed scores, blue dashed = projected scores). The linear projection process—though it produces scores strongly correlated with the observed scores—could have a profound impact on the estimated summer learning gain/loss. In this article, we therefore use the projected RIT scores in place of the observed RIT scores. However, in the online Appendix A, we reconduct the analyses using the observed scores in place of the projected scores and replicate the figures in this article that capture the main findings. For the current analysis, we first restrict the NWEA sample to students observed in Grades 1 through 8 (because these are the grades with the most complete coverage) and to the 89% of those students who neither repeat nor skip grades. In our preferred models, we also restrict the sample to a "balanced panel"—that is, the subset of students who possess test scores for the full grade range being included in the model. For instance, if we examine the test score patterns from first through fifth grade in a given model, only students who have both fall and spring test scores in every grade between first and fifth grade (i.e., a full vector of all 10 reading test scores) will be included in the sample. While this is quite a restrictive sample limitation, it ensures that our findings cannot be conflated with compositional changes from one time point to the next. In Appendix B (in the online version of the journal), we replicate our primary findings on a less restrictive sample by running models with only three consecutive grades at a time (e.g., Grades K through 2, Grades 3 through 5, etc.). In these models, more students are included because the vector of required test scores is much shorter. These two samples have different advantages in terms of internal and external validity. Ultimately, however, the results are relatively consistent (see online Appendix B). In Table 2, we compare the demographic descriptives for the students, schools, and districts from four groups: (1) the population of U.S. public schools (from the National Center for Education Statistics Common Core of Data), (2) the entire population of NWEA test takers, (3) the subset of students who meet the less restrictive inclusion criteria (for the online Appendix B), and (4) the students who meet the more restrictive inclusion criteria for our preferred results (see Table 2; for simplicity, we conduct this comparison in the 2011–2012 school year). First, recall that that a student-level indicator of FRPL status is not available in the NWEA data set. However, at the school level, the mean percentage of students in a school who are FRPL eligible is very similar across the four groups: 50% both nationally and in the NWEA universe of schools, 48% in the larger online Appendix B sample, and 51% in the more restrictive, primary analytic sample. The NWEA sample reflects the U.S. public school population in many ways. For instance, it is similar in terms of the percentage of students identified as Black, Asian, White, and male. In addition, the majority of U.S. public schools are in rural geographic codes, followed by suburban and urban geographies, and this ordering also holds in NWEA. Many of the district characteristics are also quite similar. To consider limitations to generalizability, we point out that the largest differences between the U.S. public school population and the NWEA universe are that (a) the NWEA sample has a lower percentage of Hispanic students, (b) the average NWEA school has a somewhat smaller mean enrollment, and (c) the NWEA districts tend to have more schools in them, have a lower percentage of FRPL students, and are less likely to be rural. These differences could be connected to the potential for unobservable differences between the NWEA sample and the public school population (e.g., orientation toward innovation and technology, resource allocation strategies, district leadership). What is also of note, however, is the sheer number of students in the NWEA universe in 2012 alone. NWEA students constitute more than 11% of the entire K–12 public school population in 2012. NWEA data are available in nearly 37% of U.S. public schools and in more than half of all districts. This population is large enough to be of interest in its own right. Nonetheless, the lack of national representativeness is a weakness of NWEA data, relative to ECLS-K data. Finally, we examine how the analytic sample limitations affect the characteristics of the NWEA students included in the models (compare the right three columns of Table 215). The final column reflects the requirements for inclusion in the balanced panel. Generally, the analytic restrictions do not dramatically alter the descriptive profile of included NWEA students, schools, or districts. However, the primary analytic sample has a higher percentage of White students than the NWEA full data set (60% vs. 53%), and the schools tend to be smaller (mean enrollment of 391 vs. 486) and are less likely to be suburban. We use a multilevel model to estimate an individual learning trajectory for each student as they progress through sequential school years and summers. We then look across students to estimate how much students tend to gain, on average, during the school year versus what they typically lose during the summer. A multilevel modeling approach also allows us to estimate the variation in these gains/losses across students. Our multilevel model uses a Bayesian approach to estimate the variances and covariances. This approach produces more conservative estimates of student-level variances and is therefore preferable to calculating the raw standard deviation (SD) of summer gains, which reflects measurement error (Raudenbush & Bryk, 2002). We use a two-level random effects (hierarchical) model, in which the outcome of interest is a test score, [MATH], for student i at grade-semester t. In our preferred models, we separately model scores in first through fifth grade (students included here must have all 10 math score outcomes) and then in fifth through eighth grade (again, students must have all 6 test scores in these grades). For brevity, we present the model (Equation 1) for math scores from Grades 6 through 8. These six repeated observations at Level 1 (L1) are nested within students at Level 2 (L2):[MATH][MATH](1) At L1, students' growth trajectories are modeled with a set of dummy variables—[MATH], [MATH], [MATH], [MATH], and so on—for each grade-semester. Each is coded 1 if the observation occurred on or after the ending time point for the period. This coding scheme is different from that chosen in some prior work and may at first seem confusing, but it has the advantage of giving the L1 coefficients intuitive meaning that now match the variable names: They represent an individual student i's grade-specific school-year gain or grade-specific summer gain/loss. For example, [MATH]—the coefficient on [MATH]—captures student i's Grade 6 school-year learning gain. The coefficient on [MATH] captures student i's summer after Grade 6 gain/loss. These coefficients are now the very learning gains/losses we are interested in estimating for each student. We allow all of the L1 coefficients ([MATH] through [MATH]) to vary randomly at the student level, and we assume that the L2 errors ([MATH] through [MATH]) are normally distributed with a mean of 0 and a constant variance given by [MATH] through [MATH]. At L2, we use a fully unstructured covariance matrix, meaning that we estimate the variances of and correlations among all period-specific gain/losses rather than constraining them to be 0 or any other known value. These models estimate the parameters we need to answer each of our research questions (RQs) in turn. We present findings both formally (i.e., point estimates in tables) and visually to make takeaways as tangible as possible. For instance, to address this first question, we present the beta coefficients (or "fixed effects" in the language of hierarchical linear modeling) in Table 3 (ELA) and Table 4 (math) because, substantively, they capture mean gains/losses in each grade and the following summers. These [MATH] coefficients are also graphed in Figure 2 as mean growth trajectories. To contextualize the findings about summer experiences, we first discuss mean school-year learning gains. Beginning from the left column of Table 3 (ELA), we find that students' school-year learning gains are largest in the early grades and generally diminish over time. This is depicted in Figure 2 with blue dashed lines. For instance, students gain on average 23.7 ELA MAP score points in first grade, 18.5 points in second grade, 13.3 points in third grade, and so on. By eighth grade, the average ELA learning gain on NWEA's RIT scale is just 4.4 points. We observe a very similar pattern for math (left column of Table 4). In all grade levels, the average student gains—as opposed to loses—ground during the school year. This suggests that students accumulate knowledge over time during the school years as measured by the NWEA MAP test. The patterns of mean summer learning gains/losses—the [MATH] coefficients in Tables 3 and 4—are shown as solid red lines in Figure 2. Summer estimates differ from school-year gains in two important ways. First, in both ELA and math, the summer coefficients between first and eighth grade are negative and tend to be smaller in magnitude. For instance, the average ELA loss in the summer after first grade is −6.6 test score points, −3.9 in the summer after second grade, and −3.4 in the summer after third grade, and it falls to a low of −0.9 just before Grade 8. In math, the mean summer learning estimates are also negative and of similar magnitude. An implication here is that, depending on grade, the average student loses between 17% and 28% of their school-year ELA gains (a 9-month period) during the following summer (a 3-month period). In math, the relative losses are a little larger: The average student loses between 25% and 34% of each school-year gain during the following summer. The second way in which summer estimates differ from school-year estimates is that the magnitude of mean SLL does not decrease over time to the same degree as in school-year learning. Put differently, although mean school-year gains in ELA fall from 23.7 to 4.4 across grades, mean summer losses stay within a tighter range of −6.6 to −0.9. Turning to the visual representation of these findings in Figure 2, we consistently see a zigzag pattern at every grade level, though the intensity of gains/losses flattens in the higher grades. These results generally confirm the notion that summers can be characterized as a time when, on average, students lose ground. Historically, SLL studies have not reached consensus on the direction of mean summer learning rates; some find losses, while others find stagnation, mere slowdown, or a mix of results across grades, subjects, or data sets. The current study joins those that find mean losses, but we will see that the 95% plausible value range (PVR) across students always includes 0. However, we caution against overemphasizing mean SLL since it will become clear that this mean does not well characterize what students experience in the summer, because it masks the dramatic underlying variability across students. It is important to recognize that the trends illustrated in Figure 2 only tell us one part of the story: the seasonal learning patterns for the average student. However, achievement disparities are driven by differential learning patterns, and so we now focus on how students vary on both school-year and summer learning gains/losses. We are particularly interested in determining whether student growth trajectories vary more during school years or summers. We begin by examining the variability in school-year learning across students. The first column of Table 3 (ELA) and Table 4 (math) contains the estimated SDs of learning gains/losses across students in and after each grade (i.e., the square root of the diagonal elements of the tau matrix). For example, while we saw that the average student gains 23.7 ELA points in Grade 1, students also typically differ from this mean by 9.7 points. To illustrate the magnitude of this variability, we construct a 95% PVR for learning gains across students (under the assumption of normality; Raudenbush & Bryk, 2002). These are reported in Table 3 (ELA) and Table 4 (math) beneath the corresponding student SD. To continue with the example of Grade 1 ELA gains, we expect that 95% of students would have an average learning gain between 4.4 and 42.7 ELA test score points. Therefore, in first grade, students at the high end of the PVR gain about 80% more than the average student. Estimates of the SD of school-year learning gains across students are relatively consistent across school years and subjects—generally in the range of 6 to 10 test score points. In grades that exhibit smaller average school-year gains, this variation implies larger discrepancies across students. For instance, in eighth grade, when average growth is only 4.4 test score points during the school year, we see a 95% PVR across students of −7.0 to +15.9 points. Here, students at the top of this PVR will experience nearly four times larger gains than the average student. Students at the lowest end of the same PVR, however, are actually losing ground during eighth grade. To juxtapose mean gains/losses with the variation around them, we calculate the ratio of the variation (SD) across students for each learning gain to the mean learning gain. Larger ratios indicate greater variability relative to the mean gain. In first-grade ELA, that ratio is about 0.41 (9.7/23.7), indicating that the SD is a little less than half the size of the mean gain. In ELA, that ratio grows slowly across grades and reaches 1.3 in Grade 8 (i.e., the SD is now about 30% larger than the mean). The ratio also increases across grades in math but less dramatically—from 0.40 in first grade to 0.91 in eighth grade. However, the fact that the relative variability in learning gains grows as students progress through school may suggest that inequities in achievement accumulate to some extent during school years, as students who are underprepared are left further and further behind with each successive grade. While the variability in school-year patterns are interesting in and of themselves, our main interest lies in whether the summer gains/losses vary more than the gains in the school-year periods. This has direct implications for our understanding of when discrepancies in student achievement arise across the course of students' school-age years. Turning to the third columns of Table 3 (ELA) and Table 4 (math), we see that the SD for a given summer tends to be a little smaller than the SD in the preceding school year (with the exception of first grade). For instance, in third-grade math, the SD is 6.6 in the school year and 3.6 in the following summer. This is expected; the summer is about one third the length of the school year, and so gains will be smaller. However, in a relative sense, the summer SDs are much larger with respect to the means. In ELA, the SD-to-mean ratios described above are much larger in summers, ranging from 1.4 to as high as 5.2. A ratio of 5.2 indicates that the SD is more than five times larger than the mean loss. Recall that the largest such ratio during a school year was only 1.4. In math too, we see that the summer ratios, which range from 0.8 to 2.3, are larger than the school-year ratios (which only range from 0.40 to 0.91). Keep in mind that this larger summer variation is arising in a comparatively shorter time (around 3 vs. 9 months). This highlights the fact that a great deal of variability in gains/losses is packed into a relatively short time frame. The PVRs are large for SLL. Take second-grade math as an example: SLL in Grade 2 for math (fourth column of Table 4) ranges from −16.3 to +6.8. While students at the top of that PVR are gaining during the summer another 32% of average growth from the preceding second-grade school year (6.8/18.6), students at the bottom of the PVR will lose during the summer just as much as the typical student gained in second grade. Looking across all grades in ELA, we find that students at the top of the summer loss PVR will gain during the summer from 45% to 154% of the mean growth in the preceding grade (12% to 86% for math). However, students at the bottom of the summer loss PVR will lose during the summer from 93% to 194% of the mean growth in the preceding grade (73% to 136% for math). In sum, some students experience accelerated learning during the summer relative to the preceding school year, while others lose nearly all of their prior gains. The takeaways for RQ2 are also illustrated visually in Figure 3 (ELA) and Figure 4 (math), wherein we present box plots of individual students' empirical Bayes estimated learning gains and losses in each school year and summer. These concisely capture the essence of what is presented in the tables: larger gains during school years that diminish across grades, smaller average losses during summers that are more consistent in magnitude, but real variability around typical gains/losses. In the online Appendix B, we replicate Figure 3 (ELA) and Figure 4 (math) using the results from models using a shorter, three-grade increment. Though the data coverage is sparser before first grade and after ninth grade, we do include those grades in the online Appendix B. In sum, students certainly appear to vary in terms of how much they learn during the school year, but most students tend to exhibit some test score gains while in school. However, the picture in the summer is quite different. While our results redocument the mean SLL phenomenon, this finding obscures a more problematic pattern: For mostly unknown reasons, certain students can gain at a faster rate in the summer than the mean rate in the preceding school year, while other students could lose most of what is typically gained. Up to this point, we have highlighted important variability in summer learning patterns across students. However, if that phenomenon occurs to students randomly—that is, a student might gain in one summer and then randomly lose in the next—then the contribution of SLL to end-of-school achievement disparities would be limited. However, if the same students tend to experience losses summer after summer, while others gain summer after summer, it would lead to a more dramatic "fanning out" of student outcomes as they progress through school. We would be particularly concerned if the students who exhibit the greatest summer losses also tend to be from historically marginalized student populations—a question that has been taken up in many prior SLL studies. However, since student demographics appear to only account for about 4% of the variance in summer learning rates (von Hippel et al., 2018), we explore the systematicity of SLL across grades beyond just the differences by race and class. To explore this question empirically, we examine from our multilevel models the estimated covariances of students' summer losses across grades. The upper panels of Table 5 (ELA) and Table 6 (math) present these covariances (expressed as correlations). Positive correlations are the most problematic: Summer losses accrue to the same students over time in a way that would contribute to the widening of end-of-school student outcomes. Correlations near 0 would suggest that gains/losses occur randomly. In ELA, all the correlations are positive (between 0.12 and 0.65), and most are substantively large. The corresponding correlations are also positive in math, ranging between 0.10 and 0.65. This suggests that students who lose ground in the summer tend to also lose ground in subsequent summers. Likewise, students who make gains in one summer are also more likely to make gains in other summers. While few other studies have presented similar correlations across summers, von Hippel et al. (2018) also find a positive (though weaker) relationship between learning rates in the summers after K and Grade 1 for reading (+0.06) in ECLS-K:2011, but interestingly, they find that relationship is negative (−0.21) in math. In the lower panels of Table 5 (ELA) and Table 6 (math), we also present the correlations of summer gains with school-year gains. Given that we have observed a notable zigzag pattern in learning trajectories and that the majority of students do exhibit learning gains while in school, we should anticipate that these correlations will be negative, particularly in adjoining periods (e.g., when a student loses ground in the summer after Grade 4, they start Grade 5 in the fall from a lower point from which to grow). Indeed, this is what we observe. For ELA, all but one of the 16 correlations presented in the lower panel of Table 5 are negative, and correlations from adjoining periods are the strongest. Of course, the more the time that separates the given summer (rows) and school year (columns), the weaker that negative relationship becomes. For instance, school-year gains in Grade 1 exhibit a negative correlation of −0.41 with summer gains/losses in the summer directly after Grade 1, −0.23 with the summer after Grade 2, −0.01 with the summer after Grade 3, and +0.01 with the summer after Grade 4. The results for math (lower panel of Table 6) follow a very similar pattern. These findings are also consistent with those of von Hippel et al. (2018), who also report negative correlations between summer and school-year learning rates across Grades K, 1, and 2 on the order of −0.55 to −0.21 in both reading and math. Taken together, these three findings—RQ1: slightly negative mean summer losses, RQ2: large variances in summer loss/gains, and RQ3: systematic gain/loss patterns across summers—imply that end-of-school achievement disparities arise at least partly during the summer. How large a role do summers play? To consider this question, we begin by presenting a thought experiment designed to characterize the role of summers between Grades 1 and 8. We imagine a hypothetical scenario in which all students enter first grade at the exact same achievement level and all students experience the exact same (let's say, the mean) learning gain in each grade while school is in session. If there were no summer periods, all students in this scenario would end eighth grade with the same test score because no variation in gains arises while in school. We now return to the results from our multilevel model to characterize three plausible student experiences during the summers following each grade: the typical gain among students in the top, middle, and bottom thirds of a given summer's gain/loss distribution. We now illustrate these three levels of summer experiences in Figure 5 (ELA in top panel, math in bottom panel) while assuming that school-year gains are always equal (i.e., parallel slopes of dashed blue lines from fall to spring). Figure 5 shows how the differences in summer experiences by themselves would lead to sizeable achievement over time. In ELA, the spread in test scores at the end of eighth grade is from about 185 to 255 test score points (and about 200–265 in math)—around 2.5 SDs of spring eighth-grade RIT scores. This thought experiment illustrates the idea that even in an ideal world, where school inequities could be eliminated, achievement disparities would arise simply because of the summer break. The "fanning out" of achievement during these school-age years would need to be addressed in large part with respect to summer experiences. In this article, we conduct a thorough exploration of the seasonality of learning from a data set covering nearly 18 million students in 2008 through 2016 across all 50 states. We focus on characterizing the degree of variability in students' summer experiences and the role of summers in contributing to end-of-school achievement disparities. We find that students, on average, do indeed lose meaningful ground during the summer period in both math and ELA. We add to the existing research by estimating the total variance across students in SLL. For instance, consider the SLL pattern after second grade, in which the average school-year gain is 18.6 points in math. During the summer that follows, the 95% PVR indicates that some students will lose as much as 16.3 test score points in math during the summer, while other students could gain up to 6.8 test score points (relative to a mean SLL of 4.8 points). Students do also exhibit significant variance in school-year learning; however, the lower bounds of the 95% PVR during the school year tend to be much closer to 0. This means that while some students learn more than others during the school year, most students are moving in the same direction—that is, making learning gains—while school is in session. The same cannot be said for summers. During the summer, a little more than half the students exhibit SLL, while the other half exhibit summer learning gains. It is clear that the summer period is a particularly variable time for students. We find that some students can in fact maintain average school-year learning rates during the summer in the absence of formal schooling. Other students, however, will lose nearly as much as what is typically gained in the preceding school-year. This remarkable variability in summer learning appears to be an important contributor to the widening achievement disparities during the school-age years. However, most education research tends to overlook the summer period by focusing on programs, policies, and practices designed to shape schooling experiences. But summers deserve greater attention. In Figure 6, we present the distribution, across students, in the percentage of their absolute value fluctuations from first through fifth grade that occur during summers. One can think of this as the percentage of each student's up/down "pathway" between their initial and end scores that arises during the summer. Far from having no role in outcome inequality, we see that, on average, 19.4% of students' ELA test score changes occur during the summer (19.3% for math). However, for some students, summer fluctuations account for much more—even upward of 30%—of where they end up in the achievement distribution. Our findings also suggest that summer learning gains/losses can be quite large and may accrue nonrandomly across students. If the likelihood of experiencing a loss during the summer were independent across students and grades, we would expect that only 24% of students would exhibit losses in five consecutive summers. In contrast, we actually find that 52% exhibit losses (in ELA) in all five consecutive years observed—more than double what one would expect by chance. Furthermore, the average student in this group ultimately loses 39% of their total school-year ELA gains during the summer (results are similar for math). This suggests that negative summer decrements tend to accumulate for the same students over time and that these consecutive losses add up to a sizeable impact on where students end up in the achievement distribution. Historically, SLL studies have not reached consensus on the direction (+/−) of mean SLL. Some find mean SLL (e.g., Allinder et al., 1992; Borman et al., 2005), summer learning stagnation (e.g., Benson & Borman, 2010; Downey et al., 2008), summer learning slowdown (e.g., Alexander et al., 2001; Burkam et al., 2004; Quinn et al., 2016), or a mix of the three (e.g., Downey et al., 2004; Heyns, 1978; von Hippel et al., 2018). For instance, von Hippel et al. (2018) finds positive summer learning rates in some grades, subjects, or ECLS-K cohorts but flat or negative rates in others. The current study joins those that find mean summer losses. We observe this in every summer between first grade and eighth grade in both math and ELA. How does the consistency we see across subjects align with not only recent studies but also Cooper et al.'s (1996) meta-analysis, which found, on average, more negative impacts of summer vacation in math-related subjects than in reading-related subjects? Cooper et al. hypothesize that math skills are more the domain of formal schooling while reading happens both at home and in school. However, the authors also point out that SLL skill patterns do not always fall along a math/ELA divide: Rather, the skills they view as more "procedural" (e.g., spelling and math computation) decline the most during the summer (although reading comprehension also appears to decline during summers, which does not align with this theory). Since we cannot disaggregate our results to more specific math and reading skills, it is less clear whether our findings are in conflict with those of Cooper et al. Moreover, while Cooper et al. found patterns of skill-specific gains/losses, in more recent studies that document mean SLL, no clear pattern by subject has emerged. Is the magnitude of mean SLL similar across studies? As a reminder, the current study covers different grade levels from those covered in the ECLS-K studies; the only overlapping summer is the one after first grade (see Table 1 to review which studies cover which summer grades). This may be partly responsible for any disparate findings. However, in this case (summer after first grade), we think the results from NWEA and ECLS-K:11 are complementary. Take seasonality in ELA learning as an example: von Hippel et al. (2018) document a modest but statistically significant mean SLL rate of −0.02 SDs per month. We find a mean SLL rate of around −2.2 points per month, with a 95% PVR across students that includes 0 (for context, the K fall SD is about 13 points). However, once these mean SLL rates are contextualized with respect to the student-level SDs in SLL, the studies look even more similar: Both show that the student SD is much larger—two to four times larger—than the mean SLL. Most prior SLL research has focused on SES or racial/ethnic gaps in SLL, which is not the focus of the current study. As highlighted in Table 1, we are aware of only one other study that examines seasonal patterns of unconditional variance in SLL. Our results support two primary claims. First, we find that variation in achievement grows significantly from Grades 1 to 8. Second, summer learning varies dramatically and relatively more so than school-year learning. With respect to the first claim, while we find evidence of widening achievement disparities when we follow students from Grades 1 to 8, prior research has not reached consensus on this matter. Claessens et al. (2009) used the IRT-based scale score versions of achievement from ECLS-K:99 and document SDs that grow from Grade K to 8 by 141%. Test score scaling appears to be crucial in this debate, however, because when von Hippel et al. (2018) used improved, IRT-based theta achievement measures to report grade-specific SDs of scores, they actually found that those SDs shrink from Grade K to 2. Despite the fact that both the current study and von Hippel et al. (2018) use vertically scaled scores, the former indicates that variation grows, while the latter suggests that variation may shrink. This debate about whether or not achievement disparities widen as students move through school is long-standing. It may seem counterintuitive that as students move through school, experiencing both different schools and different summer vacations, their achievement would become more homogeneous. But again, test score scaling will prove central to this question. Vertically scaled scores are probably the appropriate theoretical approach to measure growth over time, yet because the assumptions of vertical scales are hard to verify, it is difficult to conclude that a given scoring technique indeed yields the "right" scores. Vertically scaled scores, too, can suffer from measurement artifacts (e.g., scale shrinkage or ceiling effects). Camilli et al. (1993) capture the conundrum succinctly: "It cannot be determined whether developmental scales should show expansion or contraction. The criteria for determining useful vertical scales constitute a controversial topic of debate and research" (p. 387). Though not directly related to widening unconditional variance across grades, it is also useful to consider whether other researchers have found that race/ethnicity or SES gaps widen as students move through school, since demographic gaps could at least partly contribute to overall variation. Again, prior evidence is mixed. For instance, Duncan and Magnuson (2011) show increasing SES, Black-White, Hispanic-White, and gender achievement gaps in math between first grade and fifth grade. In Reardon (2008), IRT-based theta scores show that the Black-White gap increases from −0.32 in K to −0.41 in Grade 5. Recent results based on ECLS-K:11 from von Hippel and Hamrock (2019) and Quinn et al. (2016) both suggest that the Black-White gap may grow in the early grades but—in contrast to prior studies that may suffer from measurement artifacts—SES gaps may shrink between Grades K and 2. With regard to our second claim that summers contribute more to achievement disparities than school years, our results are consistent with those of the one other study in this domain (von Hippel et al., 2018). In both studies, there is meaningful student-level variation in both school-year and summer learning. But, as in the current study, von Hippel et al. (2018) find that the student-level SDs of learning rates are larger in summers. They find this in both ECLS-K cohorts, in both subjects, and in the summer after Grades K and 1. Though school years are generally three times longer than summers and thus have more opportunity to contribute to widening achievement disparities, summers clearly play a key role in where students end up in the achievement distribution. Finally, we can provide some limited reflections on the recent debate about whether inferences concerning the growth and seasonality of SES or race gaps have been distorted by measurement artifacts in earlier work. The von Hippel and Hamrock (2019) article highlights the importance of scaling: The same data set can yield opposing inferences when a different version of the test scores is used. While we find the arguments made by von Hippel and Hamrock regarding preferred measurement properties compelling, we do not have the ability in the current data set to empirically explore these issues since we do not have item-level data. Moreover, their study documents a different phenomenon—race and SES gaps—from the one we document here. We should not necessarily expect that the patterns in overall variability in SLL would move in tandem with patterns by demographics, since demographics seem to explain only a little of the variation in SLL. Regardless of whether or not this is an appropriate interpretation of von Hippel and Hamrock's findings, their study has shaken some people's confidence in the idea that SLL matters. However, as in von Hippel and Hamrock, we too use vertically scaled test scores and still find clear evidence that SLL exists and contributes substantially to where students end up in the achievement distribution. This suggests that SLL is very much worthy of continued research. First and foremost, the NWEA data set does not include key variables to explore SLL gaps (e.g., FRPL, language, special education status, links to teachers). In addition, the current study rests on the assumption that NWEA's RIT scores are a valid measure of student math and reading in both fall and spring periods and over time (i.e., vertical scaling). NWEA's MAP test is a formative assessment without stakes, and it is not entirely clear that there are incentives in place for students and teachers to take it equally seriously in the fall and the spring. Students tend to spend slightly less time on their fall tests than on their spring tests. One would be concerned if this signals that students do not put forth as much effort on their fall assessments, thus making SLLs appear larger than they actually are. We believe that the difference in time spent is not large (about 6 additional seconds per item, on average, in the spring), and we find that controlling for time spent on tests affects the results very little. In addition, most of the analyses herein do not rely on making direct comparisons across distal grades, thus reducing the reliance on vertical scaling properties for these particular inferences. That said, the findings herein should be considered with these caveats in mind. Our results show that summers contribute more to achievement disparities than school years. Our findings to this effect align with prior work (e.g., Downey et al., 2004; von Hippel et al., 2018), though the current study provides perhaps the most comprehensive empirical analyses to date, given its large sample, extension beyond the early grades, and focus on overall variation. This finding has implications for outcome inequality, yet it can be viewed through two different lenses. On one hand, it can be interpreted for what it says about summers. These periods, it seems, are more relevant for the expansion of outcome variation. Some will find themselves looking to summers as a time for intervention and perhaps even questioning whether long summer breaks should be standard practice. On the other hand, this finding can be interpreted for what it says about the school year—that is, how we understand the role of schools in the production of outcome inequality. The summer can be thought of as a counterfactual to schooling, giving us a window into how inequality would grow in the absence of the school's influence. SLL researchers have pointed out that if learning rates vary less during the school year than during the summer, schools may be countering some of the powerful forces that exacerbate inequality when school is not in session. Should schools be reframed, then, as "equalizers"—ameliorating rather than exacerbating outcome inequality? Certainly, this perspective is not widely embraced in the education research community. It is still true that during the school year, some students gain much more than others. Perhaps, then, it would be more precise to say that schools may not intensify inequality but also cannot fully counter it or even hold it constant. In a sense, this question is a philosophical one that depends on what one thinks the purpose of public schooling is. We motivate the current study based on the lack of consensus across prior SLL research, along with the recent questions about measurement artifacts in foundational studies. Our goal is to conduct basic research to clarify our understanding of this important phenomenon. Since we focus more on surfacing just how varied summer learning is and how little we understand about it, making specific policy recommendations is premature. Below, we offer our thoughts about potential directions for future applied research. Since our results show that achievement disparities widen during school years, we should continue to develop policies that change how students experience schools, particularly on issues of access. Yet even in a hypothetical scenario where students all learn the same amount during the school year, the time spent out of school during summer break, by itself, gives rise to much of the dramatic spread of achievement outcomes, on the order of several SDs. One natural question, then, is whether to extend the school year to reduce summer atrophy and minimize opportunities for this divergence to occur. However existing research on year-round school calendars does not indicate that SLL is mitigated by these schedules (Graves, 2011; McMullen & Rouse, 2012). It is possible that year-round calendars implemented to address overcrowding (a common impetus) may have different impacts on learning than year-round calendars implemented explicitly to reduce SLL, but to our knowledge this hypothesis has not been tested. Another policy lever might be to focus on programs that bridge the gap between May and August, like summer school. The causal evaluation of summer school is often fraught, given the nonrandom selection of who is required to enroll and known issues around low attendance (especially in the higher grades). Yet there is growing evidence that summer interventions can help mitigate students' SLL (Kim & Quinn, 2013; McCombs et al., 2012; McCombs et al., 2015). For instance, seven New Mexico school districts randomized early-grade children in low-income schools into an ambitious (and presumably expensive) summer program called K–3+, which essentially amounted to a full-blown extension of the typical school year for much of the summer period. Early results from the experimental study indicated that the children assigned to K–3+ exhibited stronger literacy outcomes across four domains of the Woodcock Johnson achievement assessment (Cann et al., 2015). Our results also suggest that we should look beyond schooling solutions to address out-of-school learning disparities. Researchers have pointed to differential resources in terms of families' economic capital, parental time availability, and parenting skill and expectations as potential drivers of outcome inequality (see, e.g., Borman et al., 2005). Many of these resource differences are likely exacerbated by the summer break, when, for some families, work schedules come in greater conflict with reduced child care. Many social policies other than public education touch on these crucial resource inequalities and thus could help reduce summer learning disparities. We document the magnitude of a social problem, the role of summers in the growth of achievement inequality. While we can conclude that this happens, and to what extent, the current data set is not well positioned for understanding why summer learning patterns are so varied across students. Though it is an important first step to know when inequality arises and how unequal the learning patterns are, the obvious next question is "What accounts for that variation?" In some sense, we have reached a precipice on SLL research. It seems clear that summers play a key role in outcome inequality and that the range of students' summer learning experiences is sizeable. Prior research suggests that this variability may fall partly along racial and socioeconomic lines (Alexander et al., 2001; Benson & Borman, 2010; Borman et al., 2005; Burkam et al., 2004; Downey et al., 2004; Gershenson, 2013; Heyns, 1978; Quinn, 2014; Quinn et al., 2016; von Hippel et al., 2018). However, prior research has also shown that demographic factors only account for a small part of the story here. In an insightful SLL study by Burkam et al. (2004) using ECLS-K:1999 data, the authors leverage the parent surveys of children's home and summer activities in conjunction with student gender, racial, and socioeconomic demographics—that is, most of the first-order candidates for explaining variability. However, they can explain only about 13% of the variance in learning gains in the summer after K. New research is needed to reconcile the fact that summer learning differs dramatically from child to child, but to date we have only limited insight into what accounts for most of that variation. Allison Atteberry https://orcid.org/0000-0002-9409-4372 Andrew McEachin https://orcid.org/0000-0002-5113-6616
10.3102_0002831220944908	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220944908	Can Center-Based Care Reduce Summer Slowdown Prior to Kindergarten? Exploring Variation by Family Income, Race/Ethnicity, and Dual Language Learner Status	 This study examines growth in language and math skills during the summer before kindergarten; considers variation by family income, race/ethnicity, and dual language learner status; and tests whether summer center-based care sustains preschool gains. Growth in skills slowed during summer for all children, but the patterns varied by domain and group. Non-White and dual language learner students showed the largest drop-off in language skills during summer. Lower-income students demonstrated slower summer growth in math skills than their higher-income peers. Students enrolled in summer center-based care had faster growth in math skills than those who did not attend care. Yet lower-income students who attended center-based care showed slower growth in language skills during summer than similar nonattenders. Implications are discussed.	 Gaps in test scores between children from more versus less advantaged backgrounds are large (Reardon & Portilla, 2016), and emerge in the years before children start kindergarten (von Hippel et al., 2018). Supported by evidence that high-quality preschool programs may lessen gaps that disadvantage lower-income, racial/ethnic minority, and dual language learner (DLL) children (Yoshikawa et al., 2016), a number of states and cities across the country have made major investments to expand access to early childhood education in the year or two prior to kindergarten. Yet little is known about growth, loss, or maintenance of students' academic skills during the summer between preschool and kindergarten, when most school-based preschool programs are on break (Friedman-Krauss et al., 2019). Given work showing that children's readiness for kindergarten is predictive of later academic achievement (G. J. Duncan et al., 2007) and educational attainment (Jones et al., 2015), it is critical to understand the role that the summer prior to kindergarten plays in promoting children's school readiness. Prior research examining the elementary school grades has found that summer school breaks can contribute to income- and race-based gaps in test scores (e.g., Alexander et al., 2016; Heyns, 1978; McEachin et al., 2018). Such work has spurred policymakers' interest in enhancing the quality of lower-income and minority students' summer break experiences. For example, the Every Student Succeeds Act passed by the federal government in 2015 encourages states and districts to use flexible funds to support evidence-based summer learning programs. Given growing support for both preschool and summer programming (e.g., McCombs et al., 2019), it is important to better understand patterns of summer learning prior to kindergarten and the role that center-based care during the preschool summer can play in sustaining the academic gains that children typically make in early childhood programs (Condliffe et al., 2018; Yoshikawa et al., 2013). To this end, the current study contributes to the literature in three key ways. First, we leverage student assessment data from the fall and spring of the preschool and kindergarten years in order to examine growth in preschool-attending students' language and math skills during the summer between preschool and kindergarten, relative to the preceding and subsequent academic years. Next, we rely on a diverse sample of students recruited from both public and community-based prekindergarten programs in the city of Boston in order to compare summer learning rates across family income, race/ethnicity, and DLL status for children who did attend formal preschool during the 4-year-old year. Third, we access information on children's care during the summer prior to kindergarten in order to test whether enrollment in center-based summer care helps support kindergarten readiness by reducing income-, race-, and language-based gaps in summer learning rates. Results from the study will provide needed information on the role that early summer experiences play in influencing gaps in children's kindergarten readiness among students who attend formal preschool. The majority of American 4-year-old children—68% in 2017—now attend either part- or full-time center-based preschool (National Center of Education Statistics, 2017). Expansion of preschool programming has been supported in part by a large literature demonstrating that children who enroll in early childhood education programs arrive at kindergarten more school-ready than children who do not (Lipsey et al., 2018; Weiland & Yoshikawa, 2013; Wong et al., 2008; Yoshikawa et al., 2013). Moreover, preschool programs have been shown to reduce income-, race-, and language-based gaps in school readiness (e.g., Bassok, 2010; Bloom & Weiland, 2015; G. J. Duncan & Sojourner, 2013; Magnuson & Waldfogel, 2005; Weiland & Yoshikawa, 2013). Blair and Raver (2012) define school readiness as a multidimensional construct that includes cognitive ability, attention, language, executive functioning, and social-emotional skills—all competencies cited by teachers as critical for the transition to kindergarten. Yet the majority of studies have found that early academic skills—namely, language and math—are predictive of future school success (Claessens et al., 2009; G. J. Duncan et al., 2007; Goldstein et al., 2017; Nguyen et al., 2016). This body of work has spurred heightened interest in identifying how early childhood experiences—including those during the summer—can promote language and math skills in particular prior to children's transition to kindergarten. Although early childhood education is a promising approach for boosting school readiness (Gormley et al., 2008), it is unlikely that preschool programs on their own will be able to close the substantial kindergarten readiness gaps associated with family income, race/ethnicity, and DLL status (Park et al., 2017; Reardon & Portilla, 2016; Valentino, 2018). Indeed, children from different backgrounds are exposed to a range of learning environments outside of formal school settings and variation in these experiences can further contribute to differential growth in skills (Bailey et al., 2017). Data from elementary school contexts suggest that gaps in academic skills at kindergarten entry remain substantial, despite the proliferation of preschool programming in recent years (Reardon & Portilla, 2016). The learning experiences that children have in the summer between preschool and kindergarten are understudied influences that may affect the degree to which gaps in test scores are observed at kindergarten entry (Alexander et al., 2016; Pears et al., 2016). Indeed, work examining samples in early elementary school suggests that income-, racial/ethnic-, and home language–based gaps in academic skills may grow slightly during the summer (e.g., Alexander et al., 2016; Downey et al., 2019; von Hippel et al., 2018). However, these studies are all limited to the postkindergarten period and we know of only one study that has examined learning in the summer after preschool. Using an item response theory approach and teacher reports of students' language and literacy skills, Kim and Camilli (2014) demonstrated that preschool students' skills grew across the school year but slowed during the summer, a similar trend to the ones documented in studies of elementary school students. Research on disparities in access to educational enrichment among families with young children would suggest that gaps in summer learning might be similar across preschool and elementary school samples. For example, outside of early childhood center-based care, lower-income children are less likely than higher-income children to be exposed to cognitively stimulating activities, such as reading, engaging in high-quality conversations with adults, discussing mathematical concepts with adults, and visiting libraries, historic sites or museums (Bassok et al., 2016; Bradley et al., 2001; Gershenson, 2013; Kalil et al., 2016; Rowe, 2012). In addition, ecological theory (Bronfenbrenner & Morris, 1998) would suggest that the home- and school-based learning experiences of students from lower-income families may differ from students from higher-income families because lower-income parents may not be able to invest as much time or money in their children's cognitive development (Bassok et al., 2016; Harding et al., 2015; Kalil et al., 2016), with variation in this exposure differentially predicting outcomes. Such income-based disparities might be exacerbated during the summer months when many children lack concurrent access to publicly funded school programming (Chin & Phillips, 2004; Condliffe, 2016). Given significant correlations between race/ethnicity and family income, this same pattern may also hold true for understanding differences in the summer learning experiences of racial/ethnic minority students compared with White students (Atteberry & McEachin, 2016; Downey et al., 2004; Johnson & Wagner, 2017). Racial/ethnic gaps in test scores may be even more challenging to address, however, with recent work using nationally representative data (Quinn, 2015; Quinn et al., 2016) showing that race-based disparities favoring White students over Hispanic and Black students are substantial at kindergarten entry, and remain relatively consistent during the postkindergarten school-year and summer months. The key takeaway from this recent work is that for race-based gaps in test scores, "nearly all of the inequality ‘action' occurs prior to school entry" (von Hippel, pp. 348). Better understanding how students' experiences and skills in the summer prior to kindergarten do or do not contribute to this school readiness gap will inform efforts to promote equity in outcomes for students from varied racial/ethnic backgrounds. There have been fewer studies examining gaps in summer learning between DLLs and students who only speak English. This is a significant limitation in the extant literature because DLLs represent a sizeable and policy-relevant group of American children. Approximately 33% of children in the United States live in a household where a language other than English is predominately spoken (Child Trends, 2019) and DLL children tend to have lower levels of English-language skills than English-only speakers at kindergarten entry (Park et al., 2017; Reardon & Portilla, 2016). Although the causes of these gaps have not been explained in the literature, there are key differences in the early language experiences of DLLs compared to monolingual children that inform whether we would expect differential growth in academic skills across these groups In early childhood, DLL students are heterogeneous in their experiences with their two languages (i.e., the amount and quality of language exposure, and/or whether they are simultaneous or sequential language learners; Branum-Martin et al., 2014), as well as in their language and literacy abilities at school entry (Hammer et al., 2014; Hammer et al., 2020). However, research shows that DLLs differ from their monolingual peers in language processing, vocabulary development, and oral comprehension. Studies with toddlers have found that DLLs are more efficient at processing their dominant language when engaging in language processing tasks (Conboy & Mills, 2006; Marchman et al., 2010). Further work has found that in preschool, monolingual children are able to process language faster than DLLs using their second language (Sevinc & Önkol, 2009). DLL students' vocabulary and oral comprehension tend to be lower than monolingual students at the beginning of the preschool year, with these skills increasing substantially across a 2-year period where English was the language of instruction (Bloom & Weiland, 2015). Similarly, these skills develop differently for simultaneous and sequential learners, with the latter scoring within the typical monolingual range by the end of the preschool year (Hammer et al., 2014). In sum, despite differences in language processing, vocabulary, and oral comprehension in early years, the evidence suggests that by the end of preschool DLLs are likely to catch up to their monolingual peers. Notably, early gaps in math skills between DLLs and non-DLLs appear to be less stark after adjusting for family income (Lambert et al., 2017). Yet even given data on differences in kindergarten readiness that exist by family income, race/ethnicity, and DLL status and data examining differential learning trajectories in elementary school, the field knows little about growth, maintenance, or reductions in test score gaps that exist across these groups in the summer between preschool and kindergarten. Although ecological theory would suggest that gaps across these groups may widen during the summer between preschool and kindergarten, further work is needed to understand differences in trajectories in order to inform research and intervention that aims to sustain the gains that students make in preschool programs. To this end, there is growing interest in how center-based care during the summer between preschool and kindergarten may help sustain the benefits of preschool programming and support children's school readiness. Indeed, center-based care during the academic year—defined as enrollment in a formal preschool, Head Start, or child care setting (Chaudry et al., 2017)—has typically been linked with positive academic outcomes for students and reductions in skill-gaps favoring more advantaged students (Phillips, Lipsey, et al., 2017; Vandell et al., 2010). Expanding access to academically oriented summer programs targeted at lower-income, racial/ethnic, and DLL elementary school students is one mechanism that policymakers have supported to help quell growth in test score gaps during the summer months (McCombs et al., 2019). Center-based programs are currently available to rising kindergarten students over their summer break in a number of forms, ranging from Head Start programs that operate across the full calendar year, center-based preschool that operates year-round, or summer day camps that provide child care during the summer months, among other city-specific options. Yet access to center-based care during summer varies by family income. Although there are certainly options available to lower-income families through Head Start and state- and district-funded summer programs, private center-based summer programs can be costly, and access may be restricted to higher-income families with the resources to pay for them (Alexander et al., 2016; McCombs et al., 2011). In addition, Head Start only has the current capacity to serve 21% of low-income 4-year-old students and most cities with public preschool programs only provide funding during the academic year (Barnett & Friedman-Krauss, 2016). Transportation challenges and limited access to information about affordable care also constrain the summer program options for lower-income parents with young children (Condliffe, 2016). Yet little is known about whether there are income-, racial/ethnic-, and home language–based disparities in access to high-quality care during the summer. At the same time, there is some evidence that high-quality center-based care during summer can boost children's school readiness even after attending formal preschool, particularly among lower-income and minority students (e.g., Beach, 2004; Graziano et al., 2014; McCombs et al., 2019). For example, a randomized trial of the literacy-focused Kids in Transition to School summer program tested in a sample of children with developmental disabilities and delays who attended formal preschool found that the intervention had small positive impacts on literacy skills and reduced students' risk of reading failure (Pears et al., 2016). Similarly, a randomized trial of the Stars Summer Program—an intensive, academically oriented 4-week summer program targeted at low-income children who had typically attended center-based care during their 4-year-old year—found that the intervention improved children's readiness for kindergarten (Berlin et al., 2011). These findings suggest that center-based care during the summer may help support development of academic skills across the summer within low-income and at-risk samples of students who attended formal preschool. However, there is little work to date that has examined a broad conceptualization of center-based care in the summer taking into account variation across different program models, and comparing the effects of center-based care during summer by family income, race/ethnicity, and DLL status. Taken together, there is a need to better understand variation in growth in academic skills across levels of family income, race/ethnicity, and DLL status during the summer between preschool and kindergarten and determine whether and how summer breaks affect kindergarten readiness for a diverse group students. There is some evidence to suggest that center-based care during the summer may help promote greater school readiness for lower-income, non-White, and DLL studies. Yet more work empirically testing this hypothesis is needed in order to guide policy and practice on the provision of center-based care in the summer after preschool. We add to the literature on early childhood education and gaps in test scores by answering four research questions: Research Question 1: How does students' growth in language and math skills differ during the preschool year, the summer between preschool and kindergarten, and the kindergarten academic year? Research Question 2: Does growth in language and math skills during these time periods vary by students' family income, race/ethnicity, and DLL status? Research Question 3: Does growth in language and math skills during these time periods vary by students' enrollment in center-based care during the summer between preschool and kindergarten? Research Question 4: Does enrollment in center-based care during the summer attenuate any differences in growth in language and math skills that exist by family income, race/ethnicity, and/or DLL status? Collectively, findings will provide information on summer learning during a key period that may or may not contribute to the sizeable income-, race-, and language-based test score gaps that exist at kindergarten entry. The study will also help identify whether enhancing access to center-based care during the summer can promote children's kindergarten readiness and help attenuate any income-, race-, and language-based gaps in summer learning that we may observe. The sample for the current study consists of N = 401 students attending the Boston Public Schools (BPS) prekindergarten program or a community-based organization (CBO) implementing the BPS prekindergarten curriculum and professional development model during the 2016–2017 school year. We recruited students from 41 public prekindergarten classrooms and 10 CBO classrooms, nested within 20 public schools and 10 CBO centers. The BPS prekindergarten program is free, full-day, and open to any age-eligible child in the city. Ninety-two percent of prekindergarten teachers included in the current study sample reported using BPS's Focus on K1 curriculum (McCormick et al., 2020), which uses an adapted version of the Opening the World of Learning (Schickedanz & Dickinson, 2004) language and literacy curriculum and Building Blocks (Clements & Sarama, 2007), an early mathematics curriculum for preschool children. Ninety-four percent of the participating kindergarten teachers reported that they implemented BPS's Focus on K2 curriculum, an extension of the Focus on K1 model that aims to align with and build on the content and mode of instruction that children received in K1. See more information about these curricula in McCormick et al. (2020). On average, 67% of the students in the schools and CBOs that participated in the current study were eligible for free or reduced-price lunch (FRPL), compared to 69% of students in schools served in the broader district. Thirty-three percent of students in participating schools were Hispanic, 34% were Black, 17% were White, 14% were Asian, and 2% were mixed race or another race. Fifty-one percent of students in participating schools were DLL. About 40% of third-grade students in participating public schools met or exceeded expectations on the 2015–2016 state ELA exam, while 45% met or exceeded expectations on the state math exam. The public schools in the sample are generally representative of the population of BPS elementary schools offering a prekindergarten program. For example, at the district-level, schools were 35% Hispanic, 34% Black, 16% White, 12% Asian, and 3% other or mixed race. At the district-level, schools on average were 53% DLL, had 42% of third-grade students who met or exceeded expectations on the 2015–2016 state ELA exam, and had 43% of students who met or exceeded expectations on the 2015–2016 state math exam. The team recruited non–special education students from participating prekindergarten classrooms in public schools (N = 312) and CBOs (N = 89) in the fall of 2016. Twenty-nine percent of students in the current study sample are Hispanic, 22% are White, 31% are Black, 14% are Asian, and 4% identify as another race or are of mixed race. Overall, 68% of the current study sample was eligible for FRPL at public school enrollment, 49% are DLLs, 51% are female, 37% attended center-based care during the summer of 2017, and 26% attended center-based care during the summer of 2016 (prior to preschool). Of the students who attended center-based care before kindergarten, 72% had also attended center-based care in the summer prior to preschool. Within the group of low-income students (those eligible for FRPL), 55% of students are DLLs, 39% are Black, 37% are Hispanic, and 92% are non-White. Among non-White students, 78% are lower-income, and among DLL students, 13% are Black, 45% are Hispanic, and 90% are non-White. On average, children in the current study sample were 4.64 years old (SD = 0.31) at the time of the Fall 2016 assessment and 5.60 years old (SD = 0.29) at the time of the Fall 2017 assessment. Eighty-six percent of the students in the study sample had at least one parent who worked full time (classified as at least 35 hours per week), and 56% had a parent who was either married or living with a partner. The average student had 4.28 people living in their household (SD = 1.80), including themselves. Participating parents were 35.58 (SD = 8.10) years old in the fall of 2017 and diverse across educational backgrounds: 30% had a high school diploma equivalent or less, 29% had a 2-year degree, 17% had a 4-year degree, and 24% had some graduate school coursework or an advanced degree. We were able to locate and collect data on 323 (80% of the original study sample) of these students in kindergarten. These 323 students were generally representative of the broader group of students who enrolled in the study in preschool on eligibility for FRPL, race (Black, White, Hispanic, mixed or other race), DLL status, and family/parent characteristics. However, the 80 students we could not follow into kindergarten were more likely to have attended prekindergarten in a CBO (37% of CBO attenders but only 15% of public school attenders were lost from the sample) and less likely to be Asian. We were further interested in comparing the characteristics of the current study sample—students who had all enrolled in either the formal BPS prekindergarten program or a community-based preschool program implementing the BPS prekindergarten model—to the broader group of students who enrolled in the kindergarten classrooms of the study sample but had not participated in either of these early learning programs. Importantly, these descriptive statistics compare children who are enrolled in our study to the broader population of students who could have attended a public prekindergarten or CBO preschool program as a 4-year-old. We summarize these comparisons in the first three columns of the table in Supplemental Appendix A (available in the online version of the journal). We found that children in our analytic sample who attended public prekindergarten were significantly less likely than children enrolled in the nonpublic preschool program (i.e., CBO program) and children who enrolled in neither program to be eligible for free lunch and more likely to be White. Relative to children in the current study sample, nonpreschool attenders were more likely to be DLLs and Hispanic. In line with earlier work from Shapiro et al. (2019), these descriptive findings demonstrate that the children in the current study sample may be more advantaged than the broader population of all students who eventually enroll in kindergarten in BPS given disparities in access to the BPS prekindergarten program and the CBO program implementing the BPS prekindergarten model. Concurrent work is underway to explore the key drivers of these disparities. Within our study sample, we then compared the characteristics of students who did and did not enroll in center-based care during the summer. These descriptive statistics are presented in the fourth and fifth columns of Supplemental Appendix A. As illustrated there, we found that students who attended formal preschool and then enrolled in center-based summer care were somewhat more advantaged than the students who did enroll in formal preschool but did not enroll in center-based summer care. For example, findings demonstrated that lower-income, Hispanic, and DLL students were less likely to enroll in center-based care during the summer, compared to their higher-income, White, and English-only speaking peers. On average, students who attended center-based care during the summer had higher levels of parental education than students who did not attend center-based care during the summer. This set of findings further confirms the importance of including a rich set of covariates in our predictive models and considering key selection issues into center-based summer care. The institutional review boards at the partner organizations for this study approved the human subjects plan prior to the commencement of study activities. Public schools participating in the study were randomly selected from the 76 schools in the broader district offering the public prekindergarten program. We used this random selection process because we faced resource constraints in the number of schools that we had sufficient funds to enroll in the study. By selecting public schools randomly for inclusion in the study, we sought to create a sample that was representative of the broader population of BPS elementary schools offering a public prekindergarten program. As such, we randomly selected 25 public schools and 21 agreed to participate. The team used one school as a pilot school for developing new measures and the remaining 20 schools made up the public school sample. We also used a random process to select 10 of the 11 CBOs in Boston implementing the BPS prekindergarten model (which was supported by funding from the federal Preschool Development Grant program) to participate in the study, and they all agreed. We were unable to enroll all 11 CBOs in the study due to budget and administrative limitations. We asked all prekindergarten teachers assigned to general education or inclusion classrooms in each of the 20 public schools and all the CBO teachers working with 4-year-old students to participate in the study in the fall of 2016. Ninety-six percent (N = 51) of teachers across public schools (N = 41) and CBOs (N = 10) agreed to participate in the study activities, including allowing children in their classroom to participate in direct assessments with the research team. We then followed sample children into public kindergarten and asked their kindergarten teachers to participate in the study. Ninety-five percent of kindergarten teachers agreed to participate in study activities. After recruiting schools and classrooms, we attempted to collect active consent for all preschool students enrolled in participating classrooms. Research staff met with participating teachers to send home backpack mail providing an overview of the study and a blank consent for the parent to complete and return to the child's classroom. Field staff then made regular visits to participating classrooms to pick up these consents and document them. Recruitment activities began in late September 2016 and were completed by late November 2016. Eighty-one percent of all children in participating classrooms consented to enroll in the study. Again, the research team faced significant resource constraints in the number of students we could enroll into the study from the total pool of consented students. This limitation was related to the team's goal of collecting in-depth direct assessments of children's academic skills across multiple time points, an activity that provides rich data but is also costly. In order to generate a student sample that fit within these constraints and was representative of the broader population of consented students, we randomly selected 50% of consented students (~6–10 per classroom) to participate in student-level data collection activities for a total sample size of 401 in the fall of 2016. We found that the students in the analytic sample were representative of the broader group of students who consented to the study (and to the broader group of prekindergarten students in BPS). We trained a team of data collectors to complete direct child assessments of children's school readiness skills in the fall of 2016 (October 1 through December 12), spring of 2017 (April 5 through June 16), fall of 2017 (September 27 through December 5), and spring of 2018 (April 4 through June 14). Each training lasted 5 days and was conducted by a master trainer with multiple years of experience conducting field-based studies. Data collectors needed to pass two reliability tests—a mock assessment with an adult and a test assessment with a child not enrolled in the study—in order to be allowed to collect data in the field. A field supervisor also observed 10% of field assessments directly in order to maintain high-quality data collection throughout each data collection period. We used the Prelanguage Assessment Scale (preLAS; S. E. Duncan & De Avila, 1998) Simon Says and Art Show tests (S. E. Duncan & De Avila, 1998) as a warm up to the assessment battery and to determine the administration language for a subset of assessments (Barrueco et al., 2012). The preLAS assesses preliteracy skills and an individual's proficiency in English. Of the 401 children in the current study sample, 43 (11%) completed a subset of assessments in Spanish in the fall of 2016, 16 (2%) in the spring of 2017, 3 in the fall of 2017, and none in the spring of 2018. In the fall of both 2016 and 2017, we contacted the consenting parents of all students who were selected for the study sample to complete a 20-minute survey. Field staff first contacted parents via text message and email and asked parents to complete the surveys online. Parents received biweekly text message and email reminders to complete the survey. The team used a backpack mail procedure to collect remaining parent surveys, sending hard copy surveys home with children to be completed. The surveys were translated into Spanish, Vietnamese, and Mandarin in order to include the range of languages spoken by parents in the study sample. Parents provided demographic information about themselves and their child, and reported on a range of educational activities they engaged in with their child in the past month. Parents also reported on their child's prior experiences in care and education, including their care during the summer between the prekindergarten and kindergarten years (Summer 2017). All parents received a $25 gift card to thank them for their time. Three hundred and forty-two parents completed the survey in 2016 (85% of the total sample), and 262 students' parents (84% of the kindergarten sample) completed the kindergarten survey in the fall of 2017. Again, due to resource constraints we were unable to generate a 100% response rate for the parent survey. However, we did find that the parents who completed the survey were representative of the broader group of parents included in the analytic sample. The team used the Peabody Picture Vocabulary Test IV (PPVT IV) to directly assess children's receptive language skills in the fall and spring of the prekindergarten year. The PPVT IV is a nationally normed measure that has been used widely in diverse samples of young children (U.S. Department of Health and Human Services, 2010). The test has excellent split-half and test-retest reliability estimates, as well as strong qualitative and quantitative validity properties (Dunn & Dunn, 2007). It requires children to choose (verbally or nonverbally) which of four pictures best represents a stimulus word. In our primary analysis, we used the raw score total as our outcome measure. We assessed all children on the PPVT—regardless of whether they passed the preLAS language screener—in order to be able to describe an equivalent measure of receptive language skills in English across the full sample. This study used the Woodcock Johnson Applied Problems III (Woodcock et al., 2001) subtest to directly assess children's math skills in the fall and spring of the prekindergarten year. The team assessed Spanish-speaking children who did not pass the preLAS language screener using the equivalent Spanish language version of the assessment from the Batería III Woodcock Muñoz (Woodcock et al., 2005). The WJ/WM Applied Problems direct assessment is a numeracy and early mathematics measure that requires children to perform relatively simple calculations to analyze and solve arithmetic problems. Its estimated test-retest reliability for 2- to 7-year-old children is 0.90 (Woodcock et al., 2001), and it has been used with diverse populations (Gormley et al., 2005; Peisner-Feinberg et al., 2001; Wong et al., 2008). In our primary analyses, we present results using the W score of the measure, which is appropriate for examining growth in skills over time (Belsky et al., 2007). We combined scores from the English and Spanish versions of the assessments so the full sample could be analyzed together. As noted below, we conducted a robustness check by excluding assessments completed in Spanish from the math analysis to test whether the results were sensitive to this measurement decision. When completing the parent survey, parents reported on their child's care during the summer between prekindergarten and kindergarten. Specifically, parents listed all locations where their child spent daytime hours during the summer of 2017, including a Head Start center, a private child care center, a summer camp, an in-home child care program, in their home cared for by a parent, cared for by a family member, friend, or a neighbor, or in their home or another home cared for by someone other than a member of their family (including a paid babysitter). Parents then listed the location where their child spent the most time during daytime hours in the summer of 2017. We reviewed parents' answers to both questions and hard-coded the location where the child spent the majority of his or her time during the summer of 2017. We then used online resources, existing work done for a complementary project in the BPS (see Shapiro et al., 2019), and confirmatory phone calls to code locations as center-based care (coded as 1) or not (coded as 0). Center-based care included enrollment in a public school summer program, a summer camp, or a private child care center. The team considered all other care—including care by a parent, friend, family member, neighbor, babysitter, or at a licensed or unlicensed home-based child care—as noncenter based. We accessed administrative data on child demographics from the school district in the fall of the prekindergarten year. We first created a series of indicators to describe children's race/ethnicity (Black, Hispanic, Asian, or Other Race/Ethnicity, including mixed race children), coding 1 if the child fell into the indicated category and 0 otherwise. The reference group was White. Initial descriptive analysis demonstrated that trends in learning rates appeared fairly similar for non-White groups, while White students were qualitatively different from their non-White peers examined collectively. Given constraints on our sample size when examining interactions in models, we then also created one dummy variable for non-White to use in future subgroup analyses when considering variation in growth by race/ethnicity. We used similar indicators to describe children's eligibility for FRPL (1 = eligible; 0 = not eligible) and gender (1 = female; 0 = not female). Throughout the article, we describe students as lower-income if they were eligible for FRPL and higher-income if they were not eligible for FRPL. We set a dummy variable for DLL equal to 1 if administrative data showed evidence that the child spoke a language other than English. We also included an indicator to describe whether the child had attended a CBO for prekindergarten (1 = CBO) or not (0 = public school for prekindergarten). Finally, we used the child's birthdate made available by the school district to calculate age at each assessment time point. Child age at the time of each assessment was included as a time-varying covariate in order to account for variation in assessment dates across time and between-student differences in the length of each of the three time periods of interest. Parents reported on demographic characteristics in the fall of the prekindergarten year, and we used this information to create covariates for analyses. We coded variables as 1 if the characteristic described the parent and 0 if not. These variables indicated whether there was at least one parent in the home working full-time and whether the parent was married or lived with a partner. We used parents' reports of their education to create mutually exclusive groups: (1) high school diploma/GED or less, (2) 2-year college degree or less, or (3) 4-year college degree or less. The reference group was more than a 4-year college degree/graduate degree. We used continuous variables to describe the age of the child's mother at her first birth, the number of people living in the household, and the parent respondent's age in the fall of the kindergarten year. We also included a covariate for the date that the parent survey was completed to account for some variation in the timing of the parent survey data collection across students. We first examined descriptive statistics on students' language and math assessment scores at each data collection time point for both the full sample, and then disaggregated between lower-income (eligible for FRPL) versus higher-income (not eligible for FRPL) students, non-White versus White students, and DLLs versus native-English speakers. We used independent samples t tests to examine whether assessment scores varied between subgroups. Specific details on decision making related to our analytic approach are included in Supplemental Appendix B (available in the online version of the journal). Because our assessment time points were nested within students who were nested within prekindergarten/kindergarten settings, we considered a range of individual growth models in order to explore our study research questions. Students were initially grouped in prekindergarten classrooms but then transitioned to kindergarten classrooms in different combinations. To account for the change in classroom membership across years, we created a new group membership ID to represent students' unique prekindergarten/kindergarten classroom combination. We then used a series of unconditional growth models with a linear effect of time to disaggregate variation in language and math skills explained at the student- and group-level. We next fit piecewise individual growth models regressing time-varying outcomes (Yijt) on indicators representing the spring of prekindergarten, the fall of kindergarten, and the spring of kindergarten (Singer & Willett, 2003). This model allows the intercept to randomly vary across students, and for growth to vary randomly across students. The model also allows the intercept to vary randomly across the prekindergarten/kindergarten groups. Work by Quinn and McIntyre (2017) has shown that random effects models may produce less biased estimates when examining learning trajectories than gain score models with intercepts. Examination of unconditional models showed that there was no evidence that growth over time varied by group, so we excluded random slopes for group. By including indicators for three time points, we were able to model distinct differences in growth rates between the prekindergarten academic year, the summer between prekindergarten and kindergarten, and the kindergarten year and answer the study research questions. Even so, we conducted further empirical tests to determine if a piecewise growth model was a better fit to the data than a model with a linear or quadratic growth trend (see Supplemental Appendix B for more details). The intercept in this model represents the mean score in the fall of prekindergarten and the coefficients on each of the time point indicators represent the difference in the outcome between that particular data collection time point and the fall of prekindergarten. By using these coefficients, we can calculate growth rates separately for the prekindergarten academic year, the summer between prekindergarten and kindergarten, and the kindergarten academic year. The full model is included in Supplemental Appendix B. Our first research question was whether growth in vocabulary and math skills differed between three distinct time periods—the prekindergarten year, the summer between prekindergarten and kindergarten, and the kindergarten year. To answer this question, we fit separate multilevel random effects models for language and math skills. We added child- and parent/family-level covariates in conceptual blocks. As noted above, we included child age at the time of the assessment at Level 1 to capture variation in the timing of assessments. Using the resulting parameters from the fully controlled model, we then calculated separate growth rates for each of the three time periods and to test whether those growth rates were statistically significant. Importantly, the lengths of the time periods that we compared were fairly similar to one another. The average time period between assessments done in the fall and spring of preschool was 6.36 months, while the amount of time between the spring of preschool and the fall of kindergarten (the summer period) was 5.30 months and the time period between the fall and spring of kindergarten was 6.37 months. Finally, we used the resulting parameter estimates to calculate the difference in the growth rates between each of the three distinct time periods and test whether those differences were statistically significant. This result would inform whether children did exhibit slower growth in skills during the summer than during the preceding or subsequent academic year. To answer our second and third research questions, we tested whether growth in language and math skills across these three distinct intervals varied for children who were lower-income, non-White, and DLLs, and for children who attended center-based care during the summer between prekindergarten and kindergarten. We built on our existing analysis and included interactions between the dummies for the subgroup of interest (lower-income, non-White, DLL, enrollment in center-based care) and each of the three indicators for time point (spring of prekindergarten, fall of kindergarten, spring of kindergarten). We fit a separate model for each outcome and subgroup combination and used the resulting coefficients to calculate the growth rate for each subgroup across the prekindergarten, summer, and kindergarten time periods. We further tested whether growth rates varied between subgroups, as well as whether the difference between growth in summer and growth during either academic year was larger for lower-income students compared to higher-income students, non-White students relative to White students, and DLLs compared to non-DLLs. For our fourth and final research question, we tested whether associations between enrollment in center-based care during the summer and growth in language and math skills during the summer varied for lower-income, non-White, and DLL students. To explore this question, we added to our base model by including two-way interactions between enrollment in center-based summer care and the three time points, two-way interactions between the subgroup of interest and the three time points, and three way interactions between the three time points, enrollment in summer center-based care, and the subgroup of interest. Using the parameter estimates, we calculated growth in language and math skills across prekindergarten, summer, and kindergarten for the varied combinations of summer center-based attendance and subgroup. We compared growth rates across subgroup types and tested whether the differences in growth in summer and the academic years varied by attender/subgroup combination. As summarized earlier, 401 students enrolled in the study in prekindergarten. We were able to locate and assess 323 of these students during at least one data collection time point in the second year of the study when students transitioned to kindergarten. The current study sample includes the 323 students who have at least one assessment during the prekindergarten year and one assessment during the kindergarten year. Because data were captured across four time points, this sample yielded a maximum of 1,292 unique person × period observations. A series of analyses discussed in further detail in Supplemental Appendix C (available in the online version of the journal) suggested that covariate data were missing at random. As such, we used multiple imputation (Enders, 2013) to impute child and parent covariates and assessment scores. We present the results using multiple imputation in the main text results section and the results from complete case analysis in Supplemental Appendix D (available in the online version of the journal). Substantive findings did not differ across the two approaches. Findings from descriptive analyses are fully displayed in Supplemental Appendix E (available in the online version of the journal). We found that, on average, students' language and math skills increased over time for all groups. As further summarized in Supplemental Appendix Table 1 (available in the online version of the journal), the full sample of students improved in their language scores by 6.42 (SE = 1.67, p < .01) points during the prekindergarten year, by 8.76 (SE = 2.71, p < .01) points by the fall of kindergarten, and by 15.55 (SE = 4.14, p < .01) points by the spring of kindergarten. In addition, as summarized in the Supplemental Appendix Table 2 (available in the online version of the journal), the full sample of students improved in their math scores by 6.32 (SE = 1.69, p < .01) points during the prekindergarten year, by 8.60 (SE = 2.87, p < .01) points by the fall of kindergarten, and by 13.95 (SE = 4.17, p < .01) points by the spring of kindergarten. However, there were substantial subgroup differences in language and math scores across time favoring higher-income, White, and monolingual students over lower-income, non-White, and DLL students on both language and math assessments. We found that the gap between lower- and higher-income students for language and math skills increased between the spring of prekindergarten and the fall of kindergarten for both language, δ = 1.03, t(321) = 4.84, p < .05, and math skills, δ = 1.98, t(321) = 5.53, p < .05, indicating differential rates of summer learning. Differences between non-White and White students grew for both language, δ = 5.12, t(321) = 12.85, p < .01, and math skills, δ = 1.54, t(321) = 3.95, p < .05, across the summer as well. The difference in language skills between DLLs and non-DLLs was larger in the fall of kindergarten than it was in the spring of prekindergarten, δ = 1.21, t(321) = 3.85, p < .05. Estimated slopes used to answer the first research question are illustrated in the top (language) and bottom (math) panels of Table 1. Full model results with fixed and random effects and average growth rates are included in Supplemental Appendix Tables 1 and 2. When examining the model predicting language skills, we found that children grew in their skills during the prekindergarten (γ = 6.42, SE = 1.67, p < .001) and kindergarten (γ = 6.79, SE = 1.63, p < .001) years. The growth rate in language skills during the summer between prekindergarten and kindergarten was not statistically significant (γ = 2.34, SE = 1.39, p < .10). Further, as illustrated in Table 2 summarizing the differences in the slopes between each academic year and the summer, the summer rate of growth was significantly slower than growth during the prekindergarten (δ = 4.09, SE = 1.46, p < .05) and kindergarten years (δ = 4.45, SE = 1.31, p < .01; standardized difference = 0.15 SDs). In models predicting math, we also found that children grew in their skills during prekindergarten (γ = 6.32, SE = 1.69, p < .001) and kindergarten (γ = 5.35, SE = 1.64, p < .01), but the growth rate during the summer was not statistically significant (γ = 2.28, SE = 1.35, p = .09). We then found that the growth rate in math skills during the summer was slower than during the prekindergarten (γ = 4.03, SE = 1.30, p < .01; standardized difference = 0.15 SDs; see Table 2) and kindergarten (γ = 3.07, SE = 1.27, p < .05; standardized difference = 0.11 SDs; see Table 2) years. The differences in growth rates during the academic years were not significantly different from one another for either outcome. To answer our second research question, we examined whether rates of growth in language and math skills varied by students' family income, race/ethnicity, and DLL status. Results summarizing individual slopes for each group are presented in Table 1. Table 2 then summarizes the differences in the slopes between each academic year and the summer for each group and tests whether those differences are statistically significant. For language, we found that all groups demonstrated growth in language skills during the prekindergarten and kindergarten years. However, only White students exhibited statistically significant growth in language skills during the summer (γ = 5.67, SE = 2.02, p < .01). We further found that the difference in growth in language skills between kindergarten and the summer was larger for DLL than non-DLL students (γ = 5.86, SE = 2.53, p < .05; standardized difference = 0.20 SDs, see Supplemental Appendix F Figure 1 in the online version of the journal). In addition, the difference in growth in language skills between the prekindergarten year and summer was larger for non-White versus White students (γ = 7.24, SE = 3.36, p < .05; standardized difference = 0.24, see Supplemental Appendix F Figure 1) as was the difference between the kindergarten year and the summer (γ = 7.22, SE = 2.99, p < .05; standardized difference = 0.24, see Supplemental Appendix F Figure 1). The size of the differences in summer versus academic year growth rates did not differ for lower- and higher-income students. With respect to math, we again found that all groups made progress during both the prekindergarten and kindergarten academic years (see bottom pane of Table 1). However, only higher-income students exhibited statistically significant growth in math skills across the summer (γ = 3.32, SE = 1.66, p < .05). We further found that the difference in growth in math skills between the prekindergarten year and summer was larger for lower-income students than for higher-income students (γ = 5.17, SE = 2.64, p < .05; standardized difference = 0.19, see Supplemental Appendix F Figure 2 in the online version of the journal) as was the difference between kindergarten and the summer (γ = 5.10, SE = 2.56, p < .05; standardized difference = 0.19, see Supplemental Appendix F Figure 2). Differences in growth in math skills between the summer and either academic year did not vary by race/ethnicity and DLL status. For our third research question, we tested whether enrollment in center-based care during the summer attenuated the slowing of growth in children's academic skills that we observed in our first research question. We found that students who attended center-based care during the summer exhibited statistically significant growth in math skills during the summer (γ = 3.13, SE = 1.48, p < .05). We further found that the drop in the rate of growth in math skills between the prekindergarten year and summer (γ = −9.52, SE = 3.04, p < .01; Figure 1) and between the kindergarten year and summer (γ = −6.11, SE = 2.83, p < .05; Figure 1) was smaller for children who did attend center-based care in the summer versus those who did not. Growth in language skills between summer and the preceding and subsequent academic years did not vary by enrollment in center-based care during summer (see full results in Supplemental Appendix Table 1). For our fourth and final research question, we probed these models further and examined whether lower-income, non-White, and DLL students benefited more than their peers from enrollment in center-based care during the summer. We found that White students who attended center-based care in the summer were the only group to exhibit significant growth in language skills during the summer (γ = 6.61, SE = 3.24, p < .05). We found that differences in growth rates between children who attended center-based care in the summer and children who did not attend center-based care during the summer varied by family income (δ = −8.80, SE = 3.93, p < .05; Figure 2). Growth in language skills during the summer was faster for higher-income students who attended center-based care versus higher-income students who did not. Lower-income children who attended center-based care during the summer demonstrated slower growth in language skills than lower-income children who did not attend center-based care. The effect of center-based summer care on growth in math skills did not vary by race, DLL status, or socioeconomic status. We conducted three robustness checks to test the sensitivity of our results to different model specifications and potential threats to the validity of the analysis. As recommended by von Hippel et al. (2018), we considered how sensitive findings were to the number of days between assessments. Second, we tested whether results replicated when using complete case analysis. And third, we considered whether our findings were robust when we excluded math assessments done in Spanish. The full set of robustness check results is included in Supplemental Appendix D. As illustrated there, our results were robust across all three sets of checks. The current study sought to add to the early childhood and summer learning literatures by examining language and math skills in the summer between preschool and kindergarten for a sample of students who participated in a 4-year-old preschool program implementing the BPS prekindergarten model and then transitioned into BPS kindergarten. In contrast to some work on summer learning done in samples of elementary and middle school (Alexander et al., 2016; Allington & McGill-Franzen, 2018), we did find not find evidence of summer learning loss in this study. However, we did find that growth in children's language and math skills in general did slow down during the summer, a result aligned with more recent work by von Hippel et al. (2018). This finding may reflect the fast speed with which learning occurs during this developmental period when children are enrolled in early childhood education and making significant gains in their ability to understand others and express themselves through language (Girard et al., 2017), to think abstractly (Aras, 2016), and to engage in complex thinking and problem solving through their interactions with adults and peers (Burchinal et al., 2015). When we examined these trends by subgroup, we found that there appeared to be greater inequality in growth in English language skills during the summer for DLL and non-White students. Findings showed evidence that some early test score gaps did grow during the summer before kindergarten. These findings align with other work done in elementary school populations showing that gaps in language skills between these groups grow during the summer and are maintained or even contract during the academic year (Downey et al., 2004; Quinn et al., 2016; von Hippel et al., 2018). We cannot tell from our data whether there were qualitative differences in the environments that DLLs and non-White students were exposed to over the summer relative to their monolingual and White peers. However, prior work would suggest that DLLs were less likely to be spoken to and read to in English over the summer (Buysse et al., 2014; Lugo-Neris et al., 2010). The extant literature suggests that parents of non-White students—56% of whom were also DLLs—may have fewer financial resources than parents of White parents to access activities in and outside of the home to support language development during the summer (Bassok et al., 2016; Reardon & Galindo, 2009). Unfortunately, any increments in these students'home-language skills were not measured, as most students took the kindergarten assessments in English. Interestingly, we did not see any differences in the summer drop-off in growth in language skills for lower-income versus higher-income students. This trend may reflect recent increases in parents' engagement in home-based learning activities that support language and literacy development, across all socioeconomic levels (Bassok et al., 2016). For example, parents from all income brackets are reading to their children at higher rates than ever before (Kalil, 2015) perhaps in part due to large-scale efforts over the past 20 years to increase the frequency of home-based reading and children's exposure to books (Roskos, 2017). However, given that we did find differences in summer growth between White and non-White students, it may also be that our indicator for family income in this study—eligibility for FRPL—is not capturing important variation in children's home-based learning that we would be able to detect with a richer measure of family income. In contrast, lower-income children showed larger drop-offs in math gains during the summer than higher-income children. This result aligns with past work finding that higher-income children are more likely than their peers to be exposed to more advanced math content (Vandermaas-Peeler et al., 2009) and math outside of school contexts (Verdine et al., 2014). In addition, while most children develop basic counting skills by the start of kindergarten, income-based differences are more likely to emerge in advanced number sense skills (e.g., numerical magnitude estimation) and in subsequent math skills measured with standardized assessments (Engel et al., 2013). Growing income-based gaps in math skills during the summer may reflect variation in the home math environments that higher- versus lower-income students are exposed to when not attending formal schooling. As we hypothesized, we found that children who enrolled in center-based care during the summer showed less drop-off in growth in math skills than children who stayed at home with a parent or other informal care provider. Initially, findings reflected general trends in the field showing that school and center-based care are more likely to support growth in math skills than the home context (Berkowitz et al., 2015). For example, parents across socioeconomic backgrounds are less likely to participate in math and complex problem-solving activities with children at home than they are to read to their children and engage in activities like storytelling and discussion (Berkowitz et al., 2015; Sheldon et al., 2010). However, in our sample, once we considered enrollment in center-based care jointly with family income, we found that higher-income children enrolled in center-based care demonstrated less slowdown in language skill growth during summer than higher-income children not enrolled in center-based care. In contrast, we found descriptive evidence that lower-income children showed the opposite pattern: Students who attended summer center-based care showed greater slowing of growth in language skills during summer than those who did not enroll in center-based care. This is an important finding that may reflect variation in the quality of center-based summer care that lower- versus higher-income students were exposed to. A large body of work has demonstrated that, in general, children from lower-income backgrounds have less access to high-quality child care than their more affluent peers (e.g., Bassok & Galdo, 2016; Hatfield et al., 2015). Further, research by Burchinal (2018) summarizes substantial variation in the quality of existing early childhood education programs that likely extends to center-based care during the summer. Accordingly, center-based care during the summer is likely not a monolith that can be examined as one type of program. Rather, there is a need to examine heterogeneous quality in summer-based care and consider how that quality does or does not vary by income, race/ethnicity, and DLL status. Indeed, lower-quality care is less likely to yield substantial impacts on students' academic skills and decrease gaps in test scores (Valentino, 2018). Some work has demonstrated that lower-income families are constrained in their choice sets for care during the summer vacation, because low-cost options close to their homes may not optimally support students' learning (McCombs et al., 2011). Thus, although past research suggests that summer-based programming can boost academic outcomes for lower-income students (McCombs et al., 2019), the quality of such programs is crucial if they are to substantially boost skills prior to kindergarten. Importantly, work examining center-based preschool programming during the school year has shown that high-quality early childhood education can have larger benefits for children from lower-income, non-White, and DLL families (Bloom & Weiland, 2015; Phillips, Johnson, et al., 2017). Learning more about how to support quality improvements in summer center-based care serving these groups of students may yield substantial benefits, although more work focusing on enhancing quality and empirically testing this theory is needed. There is also important potential variation in the home-learning experiences during the summer that may exist between our subgroups of interest. Even when enrolled in center-based care, higher-income children are likely to be exposed to more home-based learning activities when they are not in care (Bassok et al., 2016). In contrast, lower-income children are more likely to have parents who work longer and nontraditional hours and may have less time to engage in home-based learning activities to supplement the activities children are exposed to in care (Kalil et al., 2016). These disparities in the home learning environment may further differentiate the trajectories of lower- and higher-income students enrolled in center-based care in the summer. Notably, we did see significant growth in skills across all subgroups during both the prekindergarten and kindergarten years. This finding aligns with prior work by Downey et al. (2004) that was later expanded on by von Hippel et al. (2018) showing that gaps in skills do not typically emerge or grow during the academic year. Like this prior work, we found substantial income-, race-, and language-based gaps in language and math skills at prekindergarten entry and then some small growth in gaps during the summer prior to kindergarten. All children in this study, however, were enrolled in a prekindergarten program implementing the BPS Focus on Early Learning model, which is heavily based on an approach that has been shown in prior work to produce significant impacts on a range of school readiness outcomes (Weiland & Yoshikawa, 2013). Children then transitioned into public school classrooms typically also implementing the BPS curriculum (McCormick et al., 2020). They were thus exposed to high-quality educational programming during two successive academic years that appeared to support gains in learning in a relatively consistent way across groups. Studies examining other samples with greater variation in the quality of programming that children are enrolled in for preschool and/or kindergarten may observe differences in learning gains during academic years, particularly if preschool experiences are more varied. When interpreting results, it is important to consider some of the key differences we observed when comparing our study sample—students who all attended public and nonpublic preschool—to the population of students in BPS who did not attend a formal preschool program during their 4-year-old year. In general, we found evidence mapping onto prior work from Boston (Shapiro et al., 2019) that children who accessed formal preschool were more likely to be White, less likely to be a DLL, and less likely to be eligible for FRPL, compared to students who did not enroll in a public preschool program. Findings align with prior work demonstrating lack of access to formal learning opportunities in early childhood among lower-income, non-White (and particularly Hispanic), and DLL children, the groups perhaps most likely to benefit from education programs implemented prior to kindergarten (Phillips, Johnson, et al., 2017). These comparisons suggest that the subgroup differences we observed in growth in academic skills during the summer may actually be greater than what we were able to detect with our more advantaged sample of students who were all able to access formal preschool. This same pattern of disparities emerged when we compared the characteristics of students within our study sample who enrolled in center-based care during the summer to those who did not. Moreover, we found that children who did not access center-based care during the summer evidenced more rapid growth in skills than their peers during the preschool year. This pattern may have emerged because students without access to center-based care during the summer tend to be more disadvantaged relative to students who do access summer care and thus enter school with lower skill levels and more room to grow. After evidencing rapid growth in skills during the academic year, our results suggest that low-income students who enrolled in center-based care during the summer actually showed slower growth in language skills than low-income students who did not enroll in center-based care during the summer. As noted above, this finding suggests that there may be additional disparities in the quality of summer programs that different groups of students are able to access, differences that would likely magnify if we were able to examine the summer learning experiences of all students in the district and not just those who had enrolled in formal preschool. More research is needed to understand disparities in students' access to quality care across the year and identify strategies to address them. This study has a number of key strengths, including the ability to examine growth in both language and math skills across four assessment time points as children moved from preschool to kindergarten, the use of a broad set of a covariates and analytic robustness checks, and a diverse study sample. However, there are also a number of limitations that we encountered. First, the study sample is limited to children in Boston who either enrolled in the public prekindergarten program or a community-based program implementing the BPS model. These results do not generalize to other cities or districts and future work should continue to build knowledge on this topic by conducting studies across a variety of localities and a broader set of samples. Moreover, the study does not illuminate what summer learning prior to kindergarten looks like for students who do not attend a formal preschool program. Although we were able to identify key characteristics differentiating students who enrolled in formal preschool versus those who did not, and comparing students who accessed center-based care during summer versus those who did not, future research that explicitly aims to evaluate disparities in access to early childhood education are needed to better address the issue of representativeness in this study. Second, although we have a diverse study sample, the sample size is too small to be able to consider the range of three-way interactions that would be needed to disentangle intersectionality between race/ethnicity, language status, and family income. The current study does control for those potential confounders in the models but is unable to explicitly test differences between intersecting subgroups. Relatedly, we decided to dichotomize race/ethnicity in our models examining variation in summer skills by subgroup. Although we recognize that different racial/ethnic groups face unique challenges, the early descriptive work that we did in this study demonstrated that patterns of learning during the year and across the summer most substantially differed between White and non-White students examined together. As such, we decided to collapse across our non-White racial groups when examining subgroups. Future work should continue to explore differences between these groups. Next, we have no data in the study examining the quality of students' summer learning experiences, whether they occurred in center-based care, at home, or elsewhere. We are thus only able to hypothesize about variation in the quality of summer care across populations. Future research is needed that explicitly aims to measure variation in quality of summer care by family income, race/ethnicity, and DLL status. Finally, the study leverages longitudinal data to examine trends across time but is unable to make causal inferences about the role of summer learning in promoting gains in students' learning. Future studies using experimental or quasi-experimental designs will be better poised to establish causal relationships—or the lack thereof— between center-based summer care and students' gains in language and math skills prior to kindergarten. To our knowledge, this study is one of the first to examine how growth in children's language and math skills during the summer before kindergarten differs from growth in skills during the preschool and kindergarten academic years. Findings demonstrated some disparities in summer learning rates favoring more advantaged groups, depending on the subgroup and outcome examined. Initially, it appeared that students who enrolled in center-based care during the summer showed evidence of faster growth in math skills during the summer prior to kindergarten relative to their peers who did not enroll in this summer care. This finding initially suggested descriptive evidence that the provision of opportunities for center-based care during the summer may stand to benefit students' math test scores at the start of kindergarten. Yet further work showed that lower-income students who enrolled in center-based care during summer actually demonstrated slower growth in language skills than lower-income students who stayed at home during the summer. Taken together, findings suggest that additional research examining the quality of center-based care during the summer is critical for learning more about the interventions that may stand to boost students' kindergarten readiness and close gaps in test scores prior to the start of formal schooling. In addition, it is important to conduct further research examining summer learning prior to kindergarten with a broader set of samples that includes students who did and did not attend formal preschool. The overarching goal of the current study is to inform strategies and interventions to reduce income-, race/ethnicity-, and language-based gaps in test scores at school entry for the full population of students served in the district. Yet if we were to make recommendations for policy and practice based on the current sample of students—who all attended preschool—such actions could introduce larger disparities in kindergarten readiness between those able to access formal early childhood education during the summer, compared to those who were not. As such, the field needs further research replicating this study using representative samples of students so that results can more directly inform policy and practice.
10.3102_0002831220945266	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220945266	The Effects of Providing and Receiving Peer Feedback on Writing Performance and Learning of Secondary School Students	 Research has shown that engaging students in peer feedback can help students revise documents and improve their writing skills. But the mechanistic pathways by which skills develop have remained untested: Does receiving and providing feedback lead to learning because it produces more extensive revision behavior or is such immediate implementation of feedback unnecessary? These pathways were tested through analyses of the relationships between feedback provided and received, feedback implemented and overall revisions, and improved writing quality in a new article. Overall, the number of revisions predicted growth in writing ability, and both amount of received and provided feedback were associated with being more likely to make revisions. However, providing feedback was also directly related to growth in writing ability.	 Feedback generally plays an important role in helping students learn (Gielen & De Wever, 2015; Hattie & Timperley, 2007; Kluger & DeNisi, 1996; Topping, 2009). However, it is difficult for teachers to provide timely feedback to large numbers of students, especially in writing (Applebee & Langer, 2011), because providing feedback requires a considerable amount of time and effort. The workload problem is most strongly felt when teachers have many sections of a class or large classes. In order to reduce teacher workload and make it possible for students to receive detailed and immediate feedback, many teachers look to the use of peer review as an alternative or additional method for providing feedback. More interestingly, prior studies suggest that students actually benefit more from receiving multipeer feedback than feedback from a single teacher (K. Cho & Schunn, 2007). Further, when students actively engage in the reciprocal process of peer review, their concepts and knowledge about both writing and subject matter are further developed (Jonassen et al., 1995), and learning is enhanced (Falchikov, 2001). Peer review is also beneficial for developing students' audience awareness, fostering social skills such as learning how to provide and accept critical comments, justifying one's own position, and declining nonproductive suggestions (Topping, 2009). Because of these advantages, peer review is recommended as a high leverage practice for writing instruction (S. Graham & Perin, 2007a; Topping, 2009). However, a number of concerns about peer review have been raised. First, some students worry about harming interpersonal relationships or the negative effects of power relations between students on feedback content (Topping, 2009). Such problems can be solved by anonymous peer review, which decreases bias and enables students to evaluate others' work in a nonthreatening environment (Lin et al., 2001; Patchan et al., 2018). Second, both students and teachers worry about peers having sufficient expertise to provide valid feedback (Lin et al., 2001). But again, well-structured rubrics and incentives embedded in online peer review systems, especially for honest and effortful participation in the review process, can also produce feedback with high reliability and validity (Patchan et al., 2018; Pearce et al., 2009; Sadler & Good, 2006; Schunn et al., 2016). Although a number of challenges to and benefits of peer review have been uncovered over the past 20 years, relatively little is known about how peer review produces learning. Three challenges are responsible for this surprising lack of knowledge about a practice that is pervasive across grade levels, disciplines, and countries. First, most studies (e.g., Beason, 1993; K. Cho & MacArthur, 2010; Nelson & Schunn, 2009; Patchan et al., 2016; Tsui & Ng, 2000) have looked at performance (improvements in or final quality of a document after peer review) rather than at learning (whether writing in new tasks improves). It is likely assumed that whatever benefits a given document receiving feedback should also eventually benefit learning, but there are theoretical reasons to doubt this assumption (reviewed below) and therefore this assumption must be tested. In the present study, we look at both performance and learning and their interrelationship. Second, peer review centrally involves two different components that could be responsible for learning: receiving feedback from peers versus providing feedback to peers (Tsivitanidou et al., 2011). However, prior investigations of each component alone involved somewhat artificial experimental conditions (e.g., K. Cho & MacArthur, 2011; Lundstrom & Baker, 2009). In the present study, we test their separate effects on performance and learning using a statistical regression approach. Third, it is difficult to robustly and systematically measure the details of the peer review process in the context of a data set that is large enough to robustly show evidence of learning since learning is typically a slow process with small effects from a single learning experience (e.g., one round of peer review). In the present study, we examine student reviewing (receiving and providing peer feedback), their writing performance (peer feedback implementation and general revisions), and writing in a new task across multiple schools and multiple classes per school. In the next section, we first review prior studies examining effects of peer review on performance (including implementing received feedback and making revisions more generally) and then review the literature examining learning effects. We focus the review on studies of the specific effects of receiving or providing given the focus of the current study. This review of past studies is shown graphically in Figure 1. To address the gaps in the literature, we then present a new study that simultaneously examines performance and learning in a large data set to robustly uncover the relationship of receiving and providing feedback on writing performance and learning, where writing performance includes revisions in response to specific feedback and general revisions, and learning refers to the general improvement from one writing task to a subsequent one (e.g., K. Cho & MacArthur, 2011; Van Beuningen et al., 2012). At the most general level, several studies have documented overall improvements in document quality as a result of overall peer review activities (Path 1 in Figure 1; Y. H. Cho & Cho, 2011; Gielen & DeWever, 2015; Zhang et al., 2017). Several studies have then also shown that these general document improvements appear to be the result of both receiving comments (Path 2; Huisman et al., 2018; Lu & Law, 2012; Paulus, 1999; M. Yang et al., 2006) and providing comments (Path 3; Huisman et al., 2018; Lu & Law, 2012; Philippakos & MacArthur, 2016). However, these studies leave open whether the document improvements hide the nature of the behaviors of the students: Do the changes in the document come from implementing received feedback or more generally engaging in revision work on the document? Looking more specifically at effects on implementing received feedback or making revisions, many studies have not-surprisingly found that students who receive more feedback will then implement more changes based on this feedback (Path 4; Beason, 1993; Berggren, 2015; Nelson & Schunn, 2009; Patchan et al., 2016; Tsui & Ng, 2000; Wichmann et al., 2018). Peer feedback recipients have also been found to make more revisions not necessarily tied to particular feedback (Path 5; K. Cho & MacArthur, 2010). Turning to providing effects, providing feedback has also been linked to implementing more of the received feedback (Path 6; Berggren, 2015) and more revisions overall (Path 7; Y. H. Cho & Cho, 2011). These studies involved a mixture of correlational, survey, interviews, and experimental designs. Across these studies, there is pretty good evidence supporting the benefits of providing and receiving feedback on improving the documents receiving feedback. Further, the amount of benefit of providing versus receiving appears to be roughly similar when the effects are directly contrasted (Huisman et al., 2018). Importantly, several of these studies found that the results varied by language level (higher level like argument, evidence, genre awareness vs. lower level like grammar and spelling). For example, Y. H. Cho and Cho (2011) and Berggren (2015) found the main benefit of providing feedback was on higher level aspects of writing, not on lower level aspects. Receiving feedback was sometimes found to produce improvements for higher levels (Lundstrom & Baker, 2009), sometimes for lower levels (Huisman et al., 2018; Wichmann et al., 2018), but sometimes showed no benefit at all on document quality (Y. H. Cho & Cho, 2011). It may be that students need help in making sense of all the feedback they receive from peers (Wichmann et al., 2018). Nicol et al. (2014) suggested that receiving feedback helps students focus more on areas that need improvement and develop a readers' perspective, whereas providing feedback enables students to think critically, apply criteria, and reflect on their own work. Further, students appear to benefit from both reviewing weak documents that have the same problems they made in their own document as well as from reviewing stronger documents that act as models for how to improve (Schunn et al., 2016). In other words, reviewing could help with both higher and lower levels of writing, but it depends on the focus of the criteria and the match to the areas needing improvement. While most studies discuss their findings of improved documents as being highly relevant to learning, only a few studies have looked directly at learning, defined here as writing better first drafts of later documents. Some work has examined the overall effect of peer review on student learning (Path 8; Nicol et al., 2014; Schunn et al., 2016). Two studies have observed benefits of receiving feedback on later writing ability (Path 9; Lundstrom & Baker, 2009; Wichmann et al., 2018) and three studies have observed benefits of providing feedback on later writing ability (Path 10; K. Cho & MacArthur, 2011; Lundstrom & Baker, 2009; Philippakos & MacArthur, 2016). Again, larger benefits were observed for higher level than lower level aspects of writing (Lundstrom & Baker, 2009). Interestingly, larger gains were observed in the students who were initially the weaker writers (Lundstrom & Baker, 2009). Because there are relatively few studies of learning, it is important to note weaknesses in the evidence thus far. Lundstrom and Baker (2009) divided students into receivers (received feedback from peers, did not themselves do reviewing, and used received feedback to revise papers) and providers (reviewed, did not receive feedback, and did not revise papers). Here receivers revised others' writing rather than their own writing based on peer feedback which is somewhat artificial. In the other two studies of providing effects (K. Cho & MacArthur, 2011; Philippakos & MacArthur, 2016) students provided feedback without receiving any feedback, which is a useful experimental condition but "the ecological validity was necessarily limited" (K. Cho & MacArthur, 2011, p. 79). Similarly, Wichmann et al.'s (2018) experimental study involved strong control at the cost of ecological validity: Received feedback came from trained tutors rather than normal peers in the class, and learning was measured by comparing students' problem detection and correction skills from pre- to posttest. Another problem with all four of these studies is that separating providing and receiving processes deprives students from learning in that they do not just benefit from providing explanations and producing critical reviews but also from knowing deficiencies in their work and interpreting readers' needs (Nicol et al., 2014). Zhang et al. (2017) showed that revisions were especially likely when comments provided and comments received align, although they did not track these effects into learning outcomes. As summarized in Figure 1, the effects of providing and receiving peer feedback have been studied on many outcomes and generally positive effects are found using various methods. However, the figure also makes salient the missing link between performance and learning: Does implementation or revision work lead to learning or are the effects of peer feedback independent of whether students act on the feedback? The present study focuses on the relationship between performance and learning. Because much of the prior research found the results varied by higher versus lower level aspects of writing (e.g., Berggren, 2015; Y. H. Cho & Cho, 2011; Liou & Peng, 2009), we examine these relationships separately within those two levels. Unlike prior peer feedback studies that often focused on specific types of feedback and specific types of revisions, the present study analyzes feedback and revisions at a more aggregate level in order to be able to examine the larger system of actions and outcomes at once. The approach was to look at the number of received and provided feedback, and then correlate them with writing performance (amount of implemented feedback, amount of revisions) and improvements in writing quality on a new writing task. Different from previous experimental research designs, a more ecologically valid, correlational design was chosen in which both receiving and providing comments were involved in peer review. Due to the lack of prior research examining sources of learning, such a correlational design is useful for exploring the mediational relationship of various writing performance effects of peer review on learning outcomes; later experimental work can then focus on the specific component(s) receiving support from the correlational study. Three major research questions are addressed: Research Question 1: Does both amount of provided and received peer reviews predict improved writing performance, particularly, both for implementation of received comments versus all revisions made? Research Question 2: Does both amount of provided and received peer reviews predict student learning? Research Question 3: Are the learning effects mediated by performance effects and, if so, via implementation or revision? We examine the three research questions separately with high- and low-level aspects of writing (i.e., providing and receiving high- and low-level feedback, implementation of high- and low-level feedback, high- and low-level revisions, learning on high- and low-level dimensions) since the prior literature often finds different effects at each level. The mediation (third) research question is both theoretically and pragmatically important. Theoretically, it speaks to different learning pathways (discussed in further detail below). Pragmatically, it speaks to the importance of requiring revisions at all or requiring students to specifically act on provided feedback. Teachers are often faced with the pragmatic dilemma of whether to encourage revision plans or even to require revisions at all because these come at the cost of having fewer writing tasks. We specifically investigate these questions in the context of an Advanced Placement (AP) course (i.e., AP Language and Composition). This course is meant to be equivalent to a first-year college writing course, and therefore, sits on the boundary between secondary and tertiary education. This AP course has the highest annual enrollment among all AP courses, reflecting pressure to broaden access to experiences that improve college readiness (see College Board, 2018). At the same time, it has one of the lowest successful performance levels, with fewer than half of the students taking this course performing well enough to receive college credit, likely pointing to a general weakness in writing instruction at the high school level. These concerns call for more research on writing instruction with high school students. By purposely sampling classes from high schools that vary in whether they predominantly serve high- or low-income families, the present study extends the generalizability of the research finding. First, students from lower income high schools commonly face challenges associated with the transition from high school to a more advanced curriculum. Helping such students write expands their access to universities, and also "gives them an edge for advancement in the workforce, and increases the likelihood they will actively participate as citizens of a literate society" (S. Graham & Perin, 2007b, p. 28). The skills they developed in peer review in the AP writing course can help them become an independent writer in the future. In addition, research on peer review in writing is scarce at the high school level (Schunn et al., 2016). Schools serving many low-income families (typically measured by having Title I status in the United States; U.S. Department of Education, 2018) are less likely to offer students AP courses. But there are now many programs in place to broaden participation in AP in such schools. For example, although there is a fee to take the exam, many schools serving low-income students and some states pay the fee for students. Nonetheless, because of the limited number of AP classes offered in Title I schools, these students may be less well prepared to engage with coursework at this level and have lower self-efficacy (Rymer, 2017). There are also higher rates of academic disengagement in Title I schools, which might influence norms for participation in peer review. Each of these aspects (less preparation, lower self-efficacy, lower participation norms) might influence the amount and type of feedback they give, as well as the ways in which they use the feedback they receive. At the same time, students from Title I schools might have a critical need to grow from the AP English course given how few other AP courses they will experience in high school in order to be ready to engage successfully in college. As noted above, peer review is a complex process involving both acts of providing and receiving feedback. Understanding the underlying learning pathways is vital to maximizing the benefits of peer review. In the current study, a conceptual model of peer review and learning is being proposed (see Figure 2) involving five major components—providing feedback, receiving feedback, implementation of changes based on peer comments, all revisions, and learning. Figure 2 highlights the four hypothesized pathways being tested as part of Research Question 3. The first two hypotheses are concerned with how practice produces learning. The first hypothesis (Pathway 1 in Figure 2) comes from studies of expertise development that have found that feedback is important for developing expertise (Ericsson, 2006; Ericsson et al., 1993). Peer feedback draws students' attention to areas for improvement, and by implementing feedback in revisions, students practice problem-solving strategies and improve their writing ability. The second hypothesis (Pathway 2) is based on research of improvement via routine practice (Anderson, 1982). Students make revisions after peer review, perhaps triggered by peer feedback or other sources such as friends, parents, or automated computer feedback, and students simply learn to write by revising (regardless of source). If providing feedback and receiving feedback lead to learning through writing performance, the mediation could be through implementation or revisions or both. On the other hand, providing and receiving feedback may lead more directly to learning. The third hypothesis (Pathway 3) is drawn from studies of feedback-based learning, which assumes that students learn by receiving feedback (Hattie & Timperley, 2007; Kluger & DeNisi, 1996), without necessarily implementing peer feedback or making any changes to the draft. For example, students may understand and learn from the praise and problems indicated by peers, but may not take actions in response to peer feedback due to time limitations or already being satisfied with the overall likely document grade. The fourth hypothesis (Pathway 4) is based on studies of learning by observation (Schunk & Zimmerman, 1997, 2007). Learning takes place in the process of providing feedback because students observe advantages and weaknesses in peers' essays. Because they construct solutions for the problems, students transfer what they learn from evaluating their peers' work and providing feedback to new writing tasks. Students report learning from observing strengths and weaknesses in their peers' writing (Nicol et al., 2014; Schunn et al., 2016). However, these effects may be weak due to forgetting, lack of applicability to their own areas of weakness, or complexity of the problem. It is worth noting that the most important difference between the prior research and the current study is that the former focused only on whether peer review led to learning, while the current study aims to answer both whether students learn from peer review (including receiving and providing) and also how they learn from peer review (directly or via revision/implementation work). The current study is not generally testing these broad learning pathways, but rather the open question is more specific to peer review: Which pathways are the primary sources of learning from peer review? The participants for this study consisted of 185 students from two U.S. high schools who were taking the same AP course, AP Language and Composition. Sixty participants came from a Title I school. The remaining 125 participants were from a non–Title I school serving many middle- and high-income families. All participants from a given school were taught by a single teacher across multiple sections; their teachers had agreed to implement shared writing tasks using a shared peer assessment tool at a similar time of year. The participants' age ranged from 16 to 19 years, with the average being 17.1 years (SD = 0.5). Of the 185 participants, 57% were female (3% did not report gender). Among those students reporting their race/ethnicities, White students were the largest group (59%). Among the non-White participants, 31% were Asian, 5% African American, 4% Hispanic/Latinx. 19% of participants chose not to report their race/ethnicities. The composition of each school's participants significantly varied by race/ethnicity, χ2(4) = 20.5; p < .001: the non–Title I school had a higher percentage of Asian students, and the Title I school had a higher percentage of White students (see Supplemental Appendix B in the online version of the journal). Peerceptiv (K. Cho & Schunn, 2007; Schunn et al., 2016) is an online peer assessment program used by a large number of students in high schools and universities throughout the United States and around the world. As a classroom instructional tool, Peerceptiv can be used for formative assessment, allowing students to submit multiple drafts of one document and revise their document based on the peer comments given to the drafts. Within the system, teachers assign writing tasks and specify reviewing assignment details including submission deadlines, number of reviews, reviewing dimensions, and so on. Student writers submit first drafts online, and the program randomly and anonymously distributes each paper to a specified number of student reviewers. On a reviewing form, the reviewers provide diagnostic written comments along with analytic scores on different aspects of writing as specified by the teacher. The rating rubrics usually include details for the rating levels in student-friendly terms. Each reviewer is required to offer at least one written comment on each given dimension of writing, and there are usually suggestions for useful information to include in the comments. To further improve the comment quality, student writers are asked to rate the helpfulness of the comments they received based on a 5-point scale and explain their ratings to the reviewer. There is also grading accountability for accurate ratings and helpful comments (K. Cho & Schunn, 2007; Patchan et al., 2018). Data were collected from two consecutive evidence-based, analytical-writing tasks that are a core part of the AP curriculum and more generally is a common area of struggle for secondary students (National Center for Education Statistics, 2012). Each of these two tasks asked students to read a one-page persuasive writing passage and then write a well-developed essay analyzing the rhetorical strategies used in the source passage. For example, they were required to describe what rhetorical strategies were used, support their descriptions and analysis of rhetorical strategies with evidence taken from the source text, and talk about how the rhetorical strategies connected to the overall thesis. The two tasks, drawn from prior end-of-course AP exams, were similar text-based argument writing tasks with the same requirements and identical rubrics. The only difference was different sources of passages being analyzed, which were selected to be of roughly similar length, difficulty, and reading levels. The first source passage was about the separation between people and nature because of technology, and the second passage was about the effect of migrations. As part of a high-stakes exam, the writing tasks are carefully designed by the College Board to be of equal difficulty. Further, expert scoring of these tasks in a larger study using these writing tasks across more schools found performance on these tasks to have nearly identical mean scores (Schunn et al., 2016). In general, AP courses involve some standardization of curriculum overall and assessments of writing in particular, which makes it easier to create a similarly structured experience across schools. To further reduce implementation variability which would add variance in this regression study, a carefully structured protocol was used in all classes. The protocol was based in best practice to increase the likelihood that learning could be observed. For the period of the study, the teachers participating in the project were provided with shared assignments, shared peer review rubrics, training on the use of the reviewing system, and protocols for training students in conducting peer review. Students wrote a first draft of the first writing task and turned in their first drafts to the online program by a specified deadline. The program distributed essays to peers across classrooms within a school randomly in a double-blind fashion; each student was required to review four peer essays for a given draft using a set of rubrics shared across all classes. Following best-practice, a detailed, rubrics-based comment form was provided to students (see Supplemental Appendix A in the online version of the journal); the rubrics were adapted from ones used by expert AP scorers to make them more student friendly (Schunn et al., 2016). An in-class discussion at the beginning of the reviewing period was used to provide training for students on the peer review task and the peer review system. The teacher shared two sample essays with all the students. Students read the first sample essay, and then were shown example comments for it that were generally unhelpful versus generally helpful (e.g., specific and constructive) and discussed as a class what made reviews helpful. Then students read the second sample essay and completed a review with a partner in class using the assigned reviewing prompt. The class as a whole discussed the comments and ratings that were generated. At this point, the rating scales used in reviewing were discussed and students received calibration comments. Note that high-level comments were emphasized during the peer-review training because addressing high-level problems (e.g., explanation, argument) is more complex than addressing low-level problems (e.g., spelling, grammar). Reviewers then had 1 week to provide comments and rated the essay based on given rubrics. The rubrics directed reviewers to assess eight dimensions of quality on a 7-point scale (see Supplemental Appendix A in the online version of the journal). For each dimension, they wrote comments and provided a quantitative rating. Students were required to provide at least one comment on each dimension. Then writers received peer comments, revised their draft, and submitted the revised draft to the program. To increase the quality of the reviews, the system provided a grade for the accuracy of peer ratings (based on being consistent with other reviewers) and for the helpfulness of peer comments (based on helpfulness ratings made by the authors). Finally, students completed a second writing task after a 1-week interval, using the same rubrics as with the first writing task. The peer review procedure is presented in Figure 3. Essay quality, comments, and essay revisions in response to comments were systematically coded by multiple raters who were iteratively trained and continuously assessed for interrater reliability. To establish reliability levels and increase effective reliability of the resulting data, all data were exhaustively coded by at least two coders, and disagreements were resolved through discussion with a third coder present (Belur et al., 2021). The quality of the essays was determined by two trained writing experts who had years of teaching experience and used the same peer review rubrics (see Supplemental Appendix A in the online version of the journal). The expert rater training began with discussing the rubrics. A random subset of 50 essays were then rated, and differences between two experts' scores greater than 1.5 were discussed and resolved; a mean score was used for analysis in the low-conflict cases. The interrater reliability was substantial (Anthony & Joanne, 2005) for both high-level scores (κ = .73) and low-level scores (κ = .70). Then the raters independently rated the rest of the articles. The improvement in writing quality from the first draft of the first writing task to the first draft of the second writing task serves as the primary measure of learning in this study. In order to determine how many comments students provided and received, the first step of comment coding was to segment the peer comments into idea units. An idea unit refers to a comment made on one particular aspect of the student's writing. It might be a few words, a sentence, or multiple sentences having a unified intended aim. For example, there are two idea units in this peer comment: "This thesis is incomplete. It just copies the prompt and does not make any further analysis. It is not even paraphrased. // The word ‘creat' is misspelled, which impedes understanding of the thesis." The sentences before the double slash mark are one idea unit, explaining a high-level problem. The second idea unit is the last sentence, which suggested a low-level revision. Although students as reviewers were given multiple textboxes to provide separate comments within each of the eight fine-grained rubric categories (see Supplemental Appendix A in the online version of the journal), comments were further segmented by the researchers since reviewers often submitted comments with multiple ideas in one comment box. In particular, all comments were carefully read and were segmented depending on the problems identified. When later coding comments for type, scope, and implementation, the coders further segmented the comments including more than one problem. This resegmentation occurred rarely (2% of cases), suggesting that the prior segmentation step was a highly reliable process. The full segmentation process yielded a total of 6,507 idea units. Then a total number of comments per author and per reviewer were calculated from these data. The scope of a peer comment was coded as high-level or low-level according to the type of the problem the comment intended to touch (see Table 1). High-level comments focused on thesis, argument, rhetorical strategies, evidence for claims, explaining evidence, and organization. Low-level comments considered smaller details including control of language and conventions. Then the total number of received and provided comments per individual were calculated separately for high- and low-level comments. The segmented comments were further divided into praise, summary, or implementable comments that identified problems to improve (see Table 1). A total of 3,605 comments were found to be implementable. Two writing experts who taught undergraduate composition for multiple years coded the actual implementation of all implementable comments, that is, whether the author implemented a change based on the comment in their second draft. First, the revisions were highlighted by comparing the students' first and revised drafts using MS Word's Compare Document tool. Format changes were excluded. Then, evidence that the students had implemented the peer comments was identified by matching the revisions with the comments. A comment was labeled as Implemented if a revision was attributable to it. If no revision was found to be influenced by the comment, it was determined to be Not Implemented. Twelve percent of the implementable comments were coded as vague implementation because they were stated in such vague terms that they could not be coded for implementation—the vast majority of these vague comments were general statements about low-level problems (e.g., "There were many grammatical problems"). The comments coded as vague implementation were excluded from analysis. Coding of implementation at the comment level had moderate reliability (κ = .58). To raise the effective reliability, all comments were double-coded and disagreements were resolved through discussion. Further, the data were analyzed at the level of number of implemented changes per author, further reducing the effect of coding noise. Based on comment focus, two measures were created: number of high-level implementations and number of low-level implementations. Since received comments could produce one or many revisions in a document, and comments given to other students could also lead to revisions, the amount of first-to-second draft revising was separately coded, independent of comments received. In other words, both revisions triggered by peer comments and extra revisions were summed. Each separate revision was identified and coded for focus. A low-level revision was defined as one that did not alter the meaning of the original text, while a high-level revision involved changing the meaning of the original (see examples in Table 1). The MS Word Compare Documents function was used to facilitate revision coding. The first author and two trained research assistants independently labeled each revision as low-level or high-level. The reliability of coding each revision into high versus low was moderate (κ = .67), and again was exhaustively double-coded to raise the effective reliability. Two measures were created: number of high-level revisions and number of low-level revisions. The purpose of the study was to test unique associations between the reviewing behaviors, the revision behaviors, and learning outcomes, separately for high-level and low-level aspects of writing. For this purpose, the use of multiple regression suffices. Structural equation modeling (SEM) was not used because the sample size was too small for SEM (Fritz & MacKinnon, 2007), and instead other techniques were used to address sample size in the mediation analyses. Also note that since there were only two schools, nesting students within schools in a hierarchical linear model also conveys no advantages for modeling the data (McNeish & Stapleton, 2016). However, given the wide variety of variables included coming from different time points, listwise deletion for missing values could be problematic. Four variables had some missing values reflecting missing documents: Task 1 high-level and low-level scores (0.5%), and Task 2 high-level and low-level scores (4.9%). While Little's MCAR (missing completely at random) tests conducted in SPSS 25 suggested that the data using only significant predictors of learning could be treated as missing at random, a broader test of all variables found a significant violation of this assumption for high-level writing variables, χ2(9) = 24.8, p < .005 (Little, 1988). Missing data were initially addressed using EM (expectation maximization). EM sequentially uses the observed data to predict missing values (Expectation step), and then the full data set (observed and estimate) is used to build new prediction equations (Maximization step). Missing values are reestimated using the new prediction equations and the process repeats until the covariance matrix does not change (J. W. Graham, 2009). The analyses were also rerun using multiple imputation (10 imputations using SPSS 25). All the analyses produced the same results (same variables significant/nonsignificant; only minor variation in beta-weights). Since the two approaches are equivalent in this context and expectation maximization is already familiar in peer feedback research (e.g., Strijbos et al., 2010), we reported the findings using EM for missing values. All of the main analyses involved multiple regression, but the particular form of the regression varied depending on outcomes because the distribution assumptions of linear regression were frequently violated. Negative binomial regression (NBR) and zero-inflated Poisson regression (ZIP) models were used when the outcome was implementation or revisions (including both high- and low-levels). Implementations and revisions are count variables, which are frequently right-skewed and cannot be normalized by transformation (e.g., log or square root). Such data are often modeled using Poisson regression (Coxe et al., 2009; Hilbe, 2007). However, Poisson regression requires the mean and variance to be roughly equal, and diagnostic tests of this assumption (i.e., the likelihood-ratio test) applied to implementations and revisions (for both high-level and low-levels) found that mean and variance to be unequal: for high-level implementation, χ2(1) = 929.2, p < .001; for low-level implementation, χ2(1) = 34.8, p < .001; for high-level revisions, χ2(1) = 484.3, p < .001; and for low-level revisions, χ2(1) = 495.8, p < .001. When the assumption of means equal variance is violated, NBR can provide a better fit to the data (Coxe et al., 2009; Hilbe, 2007). However, another problem in count data can be an unusually large number of zeros (e.g., having two modes in the data, with one being at zero), and the implementation and revision distributions showed some evidence of this pattern. If there are excessive zeros in the count distribution (sometimes operationally defined as at least 10% of the data; Blevins et al., 2015), a better fit to the data can be obtained using a ZIP or zero-inflated negative binomial (ZINB) models (Coxe et al., 2009; Hilbe, 2007). It is important to note that using zero-inflated models should be decided based on theoretical grounds, because these models essentially assume two different kinds of processes are taking place: something that causes zero/nonzero counts and then something that influences the count when it is nonzero (e.g., something leading a student to revise at all, and then another thing leading a student to revise more once they have decided to revise at all). If no prior theory supports zero-inflated models, NBR models can be used (Allison, 2012; S. Yang et al., 2017). In the current case, zero-inflated models were considered plausible for both implementations and revisions. For implementation, some students may have received no implementable comments to act upon or they may have decided their peers were not useful sources of comments. For revisions, some students may have decided that doing revision was not worthwhile or necessary. In those cases, the models had two sets of regression results: one set predicting zero implementation/revision and a second set predicting amount of implementation/revision. In sum, ZIP and ZINB models were tested when the outcome was implementations or revisions. The models were compared and a final model was selected based on best fit to the data defined as the smallest AIC (i.e., Akaike information criterion; Coxe et al., 2009; Patchan et al., 2016). The full set of models are presented in Supplemental Appendix E (in the online version of the journal) to show the consistency of findings across models based on different assumptions; which variables were important predictors of outcomes were largely consistent across models. The final, selected models predicting implementations and revisions based on model fit (i.e., AIC) are presented in Table 2. Linear regression models were conducted when the outcome was high-level or low-level learning (first draft score of Task 2, controlling for first draft score of Task 1). According to the scatterplots of studentized residuals and unstandardized predicted value, the data satisfied the homoscedasticity assumption. The linear regression models were also screened for outliers according to leverage, studentized residuals, and Cook's D statistics (Aguinis et al., 2013; Fox, 1991). Examination of tolerance and variance inflation factor (VIF) for each of the independent variables showed that no predictor variable had a VIF greater than 2.4. Therefore, multicollinearity was not problematic. Overall, three sets of statistical models were conducted: predicting revisions, predicting implementations, and predicting learning. In each case, the predictors included the number of comments students received and provided, with control variables of Task 1 score and school. The learning models also included number of revisions and number of implemented comments. Models also included interactions of the key predictors with school to formally test the consistency of effects across the two schools. All of these models were conducted separately at the high-level and low-level writing levels (e.g., high-level comments predicting high-level revisions, high-level implementations, and high-level learning). Unstandardized coefficients for linear regression models and eb (interpretable as odds ratios) for zero-inflated models were reported as measures of effect sizes. All these zero-inflated regression models were conducted in R (see R syntax in Supplemental Appendix G in the online version of the journal). Linear regression models were conducted in SPSS 25. As a final step, mediation of effects of comments on learning via revision or implementation was tested via a bootstrapping technique, which is recommended to test mediated effect with small to moderate samples (Shrout & Bolger, 2002). Mediation analyses included two control variables (Task 1 score and School), two predictors of interest (the number of received and provided comments), the mediator (the number of revisions/implementations) and learning. All of these models were conducted separately for high-level and low-level aspects of writing (e.g., models without and with interaction terms of the number of received/provided comments with school to test if indirect effects [if there were any] were consistent across schools). The PROCESS macro 3.4 for SPSS 25 was used to determine the significance of the indirect effects (Hayes, 2017), and, in particular, it employed 5,000 bootstrapped samples to estimate confidence intervals. If the 95% CI (confidence interval) does not include zero, the indirect effect is significant. Overall, students appeared to improve their writing over the studied window of time: mean first-draft ratings were higher for the second writing task than for the first writing task (see Supplemental Appendix C in the online version of the journal), for both high-level scores, paired t-test t(184) = 4.32, p < .001, Cohen's d = 0.33, and low-level scores, paired t-test t(184) = 3.36, p < .001, d = 0.26. The greater gains in high-level scores are consistent with the greater focus on high-level issues in comments and a higher rate of implementation for high-level comments than low-level comments in revisions (see Supplemental Appendix C in the online version of the journal). It is likely that high-level aspects of writing received more attention than did low-level issues because the criteria provided to students focused more on high-level aspects. As an initial examination of which peer review factors might be associated with the outcome variables (implementation, revision, and second task writing scores), Pearson correlations were examined (see Supplemental Appendix D in the online version of the journal). Within a given level (high-level or low-level aspects of writing), none of the predictors were highly correlated with one another, reducing concerns about collinearity in the analyses. Implementations and revisions were highly correlated with one another as one would expect, but not so highly that they were redundant measures. High-level versus low-level aspects of writing within each measure were also generally correlated at moderate-to-high levels, but again not so highly that the two levels should be collapsed for analysis. There were similar patterns in the correlations of predictors to outcome variables across levels (see Supplemental Appendix D in the online version of the journal). For both high and low levels, implementation was significantly correlated with receiving comments, and revisions were significantly correlated with providing comments and with receiving comments. For both high-level and low-level aspects of writing, the score of the second writing task was significantly correlated with providing comments and revisions. Thus, revision performance was generally correlated with providing and receiving comments, while learning appeared to be correlated more specifically with providing comments and revisions. Next, we report regression analyses that adjust for the moderate levels of covariance among the predictors. Both receiving and providing more comments decreased the odds of being among those who never implemented high-level comments (see binary equation of Model 1 in Table 2). Receiving more high-level comments also resulted in a higher number of implementations. For high-level revisions (see Model 3 in Table 2), receiving and providing more comments also decreased the odds of being among those who made no high-level revisions (see binary equation of Model 3 in Table 2). No variables predicted the quantity of high-level revisions (see count equation of Model 3 in Table 2). Thus, the effects were not always identical across the count and logistic equations (comparing Models 1 and 3), suggesting that their separate treatment is important. For example, both received and provided comments were significantly related to whether or not students implement high-level comments or make general revisions, but only received comments explained the variance in the quantity of high-level implementations (see Table 2). Similar to high-level aspects of writing, the number of low-level comments received predicted the number of level implementations made, but to an even larger extent (see Model 2 in Table 2). Everything else about low-level writing was different from what was found to significantly predict high-level implementation and revision. For example, in contrast to having multiple main effect predictors at the level of making any high-level implementation or revision, there were none for low-level implementation or revision. Instead, especially for low-level revisions, there were more predictors of the amount of work. For example, the number of received comments predicted the number of revisions (see Model 4 in Table 2). Another salient difference for low-level writing was the significant interactions of number of comments with school. Not only did students from the Title I school make fewer low-level revisions after controlling for numbers of comments they received and provided, there were also interactions of school with numbers of provided and received comments. For example, Title I students were more positively influenced by the number of comments they received in terms of making more revisions. Interestingly, the interaction of school with number of provided comments was mixed: For Title I school students, providing more low-level comments was positively correlated with making any implementations (as a dichotomous outcome), but negatively correlated with number of implementations (as a count outcome). This effect will be discussed in more detail in the "General Discussion" section. For both high-level and low-level learning, amount of revisions and amount of provided comments were significant predictors (see Table 3 and Figure 4). Note that providing comments significantly predicted low-level learning (p < .01) on its own, but this direct relationship became only marginally significant (p = .06) when the interaction term of provided comments and school was included. Since revisions were predicted by provided and received comments, there was the possibility of indirect effects via revisions. The mediation analyses revealed that the indirect effects were not statistically significant in the case of received and provided high-level comments (see Models 1 and 2 in Supplemental Appendix F in the online version of the journal). For low-level learning, by contrast, received comments had a significant indirect effect (see Models 3 and 4 in Supplemental Appendix F in the online version of the journal). In no case did the interactions with school produce significant direct, indirect, or total effects. Further, the total effect of providing comments on low-level learning was statistically significant across models including or excluding interactions. Note that high- and low-level implementations were not included in mediation analyses because they did not predict learning significantly. These reported analyses treated the high-level and low-level aspects of commenting, performance, and learning as completely separate from one another. While commenting to each level happened in different prompts, revising may have been connected. Therefore, follow-up data analyses were also conducted to investigate the influence of receiving and providing both high- and low-level comments (independent variables) on high-level scores or low-level scores of the second task (dependent variables). The results supported the initial approach of treating the two levels separately: (a) receiving high-level comments alone predicted high-level implementation and (b) receiving low-level comments alone predicted low-level implementation and low-level revisions. The writing challenges of the 21st century, coupled with consistently poor results from national assessments of writing performance in high school students, have led researchers and educators to call for an increased focus on improving students' writing abilities, especially for argument-based writing (National Center for Education Statistics, 2012). Peer review has been identified as an efficient means for helping students write, but prior research has predominantly focused on the effects of peer review on students' writing performance of the same writing task (i.e., students revise their drafts on receiving peer feedback). Whether peer review can improve learning (i.e., writing in a new writing task) and the pathways through which peer reviewing helps students learn to write were therefore unclear. Understanding the effects of peer review on learning and identification of factors that mediate peer review and learning is important in guiding teachers' effective use of peer review. Figure 4 summarizes the findings regarding significant predictors of writing performance and learning. Note that the connections for the binary outcomes have been sign-reversed since double-negatives are confusing (e.g., the regression result of providing more comments was negatively related to not making any revision was sign-reversed to be providing more comments was positively related to making any revision). Overall, the results indicated that both providing and receiving feedback predicted performance and learning, and in slightly different ways for high- versus low-level aspects of writing. Further, the patterns of results of high versus low were consistent with different learning pathways (learning by observation and learning by routine practice). The findings regarding performance on higher level aspects of writing were partially consistent with the theoretical framework (see Figure 4A). Received and provided feedback increased the probability of implementing feedback and making general revisions. This finding is different from Y. H. Cho and Cho (2011), who found that the effects of receiving feedback on revised draft quality were limited but those of providing feedback were significantly positive. Higher level writing issues (as studied here) are more likely to be shared across authors, and therefore the provided high-level comments should be more likely to overlap with received comments for high-level issues. By receiving and providing high-level feedback targeting similar problems, students develop a better understanding of their weaknesses and are motivated to revise to narrow the gap between their current and desired performance (Nicol & Macfarlane-Dick, 2006). It is possible that receiving feedback was the only significant predictor of the quantity of implementations perhaps because implementation is operationally so closely tied to receiving feedback. When students receive feedback, they focus on the areas for improvement. However, neither receiving nor providing high-level feedback seemed to increase the quantity of revisions. For lower level aspects of writing, the findings were slightly different (see Figure 4B). The interaction term of provided feedback and school positively predicted the probability of implementing any feedback, but negatively predicted the quantity of implementations. In other words, compared with non–Title I school students, Title I school students were more likely to decide to implement feedback but less likely to implement more comments as they provided more feedback. As one possible explanation, providing more feedback could encourage students to decide to implement any feedback, perhaps because providing feedback helped them develop their ability to identify and solve problems (Nicol et al., 2014). However, when it came to the phase of actual implementation, the utility of providing feedback diminished as students provided more feedback. Another possible explanation relates to differences in context. For example, Title I school students might be less motivated to engage in peer review than their non–Title I counterparts. They might think they have benefited from providing feedback so that they did not need to do actual implementations. Students from Title I schools are rarely studied in peer review research, but the topic is important: They received/provided less feedback and responded less on receiving feedback than did students at the non–Title I school. Future research could investigate how student characteristics (e.g., motivation) influence peer review and writing performance. Received feedback predicted the quantity of low-level implementations significantly potentially because receiving more feedback helped students know their low-level problems. Received feedback predicted the quantity of low-level revisions significantly for both schools, with a larger effect for Title I school students. Students from the Title I school might need more feedback to help them know their weaknesses. Provided feedback did not predict the probability of making any revisions and the quantity of revisions perhaps because students thought that they could learn from providing feedback, and it was not necessary to make low-level revisions. Past research has focused exclusively on either implementation or revisions without treating the findings as conceptually different from one another, and the current research suggests that this produces an incomplete picture that potentially explains differences in findings across studies. Some of the differences may have been due to the difficulty in coding implementation for vague comments, for example, for low-level issues. However, since received comments predicted implementation and revisions, the vague-comments problem is unlikely the main driver of the different patterns of results for high-level issues. For high-level aspects of writing, students appeared to learn to write by making revisions triggered or not triggered by received feedback. After accounting for the revisions, there was no additional predictiveness of implementations. Making general revisions enables students to use what they learn from peer review to identify and solve other problems that are not identified by reviewers, and thus, develop their detection and correction skills (M. Yang et al., 2006). Further, there was a significant residual direct effect of providing comments as found by others in studies of providing feedback (K. Cho & MacArthur, 2011; Greenberg, 2015; Lundstrom & Baker, 2009). High-level writing consists of a broad range of component skills (e.g., knowing how to explain the provided evidence, knowledge of logical and clear organization of the essay). In terms of diversity of skills practiced, providing feedback, especially when it involves many different written objects, may provide more opportunities to practice a diverse range of skills than receiving feedback on a particular writing product. Similarly, providing feedback to others might present opportunities to practice detection of writing issues (Chen, 2010; Lundstrom & Baker, 2009), and improving detection skills might be especially important for writing skills. The effects of receiving and providing feedback on learning were not mediated via revisions, perhaps because the effect of number of comments was on making any revisions whereas it was amount of revisions that predicted learning. Similar to high-level aspects of writing, revisions and providing comments were predictors of learning for low-level aspects of writing. However, the indirect effect of receiving comments on learning via revisions was significant in the case of low-level writing. Different from high-level aspects of writing, the number of received comments predicted the quantity of revisions, and thus, there was aligned at the mediator level: quantity of revisions. Interestingly, implementations did not significantly predict learning for both high-level and low-level aspects of writing. Why did implementations not predict learning? It is unlikely that no learning occurs via implementing comments after peer review, given the large amount of research supporting the pathway to learning across a wide range of domains (Astin, 1993; Ericsson et al., 1993; Kellogg & Whiteford, 2009). However, it may be that students benefit more from making general revisions than from implementations. While receiving feedback can provide students with performance goals that are just beyond their current performance but addressable (Ericsson, 2006), making general revisions may provide students with more opportunities to utilize what they learn in revisions beyond making only the specific changes suggested by reviewers. These patterns of the learning effects on low- and high-level dimensions are most consistent with learning by routine practice and learning by observation (Couzijn, 1999; Schunk & Zimmerman, 1997, 2007). In other words, the pattern of results is most consistent with the studies of expertise development, in which students improve by practicing their revising skills after receiving and providing feedback. Students improve through practice that is influenced by immediate feedback from peer reviewers and other sources. Students may also learn to write by providing feedback because it helps them transfer what they learn from detecting problems in others' work/suggesting solutions for the problems, in addition to developing a better understanding of the readers' perspectives (K. Cho & MacArthur, 2011). The findings of the current study provide useful lenses for examining and maximizing the benefits of peer review for secondary school students and beyond. First, they provide additional basic support to adopting peer review in writing instruction at secondary schools. Although peer review has already been identified as a high leverage practice for writing instruction, secondary school students are generally provided with few opportunities to write and receive feedback on their writing (Kiuhara et al., 2009). The current findings reveal that peer reviewing not only helps secondary school students improve a draft but also enhances student learning, particularly, from the activities of providing comments and making revisions after receiving comments. Second, two learning paths have been identified as relevant to learning from peer feedback: learning by routine practice and learning by observation. Students do not narrow their focus on only the feedback they receive, but they appear to take a more active part in writing processes by making general revisions (in response to feedback received and provided) and directly from providing feedback, so that "observational and emulative learning leads to self-control by the learner, automaticity of the cognitive skill, and ultimately to self-regulation" (Van Steendam et al., 2010, p. 318). The current results suggest that particular instructional variations of revision tasks may help optimize student learning: (a) teachers should require students to submit revisions (Boud, 2000), rather than only assigning more writing tasks without revisions (Nicol & Macfarlane-Dick, 2006); (b) students should be encouraged to make general revisions beyond the received feedback rather than just acting on received feedback; and (c) students should be encouraged to provide concrete and specific peer feedback in practice rather than vague feedback. Clear feedback with more information can help authors understand and use peer feedback. Pragmatically, the teacher can emphasize that the general revisions, as well as the received and provided feedback, will help improve future writing performance. Third, the current study was conducted in an online peer assessment system in the context of a specific kind of writing task, with a detailed, student-friendly rubric that was well aligned to the writing task, and incentives for students to provide accurate and constructive peer feedback. Well-structured peer review training was provided to help students develop a better understanding of the rubrics and how to provide adequate feedback. The amount and nature of learning might have been different if the writing task was less well-specified (making for less overlap in provided feedback with own writing), the evaluation criteria were overly general or poorly aligned (making the feedback less relevant or effective), the incentives for high-quality feedback were weak (making for less constructive reviewing), or the peer review was in a traditional face-to-face format (making anonymous and efficient multipeer feedback difficult). That is, the current findings are rooted in strong writing instruction practices, and there might be weaker performance or learning effects in less optimal writing instruction conditions. Fourth, the current study has included a broader focus than was found in prior peer feedback research so that it plays an important foundational role within the larger research space. For example, it investigated the relationships between receiving, providing, feedback implementation, general revisions, and learning, with consideration of a contextual factor (i.e., school title). Based on this study, future research can investigate follow-up learning questions such as how implementations and revisions might influence students' later providing feedback behaviors. Several caveats should be considered. First, the current study examined students from two secondary schools in an AP writing course in the United States. Future research should examine the generalizability of the results in other learning contexts with students at other levels of education and taking different peer assessment forms. Similarly, how receiving and providing peer feedback matter for students taking non-AP courses can also be examined. AP students may benefit more from peer review because AP courses generally involve a more homogeneous group of students who are especially motivated, hardworking, and advanced, while non-AP students are more likely to vary in academic performance, motivation, interests, and so on. In addition, feedback effects may be mediated by other variables than implementations or revisions, such as student characteristics (e.g., motivation, self-efficacy, thinking styles; Hattie & Timperley, 2007; Lin et al., 2001; Shute, 2008). Students with intrinsic versus extrinsic motivation, high versus low self-efficacy, or different thinking styles may engage differently in peer review. Further, many of the AP students from the Title I school might not come from low-income families. It would be helpful to directly examine various components of students' socioeconomic status (e.g., family income, family education levels) to develop a deeper understanding of the observed interactions of peer feedback with school context. Second, more support should be provided for Title I school students, who were found to be less likely to respond to feedback and make fewer revisions than the non–Title I school students. The observed differences might be related to a variety of factors (e.g., motivation, engagement, prior experiences with peer feedback, level of teaching resources, school learning atmosphere; Hattie & Timperley, 2007; Irvin et al., 2011; Shute, 2008). Although it is more difficult to change the contextual factors, students' academic motivation and engagement in writing could be improved. For example, additional training can be provided to help Title I school students provide more clear and meaningful feedback (i.e., with supporting details; Van Steendam et al., 2010). Third, the reciprocal nature of peer review should be taken into consideration when discussing its effects. Because students switch between the roles of assessors and assessees in peer review, what students provide as assessors and how they respond to peer feedback as assessees might change as peer review goes on (Tsivitanidou et al., 2011). Since this peer review process is dynamic, reflective, and interactive, future research can examine "reciprocal" effects between providing and receiving peer feedback acts (e.g., how providing feedback changes receiving feedback and vice versa). Fourth, a number of low-level comments were too vague to be coded for implementation. The corresponding reduction in the amount of detected low-level implementation might have influenced the results, perhaps underestimating the benefits of implementation. On the other hand, the same challenge we faced in coding for implementation could have been a challenge for peers receiving vague comments: Exactly how to implement changes in the document (or learn from the comments) might have been unclear. Finally, the current study is based on correlational analysis, which does not necessarily imply causality. However, correlational analyses are useful for ruling out causes through lack of correlations. Therefore, future experimental research, which is difficult to conduct on many variables at once, can focus on the significant variables highlighted in the current study. Further, future research can track whether the feedback-to-implementation process varies by more fine-grained distinctions. Initial analyses were conducted separately on this data set for each of the specific dimensions and found to produce roughly similar results. However, given multicollinearity issues among specific dimensions within high-level writing, a larger data set would be needed to strongly examine specificity or generality of learning within and between more specific aspects of writing.
10.3102_0002831220946309	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220946309	Competing for Bachelor's Degrees: Are Community Colleges Cutting Into the Market Share of 4-Year Institutions?	 To address local workforce needs and expand access to affordable bachelor's degrees, some states allow community colleges to offer bachelor's degree programs. Despite concerns that community college baccalaureate (CCB) programs will duplicate efforts and cut into the market share of nearby 4-year institutions, extant literature has yet to examine the impact of CCB adoption on bachelor's degree program enrollment and bachelor's degree production at 4-year institutions. Using program-level data, our findings show that local CCB degree programs have a negative effect on overall bachelor's degree enrollment and bachelor's degree production at 4-year institutions, but this effect is concentrated primarily within for-profit 4-year institutions. This study represents the first comprehensive evaluation of the impact of CCB degree programs on neighboring 4-year institutions.	 Competition within education has become a popular topic area for journalists, policymakers, and a host of stakeholders in both K–12 and higher education. Within the K–12 sector, proponents of market-based education reform typically suggest that the proliferation of school choice and alternative K–12 options will enhance competition and thereby increase institutional performance, particularly in low-performing school districts (e.g., Booker et al., 2008). In higher education, both public and private institutions have adopted market-oriented approaches in response to increased competition for students, faculty, and revenue in the form of tuition (Marginson, 2006). Although community colleges (2-year institutions) and 4-year institutions have significant differences in their student compositions and core curricular missions (Cohen & Brawer, 2008), a growing number of states have blurred the line between these institution types by allowing community colleges to offer bachelor's degrees in high-demand and localized program areas (Russell, 2010; Walker, 2005). Traditionally, public community colleges have been restricted by state laws from offering any credentials beyond the associate degree. However, a total of 23 states have adopted community college baccalaureate (CCB) degree programs (Fulton, 2020) in order to address local workforce needs, expand access to bachelor's degrees, and provide affordable bachelor's degree options for place-bound students who may have limited access to a 4-year college or university (Bemmel et al., 2008). Because community colleges provide access to higher education for a disproportionate number of historically underrepresented or nontraditional students, many students who graduate from community colleges do not have access to bachelor's degree programs due to geographic, time, or financial constraints (Floyd & Skolnik, 2019). Given the limited opportunities and significant gaps in bachelor's degree completion for Black and Hispanic students when compared with their White peers (Shapiro et al., 2017), CCB degree programs can offer an alternative pathway to the bachelor's degree for traditionally disadvantaged student types who may not be able to earn their bachelor's degree otherwise. Due to the proliferation of CCB degree programs, the total number of bachelor's degrees conferred at public community colleges increased from 525 in 2001 to 13,283 in 2018 (authors' calculations using Integrated Postsecondary Education Data System [IPEDS] data). Despite the perceived benefits associated with community colleges addressing local workforce needs by offering affordable bachelor's degrees in targeted program areas, some critics have expressed concerns that CCB degree programs detract from the core curricular mission of community colleges and may lead to unintended consequences, such as duplicating the efforts and harming the enrollment numbers of nearby 4-year institutions (e.g., Bailey & Morest, 2004). The majority of CCB-adopting states only grant the authority to offer bachelor's degrees to a small subset of 2-year institutions (Fulton, 2015), but the most robust CCB policies allow the majority of community colleges to confer bachelor's degrees. As an example, 26 of 28 community colleges in Florida have adopted at least one baccalaureate degree program (Florida College System, 2017). While a large and growing number of community colleges now have the authority to award bachelor's degrees, previous research has yet to explore the impact of CCB adoption on the bachelor's degree program enrollment or bachelor's degree production of nearby public, private, and for-profit 4-year institutions. To examine the potential presence of competition between CCB-adopting institutions and traditional 4-year institutions, we leverage detailed program-level data and address the following research questions: Research Question 1: To what extent does CCB adoption affect bachelor's degree program enrollment and bachelor's degree production at nearby 4-year institutions? In addition, over half of all Hispanic students and more than 40% of Black students enrolled in higher education attend community colleges (American Association of Community Colleges, 2017), and these results may be stratified according to students' race/ethnicity. Research Question 2: Does the impact of CCB adoption on bachelor's degree production at nearby 4-year institutions vary according to students' race/ethnicity? In this study, we draw from a unique program-level panel data set to examine changes in bachelor's degree program enrollment and bachelor's degree production at 4-year institutions before and after the adoption of localized CCB degree programs. We employ a quasi-experimental variant of a difference-in-difference-in-differences (DDD) analytical approach to examine the effects of CCB adoption on bachelor's degree production at nearby 4-year institutions. By using a DDD approach, we capitalize on program-level enrollment and degree completion data. This approach allows us to account for not only institutional and time factors but also program-level dynamics. In doing so, we make two meaningful contributions. First, we add to the limited body of evidence on the impact (and potentially unintended consequences) of CCB degree programs. Second, this study leverages a unique data set to estimate the competitive forces between institution types and across sectors of higher education. Despite the growing demand for higher education, there is, in theory, a fixed number of students interested in earning their bachelor's degree (Paulsen & Pogue, 1988). Competition for these students, and the resources they bring, drives institutional policies and plays a role in broader state and federal policy considerations. Our results illustrate that CCB adoption creates a competitive market for students that is concentrated primarily between community colleges and for-profit institutions. Although community colleges face their share of limitations, such as a lack of resources relative to flagship research universities (Hendrick et al., 2006) and low transfer and completion rates (Mullin, 2010), they continue to play an essential role within the broader higher education landscape. For example, community colleges have been lauded for providing the necessary developmental education for academically underprepared students (Bemmel et al., 2008; Bragg et al., 2006; Cohen & Brawer, 2003; Walker & Pendleton, 2013; Witt et al., 1994) and offering mobility pathways for working or low-income students (Armstrong & Hamilton, 2013). While subbaccalaureate degree production has been, and continues to be, a major focus area of the community college mission, community colleges have multiple curricular missions and face unique pressures to be responsive to their local workforce. Given the political and fiscal pressures faced by community colleges to serve their local constituencies, the implementation of the curricular missions may vary across community colleges, leading to a growing number of community colleges offering bachelor's degrees in high-demand and localized degree programs (Bahr & Gross, 2016). In 1989, West Virginia was the first state to authorize the adoption of a CCB degree program (Fulton, 2015). As of 2020, 23 states granted permission for community colleges to confer bachelor's degrees, but 21 of those 23 states had four or fewer community colleges offering bachelor's degrees, with the majority of participating community colleges administering fewer than seven baccalaureate degree programs (Fulton, 2020). The increasingly popular practice of offering CCB degree programs has been described previously as a mechanism to respond to local and state workforce demands (Russell, 2010; Walker, 2005), reduce costs for students and taxpayers (Bemmel et al., 2008), and expand access for place-bound students who do not live near a 4-year institution (Bemmel et al., 2008). Although CCB policies are designed to prevent CCB-adopting institutions from harming the enrollment or completion outcomes of traditional public 4-year institutions, private 4-year institutions (both for-profit and not-for-profit) may be negatively affected by CCB adoption due, at least in part, to the affordability of CCB degree programs. The implementation of CCB degree programs has the potential to not only lower the prices paid by baccalaureate-seeking students but also reduce the costs borne by taxpayers. For example, the average published tuition and fees for full-time students in Florida during the 2016–2017 academic year was $2,890 for community colleges, $5,924 for public 4-year institutions, $20,954 for private (nonprofit) 4-year institutions, and $13,748 for for-profit 4-year institutions. In addition, the state of Florida provides average per–full-time equivalent subsidies (i.e., state appropriation) of $4,958 to community colleges as compared with $10,510 for public 4-year institutions during the 2016-2017 academic year (authors' calculations using IPEDS data). Despite the potential benefits associated with offering affordable and accessible bachelor's degrees, critics of CCB adoption have suggested that CCBs may do more harm than good by creating an environment in which community colleges emulate 4-year institutions rather than fulfill their traditional role and purpose within higher education. Levin (2004) suggests that CCB degree programs are unnecessary and expensive duplications of content and services already provided by traditional 4-year institutions. By duplicating the efforts of 4-year institutions, community colleges that adopt bachelor's degree programs have been accused of drifting from their intended curricular mission related to subbaccalaureate degree production in an effort to enter the market for 4-year students (Floyd & Walker, 2008; Labaree, 2010, 2017; Russell, 2010; Walker, 2005). The notion of mission drift has been defined previously as "a well-known phenomenon in American higher education in which one segment of higher education redefines its mission to include the responsibility already being performed by another" (Kerr, 2001, p. 3), but references to mission drift when critiquing the logic of CCB adoption often fail to acknowledge the multiple curricular missions of community colleges, including the need to offer credentials in alignment with local workforce needs. Proponents of CCB adoption have argued that CCB degree programs provide an affordable, low-cost alternative to traditional baccalaureate degrees (Jacobs & Dougherty, 2006; Meyer, 2006; Russell 2010; Walker, 2005). Prior literature has shown that community colleges typically provide baccalaureate degree options that traditional 4-year colleges and universities are not able to provide, such as alternative scheduling, more affordable pathways to the bachelor's degree (Bemmel et al., 2008; A. Cook, 2000), and better, more targeted student services (Skolnik & Floyd, 2005; Troumpoucis, 2004). Skolnik (2008) noted the clear distinctions between CCB-adopting community colleges and 4-year institutions, as factors related to the occupational foci, applied teaching methods, student populations, and faculty-teaching orientation differ considerably at community colleges versus 4-year colleges and universities. Advocates of CCB adoption have also noted that CCB degree programs center the needs of the local constituency by responding directly to high-demand areas identified by local employers (Cohen, 2003; Floyd et al., 2005; Floyd & Walker 2008; Garmon, 2002; Walker, 2001, 2006). Projected shortages in high-demand fields, particularly occupational areas focused on public service, are problematic developments for economies at both the local and state level (Garmon, 2006). Ignash and Kotun (2005) described CCB adoption as a direct response to the unwillingness of traditional 4-year institutions to meet local workforce needs in applied, technical, and occupational fields, with other scholars noting that CCB adoption represents a response to unfilled workforce-related needs (Floyd & Walker, 2003; Furlong, 2003; Walker, 2001) and potentially a step to reduce overcrowding at selected 4-year universities (Lewin, 2009). Numerous studies examining the influence of CCB adoption have utilized qualitative methods, focusing on policy antecedents and motivational forces surrounding CCB implementation (Hrabak, 2009). In a qualitative case study of three states, Gonzalez (2005) designed an instrument to allow community colleges to begin the process of determining whether to offer a bachelor's degree. McKee (2001) also conducted a qualitative case study to identify issues surrounding the development of CCB degree programs. In addition, Petry (2006) found that administrators from five CCB-adopting institutions in the state of Florida developed CCB degree programs to increase access to bachelor's degrees and improve workforce development in their local areas. Much of the research related to the state policy decision to allow community colleges to offer bachelor's degrees has focused on the relationship between CCB adoption and the community college mission. Floyd et al. (2008) conducted a qualitative study and found that CCB adoption created challenges for participating community colleges seeking to compete with 4-year institutions by adding CCB degree programs while continuing to maintain their focus on the traditional subbaccalaureate community college mission. Another study found that several community college presidents considered CCB adoption as a mechanism to gain affiliation with the nearby 4-year colleges and universities with higher status, noting that the sub-baccalaureate community college mission may be getting "pushed away" (Essink, 2013, p. 74). Additional research has shown that CCB adoption has a positive influence on overall associate degree production (Ortagus et al., 2019). Recent work has also examined the financial implications of CCB degree programs, finding that CCB-adopting institutions increase their tuition and fees (Ortagus & Hu, 2019) and decrease their reliance on appropriations (Ortagus & Hu, 2020). Although the stated purpose of CCB adoption is to expand access to bachelor's degrees to better serve the local community's workforce needs, extant literature lacks any empirical study of the extent to which CCB adoption may cut into the market share of nearby 4-year institutions rather than expand access to bachelor's degrees offered within a given state. In 2001, the Florida Education Governance Reorganization Implementation Act changed Florida's K-20 educational system and provided community colleges with the ability to confer a limited number of baccalaureate degrees. Florida Senate Bill 1162 officially granted St. Petersburg Junior College the ability to confer baccalaureate degrees (Florida Senate, 2003a, 2003b). Florida ratified CCB adoption to help meet the "critical statewide need for trained teachers, nurses, and information technology employees" (Florida Department of Education, 2005, p. 1). The state of Florida later expanded the legislative authority of community colleges to offer CCB degree programs in areas of workforce development in 2008, targeting specific counties with nursing and education needs (Florida Department of Education, 2008). Similar to other CCB-adopting states, CCB degree programs in Florida are designed to address local workforce shortages in high-demand fields (Bilsky et al., 2012; Floyd et al., 2008; Hanson, 2009; Walker, 2005), such as nursing, teacher education, and information technology (Daun-Barnett & Escalante, 2014). CCB degree programs in Florida, and across CCB-adopting states, are typically concentrated within applied fields with local workforce shortages in order to complement, rather than compete with, nearby 4-year institutions. The logic of the complementary role of CCB degree programs is that nearby 4-year institutions may not have the capacity to serve all students, particularly place-bound students, who would benefit from earning a bachelor's degree in a high-demand field (Bilsky et al., 2012; Russell, 2010; Floyd & Walker, 2008). Community colleges in the state of Florida are organized according to designated service areas based on county boundaries. Each community college in Florida has a specific county, or set of counties, in which it is authorized to actively recruit students to enroll. Given the open-access mission of community colleges in Florida, students outside of the designated service area(s) are not precluded from enrolling at any community college within the state; however, a Florida community college cannot actively promote programs or recruit students for enrollment in counties not included within their designated service area. Because of the well-defined service areas in Florida, community colleges have developed individual articulation agreements with their local 4-year institution that are stronger than the statewide articulation agreement in order to facilitate local student transfer. Figure 1 provides a graphical representation of the Florida counties served by CCB degree programs, including a depiction of the distribution and density of CCB degree programs. Visual evidence of the distribution of CCB degree programs shows clusters of high-density CCB adoption in the southeastern region of the state and nonadoption in the northwestern region. Figure 2 illustrates the statewide distribution of 4-year institutions granting bachelor's degrees or higher, indicating clusters of 4-year institutions in community college service areas that are typically in close proximity to urban environments. In 2017, 8 of the 28 community college service areas do not include any 4-year institutions, while 18 of the 28 community college service areas do not include a publicly funded 4-year institution. CCB degree programs in Florida were stipulated to be concentrated in specific occupational fields that met proximate workforce needs (Floyd & Walker, 2008; Floyd et al., 2008) and did not duplicate the offerings of public 4-year universities (Floyd, 2006). The state of Florida has become a national leader for the CCB movement (Fulton, 2015). As of 2017, 24 of the 28 community colleges offer approximately 185 baccalaureate degree programs (Florida College System, 2017). In Table 1, we provide an overview of the number of total CCBs adopted, the number of total unique two-digit CCB programs adopted, and the year of first adoption by community college. The number of CCBs adopted range between one and 22—with Saint Petersburg College representing the largest and earliest adopter of CCB programs in the state. In Table 2, we explore the adoption of CCBs by two-digit CIP (Classification of Instructional Programs) code as well as the overlap in 4-year institutions within the community college's service area. We see that Education, Health Professions, and Business programs represent over 70% of all CCBs adopted across the state of Florida. Additionally, these three areas also have over 70% overlap with a 4-year institution within their service area. The majority of CCB degree programs overlap with at least one type of a 4-year institution, with most overlapping with private and for-profit 4-year institutions. Figure 3 illustrates the number of CCB degree programs adopted each year and the market share of statewide bachelor's degree production by CCB-adopting institutions in Florida over time. Between 2001 and 2006, a small number of CCB degree programs were approved by the state of Florida. There was a significant increase in the number of CCB degree programs adopted in Florida between 2007 and 2013, with 72% of all CCB degree programs in the state being adopted during this time period. Due to the growth in the number of the CCB degree programs in Florida, the market share of bachelor's degree recipients who attended community colleges has grown over time. By 2017, approximately 3% of all statewide bachelor's degrees were offered via CCB degree programs. Higher education researchers have long sought to identify and analyze patterns of organizational responses to external pressures. According to institutional theory, colleges and universities engage in mission drift in order to improve their status and position relative to other postsecondary institutions (H. D. Meyer & Rowan, 2006; J. W. Meyer & Rowan, 1977; Morphew & Huisman, 2002; Scott, 1987, 2014). For community colleges, institutional theory can explain why they would adopt new programs rather than maximize the efficiency within their existing offerings (Morphew & Huisman, 2002), as CCB-adopting institutions are able to adopt the rituals, programs, processes, and structures viewed as legitimate by external actors, such as state policymakers or 4-year institutional leaders (H. D. Meyer & Rowan, 2006; J. W. Meyer & Rowan, 1977; Scott, 1987, 2014; Scott & Davis, 2007). Although institutional theorists often view organizations as separate units engaging in behavior in response to external stimuli, organizations that respond to similar external forces tend to take on similar organizational structures; this phenomenon has been identified by sociologists as institutional isomorphism (e.g., DiMaggio & Powell, 1983). DiMaggio and Powell defined institutional isomorphism as a process of homogenization where organizations functioning within the same environment and under similar conditions come to resemble one another, arguing that increasing external pressures, combined with ambiguous environmental variables, have driven some colleges to converge their organizational structures by imitating high-status institutions. Traditionally, isomorphism explains the capacity of the external environment to stimulate similarities in structures and practices across organizations (DiMaggio, 1988; DiMaggio & Powell, 1983; H. D. Meyer & Rowan, 2006; J. W. Meyer & Rowan, 1977), but isomorphic change in the form of CCB adoption appears to be the result of voluntary imitation of 4-year institutions as motivated by an increase in the long-term stability of the organization and improved likelihood of survival (e.g., Bastedo, 2006; Scott & Davis, 2007). Institutional theory also suggests that colleges represent apt examples of highly institutionalized organizations in which rationalized formal structures allow organizations to gain legitimacy, stability, and resources (H. D. Meyer & Rowan, 2006; J. W. Meyer & Rowan, 1977). Community colleges may seek to emulate the practices of 4-year institutions to improve their legitimacy and offer educational credentials (bachelor's degrees) deemed more valuable to prospective students and employers (Person et al., 2006). This emulation of 4-year practices by community colleges creates an environment in which community colleges may be forced to compete with 4-year institutions for scarce resources. As community colleges enter the market for 4-year students, prior work by DiMaggio (1988) and Scott (2014) suggests that both institution types may struggle to gain influence as students may be forced to choose between new institutional forms (CCB-adopting institutions) or the preservation of existing institutional forms (traditional 4-year institutions). Community college administrators have been described in previous work as constrained actors seeking to maximize the benefits that accrue to the institutions they manage (Dougherty, 1994). In line with the logic of a resource dependence perspective (Pfeffer & Salancik, 1978), community college administrators are constrained by the external environment due to their high level of reliance on external resources (e.g., local and state appropriations). External resource providers can constrain organizational actors' behaviors when the resource being provided is both critical and not easily obtained from another source (Dougherty, 1994; Emerson, 1962). At public community colleges, administrators may alter or constrain their behavior in order to ensure that they maintain their share of a critical funding source in the form of government appropriations (e.g., Harnisch, 2011). In recent years, community college administrators have also been constrained by relatively consistent declines in the total number and relative share of public community college students (National Student Clearinghouse Research Center, 2019). Community colleges continue to face financial challenges due to constrained resources caused, at least in part, by decreasing enrollment numbers and disproportionately low levels of state appropriations relative to 4-year institutions (Snyder, 2018; Zumeta et al., 2012). Even though CCB adoption could be considered a source of mission conflict that undercuts the community college's traditional subbaccalaureate and transfer missions (Dougherty & Townsend, 2006), administrators at community colleges in CCB-adopting states may feel compelled to diversify their revenue streams by generating additional tuition and fees through the provision of high-demand bachelor's degree programs. This study leverages secondary program-level data for baccalaureate-granting institutions to examine the impact of CCB degree programs on the enrollment and bachelor's degree production of nearby 4-year institutions in the state of Florida. Given the available data at the program level, our analytical sample includes public, private nonprofit, and private for-profit 4-year institutions in existence between 1995 and 2017. Within our data set, we have 123 unique 4-year institutions, but we removed any institutions that had 70% or more of their awarded degrees at an associate degree level or less (i.e., certificate) as well as any institutions that were coded as a public 2-year institution within the IPEDS data. The logic of this decision was due to the reclassification of community colleges (or 2-year institutions) as 4-year institutions after they began awarding bachelor's degrees. Finally, we limited our sample to institutions in operation during the time period of our analytical sample to ensure that institutional openings and closures did not confound our estimates. As mentioned previously, we define "nearby" 4-year institutions as those located in the same county-based service area as the CCB-adopting institution. The logic of this decision rests on the notion that community colleges in the state of Florida are unable to recruit prospective students outside of their county-based service area, suggesting that students outside of a community college's service area are unlikely to decide between enrolling at that particular community college or its nearby 4-year institution. As a robustness check, we specify alternative models in which we define "nearby" 4-year institutions as those within a 100 or 150-mile radius of the CCB-adopting institution. Given that the mean distance between home and college is 52 miles and the median distance is 11 miles (U.S. Department of Education, 2014), our distance-based definition of a "nearby" institution represents a conservative measure. Our results are robust to either the county-based or distance-based definitions of "nearby" 4-year institutions, but we use the former definition in models reported below. Table 3 provides the descriptive statistics of key institutional covariates used within our specified models. Specifically, this table compares 4-year institutions within a county served by a CCB degree program with 4-year institutions located in a county not served by a CCB degree program. On average, 4-year institutions with a closely located community college offering at least one CCB degree program showed lower undergraduate enrollment, but other key institutional factors were not significantly different across the two groups. County-level factors appear to differ significantly across CCB and non-CCB institutions to a greater extent. This result aligns with the state policy narrative that CCB degree programs were originally intended to support counties that have large labor market shortages and limited access to bachelor's degree–granting institutions. We operationalize our program-level data through the two-digit CIP classification. CIP is a standardized reporting taxonomy generated by the U.S. Department of Education's National Center for Education Statistics to track and report fields of study and program completion activity. To this end, we created a dyadic panel data set where the unit of analysis shifts from the institution (College X) to the dyad of institution and two-digit CIP code (CollegeX_CIP). This approach allows for micro-level analyses while also accounting for trends at the individual program and institution levels. Our two primary outcomes are (1) bachelor's degree program enrollment and (2) bachelor's degree production. By looking at both enrollment and completion outcomes, we can estimate the impact of CCB adoption on the short-term enrollment outcomes related to college access and long-term outcomes related to student success. For our short-term outcome of bachelor's degree program enrollment, IPEDS collects program-level undergraduate enrollment data for six CIP codes—Education (13); Engineering (14); Biological Sciences/Life Sciences (26); Mathematics (27); Physical Sciences (40); and Business Management and Administrative Services (52). We use these CIP codes to analyze the impact of CCB adoption on bachelor's degree program enrollment. Of the six enrollment CIP codes, three were subjected to CCB adoption and three were not, which allows for enough variation to estimate a policy effect over time. Due to data limitations, the undergraduate degree programs included within our program-level enrollment outcomes are limited to the six CIP codes highlighted above. Our second outcome of interest is captured by the number of program-level bachelor's degrees produced. We focus on bachelor's degree production in addition to our enrollment outcome primarily due to broad national interest in student success and bachelor's degree production. In addition, CCB policies are often adopted as a mechanism to increase bachelor's degree production while simultaneously responding to local labor market demand. Table 4 examines changes in institution-level bachelor's degree production outcomes for 4-year institutions in the year prior to the Florida CCB legislation (2000) and the final year of our analytical data set (2014). Across both bachelor's degree program enrollment and bachelor's degree production outcomes, there appears to be a widening gap in the number enrolled and degrees produced between 4-year institutions with a local CCB degree program and those without one. For example, between 1997 and 2017, total bachelor's degree program enrollment increased by 44.8% for institutions without an active CCB policy in their county but increased 67.2% for those with an active CCB policy. A similar growth differential is seen in bachelor's degree production, as 4-year institutions without an active CCB policy increased degree production by 61.6% relative to an increase of 85.1% for 4-year institutions with an active CCB policy. These are institution-level estimates and do not account for program-specific adoptions, which will be examined later. The data from the Florida Department of Education were then merged with data from National Center for Education Statistics' IPEDS to gather institution-level factors. Factors, such as cost of attendance, institution-level student demographics, and institutional expenditure data, were included as control variables that may have influenced changes in enrollment and bachelor's degree completion. Our primary independent variable is a binary indicator that signals if a 4-year institution's two-digit CIP code was in a county served by a CCB degree program within the same two-digit CIP code. Within the state of Florida, community colleges have well-defined and distinct county-level service areas in which they can actively recruit students. While they are not prohibited from enrolling students from nonservice counties, the vast majority of students enrolled in a community college are from its surrounding area and they are restricted from actively marketing programs outside of their service area. All Florida counties are served by a single community college, but not all counties have a 4-year institution located within them. Our decision to designate 4-year institutions with two-digit CIP codes to "treat" them as if they were located in the service county of a 2-year institution in that corresponding CIP code is purposeful given the context described above. We use a DDD framework to identify the causal effects of the adoption of CCB degree programs on bachelor's degree program enrollment and bachelor's degree production at 4-year institutions. Florida allows for the identification of this naturally occurring experimental framework, as the state is one of the largest adopters of CCB degree programs. Community colleges, or 2-year institutions, are legislatively mandated to service and recruit from specified counties within the state. Additionally, CCBs are program-specific and allow us to capitalize on program-level degree production data to examine the micro-level effects of this policy change. This study employs program-level data to estimate the impacts of local CCB adoption on bachelor's degree program enrollment and bachelor's degree production at nearby 4-year institutions. Drawing from prior work by Baker (2016) and Ortagus and Hu (2019), we use a DDD approach to estimate the effects of CCB adoption on our specified outcomes. Using the logic of difference-in-differences (DiD) parameters as our base, we can compare CCB-adopting institutions with nonadopters before and after CCB legislation (the decision to offer CCB degree programs is made by individual community colleges). Since CCB legislation targeted high-demand bachelor's degree program areas in response to local workforce shortages (Moore et al., 2014), our DDD approach allows us to account for any potential bias in the types of degree programs selected for CCB adoption. In a regression framework, the DDD estimation strategy can be expressed as[MATH](1) where Yipt, is the outcome of interest (bachelor's degree program enrollment or bachelor's degree production) for institution i, program p, in year t. [MATH]is a dummy variable that indicates a 1 for 4-year institutions whose local community college adopted a CCB any time after 2001 and 0 for those that have not. Postt indicates the years after the CCB adoption period, and [MATH] indicates a 1 if the given CIP code (program) has been approved as a CCB adopting program. [MATH] controls the trends within treated programs within treated institutions, [MATH] represents specific time trends of the treated program, and [MATH] represents specific time trends for treated institutions. Our DDD coefficient of interest would be [MATH] which represents the changes in program-specific bachelor's degree production at 4-year institutions subjected to the local CCBs' pressures in the adoption period. Finally, [MATH] is the county-by-program-by-year clustered standard error. The decision to cluster the standard errors at the county-by-program-by-year level aligns with the prior recommendation of Abadie et al. (2017), who report that standard errors should be clustered at the same level at which the treatment variation occurs. As we discuss later in this article, our fully specified model isolates the effect of CCB adoption on the county-by-program-by-year variation when examining both bachelor's degree program enrollment and bachelor's degree production. Despite legislative approval for CCBs in 2001, not all adopting institutions implemented their respective CCBs immediately. To this end, we have variations in the initial adoption year at both the institution and program levels. To account for these variations, we extend Equation (1) and implement a generalized difference-in-difference-in-differences (GDDD) model. Following the logic implemented by Ortagus and Hu (2019), we specified our GDDD model as Equation (2):[MATH](2) where [MATH] is our outcome of interest in 4-year institution i for degree program p during year t. [MATH] is the coefficient of interest (GDDD indicator), which equals 1 in the year in which a 4-year institution's program had a corresponding CCB degree program adopted locally in an adopting community college and thereafter; otherwise, the indicator equals 0. The remaining terms represent a set of terms that are fixed effects to account for the interactions specified in Equation (1). [MATH] is the vector of program by institutional fixed-effects, [MATH] is an institution by year fixed-effects, [MATH] is a vector of program by year fixed-effects, and [MATH] are institutional, program, and year fixed-effects. To account for the potential of localized, program-specific changes in the demand for certain bachelor's degree programs that could influence our estimates, we include a county-by- program linear time trend ([MATH]). In alignment with prior work by Baker (2016) that uses a DDD approach, we do not include any time-varying covariates at the institution or county levels, as they would be collinear with our set of fixed effects. Finally, [MATH] represents a robust clustered, at the institutional level, standard error term that fluctuates across time. Again, our fully specified main effects models isolate the county-by-program-by-year variation. The benefit of this model specification is the full control of college-specific time effects common across academic programs combined with time-varying program effects. This specification models out any influence on the outcomes related to specific programs at individual institutions, specific programs across institutions for a particular year, and specific institutions in a particular year. The estimates on [MATH] indicate the outcome and enrollment impacts related to the adoption of CCBs. To further test the differential effects of CCB adoption by institutional type and Carnegie classification, we incorporate interaction terms with our GDDD coefficient of interest:[MATH](3) where we extend our main effects GDDD specification to include a binary indicator for private institutions ([MATH] and for-profit 4-year institutions ([MATH] where the indicator is equal to 1 for institutions holding each of those distinctions and 0 for others. We then incorporate two interactions terms to capture the varying effects of the policy of private 4-year institutions [MATH] and for-profit 4-year institutions ([MATH]), the DDD coefficient of Equation 6 ([MATH] now becomes the causal effect of CCB presence for public 4-year institutions. The difficulty in any quasi-experimental design is identifying the counterfactual in the absence of policy adoption. Our GDDD design allows this study to approximate the impact of nonadoption in adopting programs using nonadopting programs and nonadopting institutions as controls. This approach produces estimates of what could have occurred within the outcomes if the CCB had not been adopted. This counterfactual approach assumes that treatment and control units following similar (or parallel) prepolicy patterns, and the resulting variations in the outcome can be attributed to policy adoption. Although the parallel trends assumption is impossible to test in this context, our study employs two techniques to attempt to address potential concerns regarding the parallel trends assumption. First, this study introduces county-by-CIP time to guard against nonparallel trends at the county-by-CIP level. Our addition of a robust set of fixed-effects accounts for the potential that adopting programs within adopting institutions may have experienced differences in the outcomes of interest before adopting a CCB. Second, we employ falsification tests to overcome a significant concern with quasi-experimental approaches by untangling the policy effect from a potential corresponding time effect (T. D. Cook & Campbell, 1986). To this end, we artificially created the adoption of CCB degree programs years prior to the actual adoption. This approach allows the results to be viewed in context. Significant results prior to the actual adoption signal that the estimated impact on bachelor's degree production was not a product of CCB adoption but rather a time-related effect that coincides with the adoption of CCB degree programs. Finally, our decision to focus on bachelor's degree production as an outcome of interest may lead to additional threats to validity that arise when examining the effect of CCB adoption on a downstream outcome, such as bachelor's degree completion. Because bachelor's degree completion is a product of not only competition in enrollment but also additional downstream processes, such as student transfer and attrition, the effect of CCB adoption on bachelor's degree production may be difficult to isolate and identify as causal. We argue that CCB adoption should not affect the mechanisms at play for traditional 4-year institutions to graduate students and attempt to account for these complexities and threats to validity through a host of robustness checks and falsification tests to be outlined in detail below. Table 5 presents estimates from our GDDD model for both our main effects and interacted models. Each model is fully specified, including our previously discussed interacted fixed-effects specifications and county-by-program linear time trends. This approach is in line with prior work using the DDD approach (e.g., Baker, 2016). We present the 1-year lagged CCB adoption indicator for our bachelor's degree program enrollment outcome, as we would expect a lag between CCB adoption and enrollment in one of the CCB degree programs. For the bachelor's degree production variable, we show 4- and 6-year lagged CCB adoption indicators. Similar to the logic we employed when using a lagged enrollment outcome, we would expect a substantial lag between CCB adoption and the completion of a bachelor's degree given the time required to enroll (or transfer) into a CCB program, complete required coursework and meet requirements to graduate. The decision to provide lags of 4 and 6 years when examining bachelor's degree completion is aligned with a number of prior high-quality studies that specify models to capture both 100% and 150% graduation time (e.g., Goenner & Snaith, 2004; Smith & Stange, 2016). Model (1) in Table 5 presents the overall effect of CCB adoption on bachelor's degree program enrollment (1 year after implementing CCB degree programs) and bachelor's degree production (4 and 6 years after implementing CCB degree programs). Overall, we find consistent evidence that the presence of CCB degree programs reduces bachelor's degree program enrollment and bachelor's degree production in the corresponding programs at 4-year institutions within that community college's service area. For bachelor's degree program enrollment 1 year after implementing CCB degree programs, we find that CCB adoption is associated with a 12.5% decrease in bachelor's degree program enrollment within competing programs at all 4-year institutions within the CCB-adopting service area. We find similar evidence related to the impact of CCB adoption on bachelor's degree production. After 6 years, the implementation of CCB degree programs is associated with an 8.6% decrease in bachelor's degree production at nearby 4-year institutions. Given the overall estimates suggesting that CCB adoption reduces bachelor's degree program enrollment and bachelor's degree production, we focused specifically on the role of institutional sector to determine whether a specific sector was driving our overall findings. Model (2) in Table 5 provides the interaction effects between institutional sector indicator and the CCB adoption indicator. Using the same lagged outcomes as described previously, we estimate the differential effects of CCB adoption for private (nonprofit) 4-year institutions, private (for-profit) 4-year institutions, and public 4-year institutions. In general, we find statistically significant and consistent evidence of a differential effect of CCB adoption by institutional sector. For both public 4-year institutions and private (nonprofit) 4-year institutions, we do not find any evidence that CCB adoption is related to bachelor's degree program enrollment or bachelor's degree production. However, we find statistically significant and consistent evidence suggesting that CCB adoption has a negative impact on both bachelor's degree program enrollment and bachelor's degree production at private (for-profit) 4-year institutions. To contextualize our findings, we report the extent to which CCB degree programs draw from for-profit 4-year institutions. Specifically, the average program-level enrollment in CCB degree programs is approximately 642 students, and roughly one-third of those students are drawn from for-profit 4-year institutions. For bachelor's degree program enrollment at for-profit 4-year institutions, we find that CCB adoption significantly reduces bachelor's degree program enrollment between 27.7% and 19.4% in the first and second year following CCB adoption, respectively. We find similar evidence of a reduction in bachelor's degree production at for-profit 4-year institutions between 10.8% and 13% 4 and 6 years following CCB adoption. Table 6 offers further analyses of the impact of CCB adoption on 4-year institutions' bachelor's degree production by examining heterogeneous effects by race/ethnicity. We find that the presence of a local CCB degree program has a negative impact on bachelor's degree production at for-profit institutions for Hispanic and White. Specifically, Hispanic students' bachelor's degree production, within treated CIP codes, significantly decreased at for-profit institutions by 20.9%. Bachelor's degree production for White students at for-profit institutions also decreased significantly by 11.5% (the overall negative impact of CCB adoption on bachelor's degree production for White students was concentrated within for-profit institutions). While not significant, we find suggestive evidence that Black students are also substituting CCB programs for similar degree programs at for-profit institutions, although at a potentially much smaller magnitude. Given the time-varying nature of implementation of CCB degree programs after community colleges were granted legislative authority to confer bachelor's degrees, we can trace the dynamic effects both before and after local CCB adoption. To complete our falsification tests, we examined whether our main effects were influenced by other exogenous factors or additional policies separate from the implementation of CCB degree programs. To do so, we tested for significant estimates of our treatment effect prior to implementing the time-varying CCB degree programs as well as after CCB adoption. Following prior work by Dee and Murphy (2019), we estimate our falsification tests using the fully specified Equation (2) but vary the lead and lag time of our program-level CCB adoption indicator. We conduct two separate falsification tests—one on our bachelor's degree program enrollment outcome and the other on our bachelor's degree production outcome. In Figure 4, we show estimates of our treatment effect on bachelor's degree program enrollment for the 3 years priors and 6 years after CCB adoption. We find consistent and relatively precisely estimated zero effects of CCB implementation on bachelor's degree program enrollment in the 3 years prior to CCB adoption. We find suggestive evidence of negative effects beginning in the first year after CCB adoption and statistically significant negative effects that persist for 6 years following the implementation of local CCB degree programs. The results from our falsification tests align with our a priori assumptions that the effects should be close to zero and not statistically significant prior to local CCB adoption, with statistically significant effects on the enrollment outcomes after CCB adoption. We conducted another falsification test for the bachelor's degree production outcome (Figure 5). Similar to our bachelor's degree enrollment outcome, we would not expect to see a statistically significant treatment effect for the years prior to CCB adoption. Unlike the bachelor's degree enrollment outcome, which should have a more immediate effect, we would not expect to see significant changes in bachelor's degree production until at least the fourth year following CCB adoption. Figure 5 shows no significant effect prior to or through the third year following the implementation of a local CCB degree program, offers suggestive evidence of an effect in the fourth year following CCB adoption, and displays a larger and statistically significant effect beginning in the fifth year following CCB adoption. The pattern depicted in Figure 5 aligns with prior work suggesting that it takes the average undergraduate 5.1 years to complete their bachelor's degree (Shapiro et al., 2016). Given the time-varying nature of program-level CCB adoption, our GDDD approach relaxes many of the concerns that might be present when looking at a single-year policy shock. However, to test the robustness of our results, we create a within-year program-level placebo assignment. For every given institution-by-year program-level adoption, we randomly assign another two-digit CIP code to receive the treatment and then test the effects. We find no statistically significant effects on degree production using the 4- or 6-year lags (Table 7). Additionally, we randomly assign implementation of a CCB degree program to another community college within the adopted year and find no statistically significant impact on our bachelor's degree production outcomes. Without a program-by-institution-by-year fixed effect, we feel confident that our estimates project the actual impacts of CCB implementation rather than spurious or external factors. In addition to testing the placebo policy effects, we test our assumptions around the selected functional form. Our decision to log transform our outcome variables was made for ease of interpretation; however, we run alternative model specifications with total degree and total degree for every 100 students enrolled and find that our results are consistent with our selected specification in both magnitude and significance (see Appendix A). The results located in Appendix A illustrate that our findings do not appear to be dependent on functional form and, combined with other robustness checks, can be attributed to the implementation of CCB degree programs. Finally, we provide additional specifications of our models to address potential concerns regarding our isolation of targeted variations in county-by-program-by-year effects. In Appendix D, we estimate Equations (2) and (3) to allow for the inclusion of additional variations. To this end, we find generally consistent effects of CCB adoption in both direction and magnitude for our three primary outcomes. We contend that despite isolating targeted variation in our fully specified model, our results are robust to the inclusion of additional variations. This study combines program- and institution-level data to examine the impact of CCB adoption on bachelor's degree program enrollment and bachelor's degree production at nearby 4-year colleges and universities. Our results provide the first known estimates of the effect of CCB adoption on 4-year institutions' bachelor's degree program enrollment and bachelor's degree production, revealing that CCB adoption had a negative effect on bachelor's degree program enrollment and bachelor's degree production at for-profit 4-year institutions. Although critics have argued that the adoption of CCB degree programs would duplicate efforts and diminish the market share of 4-year colleges and universities, our findings suggest that the enrollment and bachelor's degree production of similar programs at for-profit institutions, and not other types of 4-year institutions, may be harmed by community college entrance into the bachelor's degree-granting market. Similar to community colleges, for-profit institutions respond to local labor market demand (Gilpin et al., 2015) and enroll a disproportionately high share of students who are low income, minority, over the age of 25 years, and parents (Deming et al., 2012). However, for-profit institutions charge significantly higher tuition than public community colleges and typically provide worse outcomes for students when compared with other institution types (Deming et al., 2013; Lang & Weinstein, 2012). Previous work has suggested that enrolling at a low-cost community college would be a better overall choice for prospective students rather than enrolling at a for-profit institution (Cellini & Chaudhary, 2014). In this study, we show that CCB degree programs appear to draw students specifically from for-profit 4-year institutions. More specifically, the average program-level enrollment in CCB degree programs is approximately 642 students, and roughly one-third of those students are drawn from for-profit 4-year institutions. Additional research can explore whether price-conscious students who enroll in a CCB degree program also experience improved academic and labor market outcomes when compared with for-profit students in similar degree programs. Given that a disproportionate number of Black and Hispanic students attend community college (American Association of Community Colleges, 2017), we also examine the heterogeneous effects of CCB adoption on bachelor's degree production according to students' race/ethnicity, finding that the presence of a nearby CCB degree program has a negative effect on bachelor's degree production at for-profit 4-year institutions among Hispanic students. This particular heterogeneous finding suggests that Hispanic students, many of whom are place-bound and low-income students (Shields 2004), may be able to benefit from CCB adoption by earning their bachelor's degree without paying the high tuition prices at for-profit colleges and universities. From a policy perspective, our results indicate that CCB adoption has achieved its aim in increasing state-subsidized bachelor's degree production without harming traditional public 4-year institutions in the state of Florida. Although institutional theory, particularly institutional isomorphism, can explain why community colleges would adopt new bachelor's degree programs rather than maximize the efficiency of their sub-baccalaureate offerings (e.g., Morphew & Huisman, 2002), legislative concerns related to the negative impact of CCB adoption on public 4-year institutions' bachelor's degree production appear to be unfounded in light of the results described above. In spite of these well-documented concerns, this study shows that the presence of a CCB degree program only cuts into the market share of for-profit colleges and universities. Because both for-profit and CCB-adopting institutions enroll a disproportionate number of students facing time and location constraints, the presence of a local CCB degree program would allow these traditionally disadvantaged students to earn their bachelor's degree in a high-demand field at a significantly lower price. This study leverages a unique data set to investigate the effects of CCB adoption in Florida, but future research can explore whether these findings hold on a national level. If findings from this study prove to be generalizable based on future work examining the impact of CCB adoption across states, community colleges across the United States may consider adopting targeted, high-demand bachelor's degree programs as a mechanism to increase overall bachelor's degree production. Despite the outcomes associated with CCB adoption presented in this study, critics may continue to suggest that the implementation of bachelor's degree programs at community colleges represents a form of mission drift that detracts from the purpose of community colleges; however, we urge those detractors to consider the evidence presented in this work and the multiple curricular missions of community colleges (e.g., responsiveness to local workforce demands) before disregarding the potential utility of CCB adoption within targeted, high-demand program areas. Justin C. Ortagus https://orcid.org/0000-0001-9415-2571
10.3102_0002831220948460	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220948460	School Effects Revisited: The Size, Stability, and Persistence of Middle Schools' Effects on Academic Outcomes	 Since the early 2000s, educational evaluation research has primarily centered on teachers', rather than schools', contributions to students' academic outcomes due to concerns that estimates of the latter were smaller, less stable, and more prone to measurement error. We argue that this disparity should be reduced. Using administrative data from three cohorts of Massachusetts public school students (N = 123,261) and two-level models, we estimate middle schools' value-added effects on eighth-grade and 10th-grade math scores and, importantly, a non–test score outcome: 4-year college enrollment. Comparing our results to teacher-centered studies, we find that school effects (encompassing both teaching- and nonteaching-related factors) are initially smaller but nearly as stable and perhaps more persistent than are individual teacher effects. Our study motivates future research estimating the long-term effects of both teachers and schools on a wide range of outcomes.	 Education researchers and policymakers have long debated how to measure school and teacher effects on students' academic outcomes (Everson, 2017). With the 2002 passage of No Child Left Behind (NCLB), standardized test scores became increasingly available and central to such evaluations. The landmark legislation rendered score-based school and teacher assessments more than an academic exercise: Educators' compensation and job security, as well as school closure decisions and state takeovers of schools, now hinge on the results (Baker et al., 2010; Everson, 2017; Goldhaber & Hansen, 2008; Hanushek & Rivkin, 2012; Isenberg & Hock, 2012). As score-based evaluations have become more consequential, the methods used to translate student scores into teacher and school effectiveness measures have become increasingly sophisticated and scrutinized. Student growth and value-added models enable researchers to evaluate not only student performance levels but also the ability of schools and teachers to improve students' performance in a given school year. A spate of recent studies employing experimental and nonexperimental data confirmed that value-added models revealed largely unbiased and sizable effects of teachers (Chetty et al., 2014a) and schools (Angrist et al., 2017; Deming, 2014) on students' end-of-year test scores. However, educational evaluation research and accountability frameworks have primarily centered on teachers', rather than schools', value-added effects on academic outcomes. Ever since the Coleman report, education scholarship has typically characterized school-level differences in academic performance as primarily reflective of student sorting rather than school-induced effects. Within-school differences (e.g., in teacher quality) appeared to play a larger role in stratifying students' academic outcomes. Subsequent empirical studies, leveraging increasingly sophisticated quantitative models, seemed to bear this out. A considerably greater proportion of the variance in end-of-year test scores resided within schools, rather than between, them—even after adjusting for students' sociodemographic characteristics (Konstantopoulos, 2006, 2007a). Concerns about the trivial size and questionable stability of schools' value-added effects from year-to-year (Kane & Staiger, 2002; Staiger et al., 2002) fueled the perception that school effects were too small and too imprecisely estimated to be of practical value. We argue that once value-added effects are reframed from short-term existence to long-term persistence and longitudinal student-level data are deployed, this perception may change. Schools offer a treatment different in length and in kind than do individual teachers. Although teachers may have more leverage than schools in shaping children's cognitive development over the course of a single year, schools provide an opportunity to intervene in a child's cognitive and socioemotional development in a more sustained way, across multiple years. Of course, schools may exert effects on students' outcomes through their differences in teaching-related factors, such as (1) the quality of individual teachers and (2) the quality of the combined teaching workforce. The latter could enhance (or depress) individual teachers' effectiveness through collective efforts like curriculum alignment and professional development. However, other school-level mechanisms beyond pedagogy are plausibly implicated, as well. Schools encompass the combined effects of: administrators and other school-wide personnel (e.g., principals, counselors, psychologists, and social workers); institutional resources (e.g., computers, libraries, physical infrastructure); neighborhood context (e.g., crime levels, environmental conditions, and toxicity); peer factors (e.g., sociodemographics, grade span, student culture and behavior); policies and programs regarding attendance, student behavior, and discipline; academic resources (e.g., curricula, books, and materials); extracurricular offerings; teacher and classroom assignment protocols; parent engagement strategies; and school community history and reputation. In this study, we conceive of school effects broadly—as encompassing the aforementioned teaching and nonteaching-related factors—and use two-level hierarchical linear models (HLMs) to estimate their persistence on students' test score and non–test score outcomes over a 4-year period. We focus specifically on middle school effects for theoretical and methodological reasons. Theoretically, middle schools enroll students at a key transition period in their K–12 schooling careers, when a host of factors with long-term implications, ranging from curricular choices to self-esteem to intrinsic motivation, are revisited and reformed (Anderman & Maehr, 1994; Rathunde & Csikszentmihalyi, 2005; Wigfield et al., 1991). Methodologically, focusing on middle school allows us to leverage both pretreatment test scores and posttreatment outcomes—the former to control for initial student achievement, which prior studies have confirmed is required to reduce bias, and the latter to track student outcomes into high school and beyond. We use a rich longitudinal data set that follows three cohorts of Massachusetts public school students—123,261 in total—who entered seventh grade between 2004 and 2006 as they move from elementary school through secondary school and into college. In our main analyses, we estimate middle schools' value-added effects on math scores at the end of the "treatment" (i.e., eighth grade) and 2 years posttreatment (i.e., 10th grade) and estimate the stability of these value-added effects across three cohorts. For one cohort of students, we also examine middle schools' effects on the likelihood of 4-year college enrollment. We find that middle schools' effects on student achievement endure beyond the initial treatment years, lasting more than 4 years after students leave middle school. Comparing our estimates of school effects to existing estimates of individual teacher effects, school effects appear initially smaller but nearly as stable across years and potentially more persistent than are the latter when measured over similar timeframes. Moreover, our estimates suggest middle schools may have a stronger influence on desirable non–test score outcomes, such as 4-year college enrollment, than do individual teachers. Importantly, sizable middle school effects on 10th-grade math scores and college enrollment remain, even after accounting for high school and district sorting patterns. Our findings, based on two-level models, motivate future research that employs longitudinal data sets and three-level models to disentangle what portion of schools' persistent effects are driven by teaching and nonteaching-related factors. If such analyses confirm sizable, stable, and durable school effects—even when accounting for school-level differences in individual teacher quality—then educational evaluation systems should reduce the disparity in emphasis on teacher versus school effects, examining the size and precise sources of both teachers' and schools' long-term impacts on test score and non–test score outcomes. Evaluating schools' and teachers' effects on students' standardized test scores has been a central occupation of policymakers and education researchers since the late 1990s. In 2002, NCLB codified this orientation into federal law. The far-reaching legislation focused exclusively on schools' levels of achievement, requiring every state to assess its schools by setting proficiency standards in reading and math. Critics noted that requiring all students in a given state to reach the same absolute level of proficiency imposed a heavier burden on schools serving predominantly low-income, minority students than on schools with more advantaged pupils (Ballou et al., 2004). Value-added models address this concern by comparing the gains of students in different schools and classrooms who not only have the same initial test scores but also come from families with similar observable sociodemographic characteristics. Absent random assignment, such models cannot control for all the compositional differences across schools and classrooms. Many scholars have mused that differential sorting of students across schools and classrooms on the basis of unobserved factors correlated with future achievement may bias value-added estimates of school and teacher effects (Hanushek & Rivkin, 2012; Reardon & Raudenbush, 2009; Rothstein, 2010). Concerns of sorting-driven bias have likely contributed to education researchers' disproportionate focus on teacher, rather than school, value-added effects. Some scholars perceive sorting across neighborhoods and schools to be a more strongly selected process, or at least a less statistically tractable one, than is sorting between teachers among students within the same school. Even if between-teacher selection (e.g., via tracking) threatens the validity of teacher effectiveness measures, statistical controls, such as student baseline test scores and student fixed effects, may more easily adjust for it than for between-school or between-neighborhood sorting (see Hanushek & Rivkin, 2012; Jencks & Mayer, 1990). Thus, value-added school effects may be more prone to uncorrected bias than are value-added teacher effects. Another key factor driving the disparity in research on teacher and school effects is the empirical finding that twice the proportion of the variance in students' end-of-year value-added gains is attributable to the former versus the latter (Kane & Staiger, 2008; Konstantopoulos, 2006; Nye et al., 2004). The large body of work on teachers' short-term effects, which spans a wide range of grade levels and geographies, produced fairly consistent estimates of the variance in teachers' contributions to end-of-year test scores, with an average SD of 0.13 for reading and 0.17 for math when comparing teachers within the same schools (Hanushek & Rivkin, 2012). For reference, this contribution translates to an increase of approximately 6 percentiles in the test score distribution (Raudenbush, 2014), and the Black-White achievement gap is estimated to be about 0.7 to 1 SD (Hanushek & Rivkin, 2012). However, other scholars have estimated smaller teacher value-added effects. For example, Chetty, Friedman, and Rockoff (2016) exploited randomization to estimate that a 1 SD increase in teacher value-added predicts a 0.10 SD increase in student test scores. School-oriented studies, which typically conceive of school effects as encompassing both teaching- and nonteaching-related factors, tend to find short-term school effects that are equal to, or slightly smaller than, individual teacher effect estimates. For example, Jennings et al.'s (2015) analysis of high school effects suggested that a 1 SD increase in school quality generates a 3.5 to 3.7 percentile (~0.14 SDs) value-added boost to end-of-year 10th-grade test scores, averaged across math and reading. Deutsch (2013) also estimated a 0.14 SD difference in fourth to eighth graders' value-added math and reading test scores between schools, while Deming (2014) generated a smaller school value-added effect, of under 0.10 SD on third to eighth graders' scores, averaged across math and reading. But it is not only the disparity in the size of teacher and school effects that matters. Concerns about the stability of school effect estimates may play a role, as well. In the early 2000s, Kane and Staiger (2001, 2002) and Staiger et al. (2002) argued that schools' effects on end-of-year test scores may vary widely from year to year and therefore may not exert a clear signal about school quality or provide useful information for rewarding or penalizing schools based on only a single year of data. Although they did not directly compare the stability of school and teacher effects, their findings may have sown seeds of doubt regarding the former's utility, contributing to the literature's path curving away from school effects. Since then, as test scores have become the taken-for-granted metric of student achievement, and as value-added models have become more widespread and sophisticated, few researchers have revisited the question of whether school effects' size and stability are sufficiently strong to justify their inclusion in accountability regimes. The empirical studies described above suggest the following two concrete hypotheses: Hypothesis 1: Schools' short-term (i.e., end-of-year) effects on test scores are smaller than are teacher effects. Hypothesis 2: Schools' short-term effects on test scores are less stable and reliable than are teacher effects. However, small and perhaps unstable short-term effects do not necessarily translate into small long-term effects. Full persistence in either school or teacher effects' size over time is likely an untenable assumption (Briggs & Weeks, 2011; Lockwood et al., 2007; McCaffrey et al., 2004). Yet the rates of decay may vary between schools and teachers. This possibility has not received sufficient examination because researchers and policymakers have primarily deployed value-added models to track teacher or school effects on test scores within a single school year rather than over a longer time period. When they do employ a longer time horizon, they tend to track the persistence of individual teachers', not schools', effects on students' test scores. Studies examining teacher effect persistence showed that the benefit (or cost) induced by a given teacher on end-of-year test scores tends to decay dramatically within a short period of time. Given two teachers who diverge by 1 SD in value-added based on end-of-year scores (~0.10–0.17 SD), 1 year later only 20% to 50% of the difference is still detectable (0.02–0.09 SD; Raudenbush, 2014). Two years later, the difference is estimated to be 15% to 40% (0.02–0.07 SD; Lockwood et al., 2007; McCaffrey et al., 2004), and 3 or more years later, this difference reduces to 10% to 20% (0.01–0.03 SD; Chetty et al., 2014b; Lockwood et al., 2007). Compared to the growing literature on the persistence of individual teacher effects on test scores, which encompasses over 10 studies (Raudenbush, 2014), the literature on the persistence of school effects remains sparse. Deming's (2014) analysis of school lottery effects on fourth to eighth graders included an appendix revealing that the value-added estimate of the school a student attended in 2003 significantly predicts the average of their 2004 math and reading scores, net of the value-added estimate of the school attended in 2004, baseline test scores, and demographics. However, the 1-year persistence rate is not directly reported. Briggs and Weeks (2011) focused squarely on school effect persistence among fourth to eighth graders and found that school value-added effects on reading scores persist at a rate of 10% a year later, while school effects on math scores persist at a rate of 48%—a difference reflecting, perhaps, the fact that the learning of the latter subject is more malleable than the former. These results suggest that the 1-year persistence rate of school effects on math scores is at the high end of the range of individual teacher effects' persistence rate, while for reading, school effects' persistence rate is on the low end. School effects plausibly persist at higher rates than do individual teacher effects for several possible reasons. First, schools typically constitute a more robust treatment than do individual teachers both within years (especially in middle and high schools, when students are taught by multiple instructors) and across years (exposure to specific schools typically lasts several years, whereas exposure to specific teachers more often lasts only 1 year). Second, whereas individual teachers appear to exert stronger effects on short-term cognitive skill growth, schools' nonteaching-related factors may exert stronger effects on socioemotional skill growth (e.g., via exposure to peer networks, counselors, social workers, extracurricular activities). To the extent socioemotional skills are more malleable over the long term, especially after early childhood, and predict subsequent degrees of cognitive skill growth and educational attainment (Heckman & Mosso, 2014), we might expect schools to exert more persistent effects on test scores than do individual teachers. Generating a complete picture of school and individual teacher effects' persistence requires attending not only to the durability of their test score effects but also to their effects on long-term, non–test score outcomes. Recent research suggests that conclusions regarding the persistence of teacher and school effects may meaningfully vary based on the particular outcome in question. For example, Chetty et al. (2011) estimated that lingering kindergarten classroom effects on student test scores 7 years posttreatment are small (SD = 1.5 percentiles), representing a decay of over 75%, from 6.3 percentiles. However, these effects can generate significant long-term effects on non–test score outcomes in early adulthood, such as teenage birth rates, college attendance, and earnings, perhaps due to larger, more durable effects on socioemotional skills (i.e., levels of effort, initiative, engagement in class, and whether the student reports valuing school). Concretely, a 1 percentile increase in estimated kindergarten class value-added is associated with a 0.1 percentage point increase in the likelihood of attending college and a $483 increase in annual wages. Another study by Chetty et al. (2014b) suggested that a 1 SD increase in estimated teacher value-added during Grades 4, 5, 6, 7, or 8 is associated with a 0.5 to 1 percentage point increase in the likelihood of attending college. This same temporal dynamic applying to individual teacher effects may also apply to school effects. For example, Jennings et al. (2015) found that between-school differences explain more of the variance in college attendance than in 10th-grade test scores. Several other studies confirmed that educational interventions' effects on test scores fade out precipitously within a few years but substantial effects emerge when longer-term outcomes are evaluated (Deming, 2009; Heckman et al., 2013). The intuition that school and teacher interventions on test scores are less durable than their interventions on socioemotional skills and non–test score outcomes is deepened by Jennings et al.'s (2015) argument that, relative to test scores, which are highly correlated across school years, nonscore outcomes (e.g., college enrollment) depend far more on students' conscious choices made within school or classroom contexts. School-level factors, such as counselors, school culture and expectations, and peer effects, at least at the high school level, may exert stronger effects on these choices than do individual teachers. Lacking robust evidence on the relative persistence of teacher versus school effects, our theoretical account above yields the following hypotheses: Hypothesis 3: School effects on math scores persist at a higher rate than do individual teacher effects. Hypothesis 4: School effects on college enrollment are stronger than are individual teacher effects. Can schools that students attend prior to ninth grade also shape non–test score outcomes in high school and beyond? While high school effects on postsecondary outcomes may be discernible because the choices students make in their last year of high school exert considerable influence on their outcomes in the immediate years afterward, elementary and middle school effects on these outcomes may decay by the time students reach high school. Yet recent studies on skill development have suggested that interventions tend to be more effective when they are implemented earlier in a student's childhood (Cunha & Heckman, 2007; Heckman, 2006; Heckman & Mosso, 2014). Middle school may be a critical intervention point, given that it is still relatively early in a child's development, and it is a key transition period during which young adolescents face a range of formidable challenges and decisions (e.g., curricular choices) that may affect their long-term trajectories (Anderman & Maehr, 1994; Rathunde & Csikszentmihalyi, 2005; Simmons & Blyth, 1987; Wigfield et al., 1991). Thus, middle school quality may meaningfully stratify students' postsecondary outcomes. We believe estimating school effects on longer-term test score and non-test score outcomes is a valuable exercise at a time when theories of children's skill development are evolving to encompass not only cognitive but also socioemotional skills (Heckman & Mosso, 2014), accountability regimes centered on test scores and teachers are receiving intense scrutiny (Koretz, 2017), and longitudinal administrative data encompassing test score and non-test score outcomes are increasingly available. Our rich dataset follows three cohorts of Massachusetts public school students as they move from elementary school through secondary school and into college. Measuring student outcomes within the K-12 pipeline both before and after middle school provides unusual analytical leverage, enabling us to construct two-level models estimating the size of middle school effects, net of baseline student abilities and high school and district effects, and these effects' persistence over time. Yet we would be remiss if we did not acknowledge two important concerns posed by our approach. First, teacher and school effects on non–test score outcomes may be artificially inflated compared to their effects on test scores because models of the latter employ lagged measures of the outcome, purging effect estimates of bias. Models of the former cannot. Moreover, holding teachers and schools accountable for longer term outcomes that may be less accurately estimated and tracked with a substantial time lag poses serious concerns, especially in an era of high educational personnel turnover (García & Weiss, 2019) and a fraught political environment. Second, as noted at the outset, school effects on test score and non–test score outcomes could theoretically be explained by school-level differences in teaching-related factors (i.e., the quality of the individual teachers and/or the quality of the combined teaching workforce). If true, then school effects would be a compositional artifact of individual and collective teaching quality rather than a distinct driver of student outcomes. Indeed, a body of research articulates the potential importance of school-level differences in teaching-related factors, such as curricula, instructional resources, and professional development for student achievement. Creemers and Reezigt (1996), for example, emphasized school-level "rules and agreements" about how classroom instruction is executed (e.g., curricular materials, grouping procedures, teacher behavior) and evaluation and intervention policies that entail testing, remediation, and counseling, as well as school-level professional development policies. However, they bemoaned the complexity of empirically measuring and disentangling the aforementioned school-level factors from each other and from teacher-level factors. The relatively few studies attempting to isolate school-level instructional factors' effects on student outcomes suggested that they exert modest effects on student outcomes (see Berends et al., 2010; Goddard et al., 2015; Kyriakides et al., 2010; Lubienski et al., 2008; Von Secker & Lissitz, 1999). Another line of research informs whether school effects merely reflect individual teacher effects by estimating the proportion of the variance in individual teacher quality that resides between versus within schools. Using administrative data from San Diego, Koedel and Betts (2007) found that despite the conventional wisdom, "essentially all" the variance in teacher value-added quality exists within schools, not between them, and implicated schools' difficulty in identifying and hiring the highest value-added teachers as a potential explanation. A review of multiple studies suggested that 80% to 90% of the variance in teacher value-added quality resides within, rather than between, schools (Xu & Swanlund, 2013; see also Chetty et al., 2014a). Thus, school effects are unlikely to be explained by between-school differences in individual teacher quality or the collective teaching workforce. We return to this issue and the aforementioned endogeneity concern below. Our analyses rely on two data sources: administrative school records from the Massachusetts Department of Elementary and Secondary Education and college enrollment data from the National Student Clearinghouse (NSC). The former source allows us to identify all students enrolled in a Massachusetts public school, including charters, between 2002 and 2010 and track them for as long as they remained in any Massachusetts public school. We can follow students who moved from one public school to another (including charters). However, we cannot follow students who dropped out of school, moved to private schools, or moved to another state. During the years covered by our data, the Massachusetts Comprehensive Assessment System (MCAS) administered a math test to students enrolled in Grades 4, 6, 8, or 10 and an English language arts (ELA) test to students enrolled in Grades 4, 7, 8, or 10 near the end of the school year. For the cohort of students who entered seventh grade during the 2004–2005 school year (subsequently referred to as the 2004 cohort), we link K–12 records to NSC college enrollment data. Massachusetts schools' grade configurations vary across local school districts (see Supplemental Appendix in the online version of the journal). For our analytic purposes, "middle school" designates a school that includes Grades 7 and 8, regardless of what other grades it includes. Just over 193,000 students entered the seventh grade of a Massachusetts public school for the first time during the fall of 2004, 2005, and 2006. We impose the following sample restrictions to generate stable estimates of middle schools' value-added:1. Considering seventh and eighth grades as our "treatment" period, we exclude students who did not remain in the same school throughout both grades.2. Because estimating schools' value-added effects requires an estimate of students' initial skills, we restrict our sample to Massachusetts public school students who took the math and ELA tests near the end of fourth grade and the math test near the end of sixth grade.,3. To gauge the persistence of middle school value-added (MSVA) effects using a measure of students' skills after they completed eighth grade, we restrict our sample to Massachusetts public school students who took the math tests near the end of 10th grade.4. Given that the MCAS tests change somewhat from year to year, we limit our sample to students who took the fourth-, sixth-, eighth-, and 10th-grade tests "on time." This restriction eliminates students who were held back or skipped grades, but it ensures that every remaining cohort member had spent the same number of years in school between any two tests.5. To ensure that our estimate of each school's impact on seventh and eighth graders is moderately reliable, we dropped all schools with fewer than 10 students in any of our three entering cohorts. These restrictions produce an analytic sample of 123,261 students, pooled across three cohorts, and enrolled in 355 middle schools. With an average of nearly 350 students per school, our sample produces precise estimates of schools' mean value-added. For every student in the analytic sample, the data set includes our core test score outcomes of interest—eighth- and 10th-grade MCAS math percentiles—as well as fourth- and sixth-grade MCAS math scores and fourth-grade MCAS ELA scores, enabling us to construct growth models using two lagged measures of the outcome and an ELA measure in fourth grade. Given concerns that advantaged and disadvantaged students tend to accumulate knowledge at different rates, we supplement these baseline score measures with a rich set of student-level sociodemographic controls measured in Grade 4, including: gender, race/ethnicity, age, country of birth, and eligibility for a free or reduced-price lunch (FRPL; an indicator of whether parents' income falls below 185% of the poverty line). Then, to address the concern that peer effects bias value-added estimates (e.g., Angrist & Lang, 2004; Barr & Dreeben, 1983; Kane & Staiger, 2008), we also control for school-level averages of each of the aforementioned sociodemographic variables (pooling all three cohorts together) for the middle school each student attended. The final variable we include is a non–test score outcome, available for only the 2004 cohort: an indicator of whether the student enrolled in a 4-year college/university in 2010 (the year they would be expected to graduate from high school if not retained in any grade). A score of 1 indicates they enrolled, and 0 indicates they did not. Table 1 presents descriptive statistics for the unrestricted sample of 193,259 students, for the 123,261 students in the analytic sample, and for the 38,620 students in the 2004 cohort for whom we also have NSC college enrollment data as well as a valid high school value-added (HSVA) estimate (we describe how these estimates are calculated in the Results section). Math and ELA scores are higher in the two subsamples with complete data than in the full sample, and the percentages of African American, Hispanic, and FRPL students are lower in the former than the latter. These differences derive mainly from two factors. First, low-income and minority students in our sample repeat grades and change schools considerably more often than do higher income and White students. Second, low-income and minority students are less likely to have taken the 10th-grade tests because they are more likely to have withdrawn from school or stopped attending regularly before the end of 10th grade. To partially mitigate these selection concerns, we reran all analyses with the 10th grade on time restriction lifted. Doing so only increased our analytic sample slightly—from 123,261 to 126,452 (an increase of ~2.6%)—and did not substantively change any of our results. Given our sample restrictions, the analyses that follow must be interpreted as capturing middle schools' effects on the test scores of students who remain in the same middle school for multiple years and who progress "on time." They cannot speak to middle schools' role in increasing or decreasing the likelihood that students remain in the same school and progress through grades on schedule. While we recognize the important limitations of this approach, we see a fundamental trade-off between stability/reliability and generalizability when measuring school effects and have opted for the former in this study. Our aim is to estimate the size and persistence of "true" middle school effects, even if on a relatively advantaged student population. Education researchers frequently employ HLMs to evaluate the effects of a school or teacher/classroom within a given year (e.g, Adcock & Phillips, 1997; Raudenbush & Bryk, 2002; Reardon & Raudenbush, 2009). These models partition the variance in academic achievement into between- and within-school and/or teacher/classroom components, while accounting for the clustering of certain types of students within schools/classrooms. The between-school/teacher variance component calculated by these models is commonly used to estimate the proportion of the variance in a given academic outcome that is not attributable to compositional (e.g., demographic, prior academic ability) differences among students. Our data permit us to follow suit by employing two-level HLMs estimating the proportions of the variance in eighth-grade MCAS math scores that reside between, rather than within, middle schools. We also gauge the extent to which these between-school differences in achievement are attributable to between-school differences in students' prior test scores ("Growth" models), student-level demographic characteristics ("Value-Added" models), and school-level demographic characteristics ("VA + Peer Effects" models). We then use the latter model to calculate MSVA estimates representing how much higher/lower each school's students performed, on average, compared to students with similar baseline test scores and demographic backgrounds, who attended demographically similar schools. Concretely, we begin by partitioning the total variance in eighth-grade math scores to between– versus within–middle school components via an unconditional random intercept model:[MATH](1) where the outcome is the eighth-grade math score (grand mean centered and standardized to have a mean of 0 and an SD of 1) of student i (Level 1), nested within middle school j (Level 2). γ00 represents the fixed component of the middle school–level intercept (the estimated math score for the average middle school, pooled across three cohorts), r0j is the random error component of the middle school–level intercept (the deviation of the particular middle school's intercept from the mean middle school's intercept), and eij is the student-level error term (the difference between a student's eighth-grade math score and the average score among students within the same middle school). This model and all that follow assume the random component of the middle school-level intercept (r0j) and the individual-level residual (eij) are normally distributed with means of zero and variances of [MATH] and σ2, respectively. We first assess whether the middle school-level intercept's random component ([MATH]) has a variance that is significantly different from 0. If the unconditional random intercept model suggests it does, then we confirm that test scores indeed vary across middle schools and random noise in the data does not fully account for these differences. We then examine the precise proportion of the total variance in eighth-grade math scores that is accounted for by [MATH] (i.e., the middle school–level variance) versus σ2 (i.e., the student-level variance) by dividing the middle school–level variance component by the total variance, which produces an intraclass correlation (ICC). The ICC ranges from 0 (where Level 1/student variance accounts for the entirety of the total variance) to 1 (where Level 2/middle school variance accounts for the entirety of the total variance). The higher the ICC, the higher the proportion of the total variance accounted for by middle school–level variance. We then determine the extent to which estimated middle school differences in eighth-grade math scores reflect compositional factors: (1) students' prior test scores, (2) student-level demographic characteristics, and (3) school-level demographic characteristics. Equation 2 represents our most complete model of eighth-grade math scores by returning to the unconditional random intercept model as a base and adding in the full slate of student- and school-level covariates available (all grand mean centered and standardized to have a mean of 0 and an SD of 1), producing a random intercept and fixed slope model:[MATH][MATH](2) β1,β2, and β3 represent the fixed slopes quantifying the relationships between the eighth-grade math score of student i within school j and their sixth-grade math score, fourth-grade math score, and fourth-grade ELA score, respectively. β4,β5,β6,β7, β8, and β9 represent the fixed slopes quantifying the relationships between the eighth-grade math score of student i within school j and their age, racial/ethnic, class, and nativity characteristics. γ01,γ02,γ03, γ04, γ05, and γ06 represent the relationships between a given student's eighth-grade math score and school-level demographic characteristics whose values vary across schools but not among students within schools. If, after accounting for these student-level score differences and student- and school-level demographic differences, variance in the school-level intercept still significantly exceeds 0, then we have strong evidence that true middle school effects exist and that they are not merely the function of student- or school-level demographic differences. The next step is to quantify these residual (i.e., noncompositional) middle school effects on students' eighth-grade test scores by calculating a value-added estimate for each of the 355 middle schools (which we subsequently refer to as MSVA). MSVA represents how much higher or lower that school's students perform, on average, compared to how students with similar baseline scores and demographic characteristics, who attend demographically similar schools, would be expected to perform. To generate the estimates we return to Equation 2, the most complete model of students' eighth-grade test scores, and calculate the best linear unbiased predictors (BLUPs; Raudenbush & Bryk, 2002) of the middle school random effects (r0j). It is important to note that the variance of the BLUP estimates is negatively biased by a factor equal to the reliability of the school value-added estimates. However, this bias appears negligible when groups (i.e., schools) exceed 40 students (von Hippel & Bellows, 2018), which the vast majority of our schools do. The random effects (BLUP estimates) are employed as school value-added estimates and assumed to be normally distributed, with a mean of 0 and a variance of [MATH]. MSVA can thus be interpreted as the predicted effect of attending a given middle school on a student's eighth-grade math score, net of student baseline math and ELA skills and both student- and school-level sociodemographics. The variance of MSVA ([MATH]) gauges the distribution of MSVA. Our core MSVA estimates are based on pooling data across all three cohorts of middle school students; Kane and Staiger (2002) argued that pooling data across years is critical to generating stable and reliable estimates of school quality, especially when school sizes are small. However, we also create cohort-specific MSVA estimates and correlate the cohort-specific estimates with each other to gauge how stable the estimates are from one year to the next. We then shift to evaluating whether middle school effects not only exist but persist over time. To this end, we follow prior analyses, such as Konstantopoulos (2007b), by constructing multivariate regressions predicting student test scores at least 1 year posttreatment, as a function of school-level value-added estimates (in our case, MSVA)—the key predictor of interest—as well as pretreatment test scores and student sociodemographics. After running an unconditional random intercept model to partition the variance in 10th-grade math scores into between- and within-middle school components, we begin with Equation 2 and make three adjustments: changing the outcome of interest from the original, short-term effect (i.e., eighth-grade math score) to the longer term outcome (i.e., 10th-grade math score); adding in the MSVA covariate; and removing the sociodemographic characteristics of the middle school attended by the student. Note that we do not include eighth-grade math score as a predictor because one of the key channels through which middle schools are likely to matter is via eighth-grade math scores and we do not want to "control away" this effect.[MATH][MATH](3) The key parameter of interest in Equation 3 is γ01. Its significance indicates whether or not middle school quality effects (measured via test scores) on 10th-grade math scores are discernible. Because MSVA is grand mean centered and standardized, with a mean of 0 and an SD of 1, γ01 can be interpreted as the effect of a 1 SD increase in test score–based middle school quality on 10th-grade scores. By dividing the γ01 coefficient by the original MSVA estimate generated via Equation 2, we evaluate the percentage of the middle school effect that is still discernible 2 years later (i.e., the persistence rate). An important consideration is that middle school effects' persistence may reflect, in part, the disproportionate propensity of students attending higher value-added middle schools to subsequently attend higher value-added high schools through a sorting process driven by (1) achievement gains induced by the middle school (e.g., in the case of selective high schools) and/or (2) an unobservable set of individual- and/or household-level factors predicting both middle school and high school sorting. We perceive channel (1) to be a valid middle school–induced mechanism shaping longer term outcomes, so we first report MSVA effects on 10th-grade math scores without controlling for any information related to high school of enrollment. However, to address the concern that channel (2) is confounding middle school effects on 10th-grade math scores, we calculate a set of HSVA estimates based on 10th-grade math scores and add them into our model. By comparing MSVA effects with and without an HSVA control, we generate a range of plausible MSVA effect sizes on 10th-grade scores. To calculate HSVA, we use the same framework as Equation 1 but define the timeframe between the eighth- and 10th-grade tests (roughly ninth and 10th grades) as the treatment period, adding eighth-grade math scores to the model as an additional control. Students are assigned to the high school they entered in ninth grade, without regard to whether they remained in the high school until they took the 10th-grade test. As with middle schools, we limit the sample to high schools with 10 or more eligible students in each cohort, yielding a subsample of 117,932 students enrolled in 314 high schools. (Model output underlying our HSVA estimates is available upon request.) After generating HSVA estimates, we assign these school-level high school estimates and the school-level middle school estimates we generated previously to individual students and then rerun Equation 3, with one additional control: HSVA. The significance of MSVA and magnitude of its coefficient, net of HSVA, provides stronger evidence regarding the extent to which middle schools exert direct effects on 10th-grade math scores, beyond sorting processes. Finally, we replicate the exact same analysis (i.e., Equation 3 with and without HSVA) for the 2004 student cohort, replacing the 10th-grade math score outcome with a longer term, non–test score outcome: whether the student enrolled in a 4-year college by 2010 (i.e., the year they would have been expected to graduate from high school if not retained in any grade). Given that this final outcome is binary, rather than continuous, a logistic regression model would typically be recommended. However, in order to preserve consistency across the study's analytic models and to generate more easily interpretable results, we construct a linear probability model, using HLM for our core analyses of school effects on 4-year college enrollment. As a robustness check, we replicate these analyses using logit models with standard errors clustered by middle school and generate results that are substantively unchanged. As mentioned above, estimating MSVA effects on the 4-year college enrollment outcome is likely more prone to omitted variable bias in our models than is estimating MSVA effects on test scores because the former outcome, unlike the latter, likely reflects difficult-to-observe factors (e.g., parental wealth, educational aspirations) that cannot be controlled away using a lagged test score measure. Although data limitations preclude us from directly accounting for these broader sets of factors, we rerun our college enrollment analyses using district fixed effects, under the assumption that the distribution of parental wealth and educational aspirations is likely to be far larger between school districts than within them (Owens, 2016; Owens et al., 2016). To what extent do students' eighth-grade test scores diverge between, as opposed to within, middle schools? The unconditional random intercept model produces a middle school–level intercept of −0.050—the average of all Massachusetts middle schools' grand mean centered eighth-grade math test scores—with a variance of 0.168, which is significantly different from 0 (likelihood ratio = 18957.21, p < .001) and translates to an SD of 0.410. By dividing the random intercept's variance by the total variance in the outcome (the ICC), we estimate that 17% of the total variance in eighth graders' math scores resides between, rather than within, the middle schools they attend. Jennings et al. (2015), Konstantopoulos (2006), and Nye et al. (2004) similarly found that approximately 20% of the variance in reading and math test scores resides between, rather than within, schools. We can infer the vast majority of test score variance resides not between schools but between students and classrooms within the same schools. A large portion of the estimated between-school variance in test score levels may merely reflect differences in the composition of students served. To test this possibility, we first estimate growth models of eighth-grade math scores that incorporate prior test scores from fourth and sixth grades and then estimate value-added models (Table 2, Model 1). In this growth model, both the Level 1 and Level 2 variance components decline dramatically relative to the unconditional model, from 0.848 to 0.247 and from 0.168 to 0.029 (SD = 0.171), respectively, though the random intercept's variance remains significantly different from 0 (likelihood ratio = 9488.33, p < .001). Over four-fifths of the estimated variance in middle schools' eighth-grade math scores appears to reflect differences in the baseline (i.e., "pretreatment") achievement levels of their student bodies. This finding echoes the large literature arguing that school-level evaluation procedures must adjust for differences in baseline aptitude in order to generate informative score-based measures of school effectiveness. It is also worth noting that not only do sixth-grade math scores significantly predict eighth-grade math scores, as we would expect, but fourth-grade math and, to a lesser extent, fourth-grade ELA scores are also significant predictors. These secondary findings suggest that the inclusion of multiple pretreatment skill measures helps reduce bias in estimating school effects (see also Deming, 2014). Once these baseline score differences are accounted for, the remaining between–middle school variance suggests attending a middle school 1 SD more effective than average is associated with an approximately 0.17 SD gain in a student's eighth-grade math score relative to the score that would otherwise be expected. Interestingly, the subsequent value-added models that incorporate student- and school-level demographic characteristics do not meaningfully account for between-school differences in eighth-grade math scores, net of baseline test scores. Adding in student-level demographic characteristics when predicting eighth-grade math scores (Table 2, Model 2) reveals effects in the theoretically predicted directions: Black and low-income students score between 0.01 and 0.02 SD lower on eighth-grade math than do other students, net of baseline test scores and other demographics, while immigrants score about 0.02 SD higher. Taken together, student-level demographics reduce the student-level variance component from 0.247 to 0.245, while the school-level variance component remains virtually unchanged at 0.030. The addition of school-level demographic characteristics (Table 2, Model 3) generates some surprising results: proportions Black, Latino, and immigrant are significantly associated with higher eighth-grade math scores, net of all other covariates, while proportion FRPL is significantly and negatively associated with the outcome. The school-level factors only marginally diminish the random intercept to 0.027 (SD = 0.16), suggesting that exclusion of peer effects only slightly biases MSVA effects in this sample. With all variables included, the random intercept indicates that 10% of the remaining variance in eighth-grade math scores resides between rather than within middle schools. This is just above the analogous percentages calculated by Jennings et al. (2015) when estimating HSVA effects on 10th-grade scores of Massachusetts and Texas students (9% and 6%, respectively). The most complete model (Table 2, Model 3) also suggests that attending a middle school 1 SD more effective than average is associated with an approximately 0.16 SD boost in a student's eighth-grade math score compared to students with similar baseline scores and demographic characteristics, who attend demographically similar schools. This estimate is just slightly above the estimated 0.14 SD average reading and math score boost associated with a 1 SD boost to school value-added calculated by Jennings et al. (2015) with regard to 10th graders and Deutsch (2013) with regard to fourth to eighth graders. It is considerably above the 0.10 SD boost calculated by Deming (2014) with regard to fourth to eighth graders' math and reading score average. Although our data preclude us from directly comparing the analogous short-term estimate of teacher effects using the same sample, we can benchmark our school effect estimates with the analogous end-of-treatment teacher effect estimate of 0.10 to 0.17 SD on math scores from results of multiple studies, conducted across grade levels and geographies (Chetty, Hendren, & Katz, 2016; Hanushek & Rivkin, 2012). Our estimated end-of-treatment middle school effect size (0.16 SD) may initially seem comparable to the average teacher effect size. However, there is an important difference. The teacher "treatment" in most teacher effects studies is conceptualized as lasting 1 year, while the middle school "treatment" in our study lasts 2 years. Thus, it may not be appropriate to directly compare the end-of-treatment gains between a 1-year (i.e., teacher) and 2-year treatment (i.e., school). To generate a fairer comparison, we compare our end-of-treatment middle school effect estimate (0.16 SD) to the estimated effect of receiving 2 consecutive years of instruction from a teacher that ranks 1 SD higher than average in value-added. If teacher effects persisted at a rate of 100% 1 year posttreatment (an untenable assumption), then receiving 2 years of instruction from a teacher that is 1 SD higher than average quality would be associated with a 0.20 to 0.34 SD score gain at the end of the 2-year treatment (0.10–0.17 SD from Year 1 instruction + 0.10–0.17 SD from Year 2 instruction). If teacher effects persisted at a rate of 20% to 50%, as empirical evidence suggests, then 2 years of instruction from a teacher that is 1 SD higher than average quality would be associated with an approximately 0.14 to 0.26 SD score gain (0.04–0.09 SD from Year 1 instruction + 0.10–0.17 SD from Year 2 instruction). Integrating these calculations with our estimated 0.16 middle school effect size provides tentative support for Hypothesis 1: School effects on test scores are initially smaller than are teacher effects—especially when adjusted for intervention length. If school effects are both smaller in size and considerably less stable across years than are teacher effects, then the educational evaluation research is justified in focusing on the latter rather than the former. We have shown evidence that our short-term school effect estimates may be initially smaller than are widely cited estimates of teacher effects, but are they less stable? To gauge this possibility, we shift from calculating MSVA estimates by pooling data across all three cohorts—as Kane and Staiger (2002) recommended—to recalculating these estimates, disaggregated by cohort. We then calculate the correlation in each school's cohort-specific MSVA estimates across each of the 3 years. We find that for our full analytic sample of 355 middle schools, cohort-specific value-added estimates are correlated ~0.6 in consecutive years. As expected, these year-to-year correlations slightly increase when only middle schools with the largest cohort sizes are included (200+ students per cohort)—to ~.7—and slightly decrease when only schools with the smallest cohort sizes are included (fewer than 20 students per cohort)—to .4 to .5 (full results available upon request). Prior studies estimate year-to-year correlations of .6 to .8 for teacher value-added effects on math scores, though several have estimated even lower correlations of .2 to .6 (Loeb & Candelaria, 2012). Thus, if all middle schools are considered, school-level value-added effects' stability appears comparable to, if only slightly lower than, that of teacher value-added effects. Because accountability systems typically target schools at the very top and very bottom of the distribution, we conduct another stability analysis focused on middle schools whose first of three cohorts in our data placed them in the top or bottom quintile of the state distribution. We ask, what percentage of these top- and bottom-quintile middle schools remained in the top or bottom quintile during the subsequent year? The higher the percentage, the more stable MSVA effects are and the more useful their signal is likely to be for policymakers. Our data suggest about 50% of the schools ranked in the bottom and top quintiles based on the first cohort remain in their respective quintiles based on the second cohort. Similar quintile persistence rates are obtained when comparing results from the second to third cohorts (full results available upon request). An analogous set of analyses conducted based on the value-added estimates of San Diego high school teachers (Koedel & Betts, 2007) and Florida elementary school teachers (McCaffrey et al., 2008) generated top- and bottom-quintile 1-year persistence rates of 30% to 50%, depending on the particular model specification used to generate the value-added estimates. Thus, school value-added effects appear to be more valid and stable than previously thought—perhaps even more stable than are teacher value-added effects. Hypothesis 2 is not supported. Having estimated the size and stability of middle schools' short-term effects, we shift to their persistence over time. We begin by decomposing the variance in 10th-grade math scores to discern whether between–middle school differences in these scores are evident 2 years posttreatment. If middle school effects on 10th-grade math scores diminished entirely 2 years posttreatment and students attending the same middle school were not systematically more likely to attend higher or lower value-added high schools, there would be no discernible between–middle school variance in this outcome. However, the unconditional random intercept model of 10th-grade math scores produces a middle school–level intercept of −0.053, with a variance of 0.166, which is significantly different from 0; middle school effects remain discernible 2 years later. The ICC suggests 16% of the total variance in 10th graders' math scores resides between, rather than within, the middle schools they attend—just slightly below the 17% result generated when eighth-grade math scores were considered—providing additional evidence that middle school effects may persist 2 years later. For a more rigorous assessment of MSVA effects' persistence, we add in fourth- and sixth-grade test scores and student-level demographics, the estimated MSVA of the particular middle school the student attended, and finally the estimated HSVA of the high school the student attended. For each model, we evaluate the size and significance of the MSVA covariate's coefficient. Incorporating fourth- and sixth-grade test scores reduces the random intercept's variance from 0.162 in the previous model to 0.030; 80% of the middle school–level variance in 10th-grade test scores is attributable to baseline aptitude and sociodemographic differences between groups of middle school alums (Table 3, Model 1). Once we include the MSVA covariate in the model, a clearer picture of middle school effects' persistence begins to emerge. MSVA's estimated effect on 10th-grade math scores is strongly significant and substantively large (β = 0.107, p < .001; Table 3, Model 2). In other words, a 1 SD increase in MSVA is estimated to increase a student's math score 2 years later (i.e., in 10th grade) by about a tenth of an SD, even after accounting for pre–middle school scores and student-level demographic characteristics. By dividing this predicted MSVA effect on 10th-grade math scores by the original MSVA effect based on eighth-grade scores (recall that a 1 SD increase in MSVA translated to a 0.163 SD eighth-grade score increase), we calculate a 2-year persistence rate of 66%, which exceeds the estimated 15% to 40% 2-year persistence rate of teacher effects calculated by recent studies (Chetty et al., 2014b; Lockwood et al., 2007). To what extent is the persistence of this MSVA effect attributable to post–middle school student sorting? Students from middle schools with high (/low) value-added effects may tend to move on to high schools with high (/low) value-added effects due, perhaps, to a middle school–induced boost to children's educational performance and/or expectations, which in turn, leads them to attend a higher quality high school than they would have otherwise attended had they attended a lower quality middle school. If this is the case, then high school quality–based sorting is a valid mechanism by which middle schools exert their effects on 10th-grade test scores, and the 2-year persistence rate of 66% is applicable regardless of whether HSVA effects mediate some portion of this lingering effect. However, another plausible explanation for correlated middle school and HSVA estimates implies that middle school effects on 10th-grade test scores should be estimated independent of high school quality. If high (/low) value-added middle schools tend to feed into high value-added high schools due to district policies or neighborhood resources that shape the performance of both school types, then the middle schools themselves are doing little to drive the additional test score benefits reaped by attending a high-quality high school, and MSVA effects on 10th-grade test scores should be adjusted accordingly. To account for this scenario, we use the prior model and add in students' HSVA estimate, which is correlated at .18 (unweighted) with students' MSVA estimate, indicating a modest degree of sorting. In this model, the HSVA covariate is highly significant (β= 0.128, p < .001), and the MSVA covariate remains strongly significant, though its coefficient attenuates modestly in size, from 0.107 to 0.085 (a reduction of about 20%). In this more rigorous analysis, a 1 SD increase in MSVA is associated with a math score boost 2 years posttreatment of about 0.09 SD and exhibits an estimated 2-year persistence rate of 52%. High school effects on 10th-grade test scores are about 50% larger than are middle school effects on the same outcome, which is plausible given the temporal ordering of the school treatments. Next we run a model identical to Table 3, Model 3, but add in district fixed effects to partially account for the possibility that difficult-to-observe factors, such as parental wealth and educational expectations, could confound MSVA effects on 10th-grade test scores. We argue that these types of confounders likely vary primarily between rather than within school districts. In this model, the MSVA coefficient modestly diminishes, from 0.085 to 0.071 (p < .001; see Table 3, Model 4), generating an estimated 2-year persistence rate of 44%. For an even more rigorous test of middle school effects, we replace HSVA estimates and district fixed effects with high school fixed effects (full model results available upon request). This model goes even further in netting out school sorting effects by comparing middle school value-added effects on 10th-grade math scores only among students attending the same high school. The MSVA coefficient once again reduces slightly; however, at 0.067, it remains significant and displays a persistence rate of 41%. It is important to remind the reader that the prior three models "control away" the effects of MSVA induced via high school sorting (e.g., by leading students into higher value-added high schools than they otherwise would have attended) and thus reflect a narrower conception of middle school effects. Figure 1 summarizes the calculations of middle school value-added effects' persistence rates based on each aforementioned model specification. Every model specification generates a 2-year middle school effect persistence rate that exceeds the range of estimated 2-year teacher effect persistence rates (i.e., 15%–40%), providing tentative support in favor of Hypothesis 3: School effects are more persistent than are individual teacher effects, at least with regard to math scores. As a final robustness check, we rerun the 10th-grade math score model for only our oldest cohort using middle school value-added estimates based only on the two younger cohorts, and not all three cohorts. Purging the middle school value added estimate of data from the cohort for whom we are tracking 10th-grade math scores further assuages concerns about the reliability and stability of MSVA effects. This approach yields virtually identical results (available upon request) to those produced by the core model output described above. Our final core analysis replicates Equation 3 but replaces the 10th-grade math score with a longer term, non–test score outcome: whether or not a member of the 2004 student cohort enrolled in a 4-year college/university "on time" (i.e., in the year 2010). The unconditional random intercept model of 4-year college attendance produces a middle school–level intercept of 0.553, with a variance of 0.031, which is significantly different from 0 and translates to an SD of 0.176. The ICC suggests that 13% of the total variance in middle schoolers' propensity to attend a 4-year college/university on time resides between, rather than within, the middle schools they attend. Incorporating fourth- and sixth-grade test scores reduces the random intercept's variance from 0.031 in the previous model to 0.011 (Table 4, Model 1). Thus 65% of the middle school–level variance in 4-year college enrollment is attributed to pre–middle school aptitude and sociodemographic differences across middle school student bodies, which is approximately 15 percentage points less than the analogous school-level variance in 10th-grade math accounted for by the same factors. This reflects one of two possibilities: (1) middle schools exert stronger value-added effects on students' decisions regarding college enrollment than on test scores, or (2) unobservable factors, such as parental expectations and aspirations, shape both middle school sorting and the likelihood of 4-year college enrollment, net of sociodemographics. In other words, the relatively large remaining between-school variance may not reflect middle school–induced differences. We attempt to account for this possibility shortly. After controlling for between–middle school sociodemographic and test score differences, MSVAs' estimated effect on 4-year college attendance is strongly significant and substantively large (β = 0.036, p < .001; Table 4, Model 2). A 1 SD increase in MSVA is estimated to increase a student's likelihood of attending a 4-year college or university by about 3.6 percentage points, or an approximately 6% increase relative to the overall average likelihood of enrollment for this analytic sample (57%). The estimate remains significant but reduces by 11% to 3.2 percentage points when the HSVA control is included. However, again, if one believes a valid pathway by which middle schools shape longer term outcomes is by propelling students to attend a higher quality high school than they otherwise would have then this model "over controls" for middle school effects. Regardless, it is important to note that MSVA's effect on 4-year college enrollment is considerably larger than the estimated HSVA effect on the same outcome (β = 0.018, p < .001; Table 4, Model 3). As we alluded above, one might counter that our analytical framework is more susceptible to generating biased estimates of MSVA effects on college enrollment than biased estimates of middle school effects on test score outcomes. In this view, accounting for lagged test score measures likely purges bias induced by omitting variables that predict our core test score outcomes of interest, as prior studies have shown (Deming, 2014). However, because there is no comparable lagged measure of college enrollment—and college enrollment may be influenced by difficult-to-observe factors like parental wealth, social networks, and educational aspirations, which are not included in our data set—it is more difficult to purge omitted variable bias from our estimates of MSVA effects on college enrollment. In an attempt to "net out" the effect of unobservable factors underlying both middle school value added-based sorting and college enrollment decisions, we once again add in district fixed effects (see Figure 2). In this model, the estimated value-added effects of MSVA on 4-year college enrollment reduces by 50%—from 0.032 to 0.016—but the coefficient remains statistically significant (p < .01; see Table 4, Model 4). We then replicate all HLM model specifications from Table 4 using a logit model, with standard errors clustered by middle school (results available upon request). The estimated middle school effects are nearly identical regardless of whether HLM or logit is used. Overall, then, our estimate of approximately 1.5– to 3.5–percentage point boosts associated with a 1 SD increase in MSVA exceeds the estimated 0.1– and 1.0–percentage point increases to the likelihood of college attendance spurred by a 1 SD increase in kindergarten class quality (Chetty et al. 2011) and fourth- to eighth-grade teacher quality (Chetty et al. 2014b), respectively. Hypothesis 4 is supported. Spurred in part by NCLB, rigorous evaluations of teacher and, to a lesser extent, school effects on student test scores have become a fixture of the contemporary U.S. educational landscape. Despite ongoing controversy, value-added models have proven quite durable, and recent analyses suggest they yield largely unbiased estimates. However, the vast majority of value-added models estimate individual teacher effects, rather than school effects, and do so based on end-of-year test scores. In this study, we consider whether schools' effects on test score and non–test score outcomes deserve a greater share of researchers' and policymakers' attention than they have received in recent years. To this end, we estimate middle schools' value-added effects on both short-term (eighth grade) and longer term (10th grade) math scores among three cohorts of Massachusetts public school students and, within the eldest cohort, gauge their effects on 4-year college enrollment. Our two-level models confirm that schools' effects (encompassing both teaching- and nonteaching-related factors) on test scores are initially smaller than are existing estimates of individual teachers' effects on test scores. However, school effects appear nearly as stable across years and more persistent over time, especially when non–test score outcomes are considered. Theories of skill development, combined with our empirical results and our account of plausible school-level mechanisms that shape children's development, lead us to speculate that schools induce larger effects on students' socioemotional skills than do individual teachers, and individual teachers induce larger effects on students' cognitive skills than do schools. Recent evidence suggests that educational interventions' effects on cognitive skills, measured via test scores, rapidly fade whereas meaningful effects on socioemotional skills may persist and, in turn, support long-term life outcomes (Heckman & Mosso, 2014). Although we do not directly measure socioemotional skill growth due to data limitations, the facts that (1) middle school effects on 10th-grade test scores appear smaller than do high school effects and (2) the pattern is reversed when the outcome shifts to college enrollment is consistent with the hypothesis that middle schools may shape socioemotional development. This proposition is theoretically plausible in light of evidence that socioemotional and other skills are more malleable at earlier phases in the lifecycle, and that socioemotional skills exert greater effects on non–test score outcomes, such as college enrollment, than on test scores (Heckman et al., 2006). If true, then schools may matter more than the Coleman report and subsequent education research has suggested, but score-based measures, especially if examined only in the short term, may understate their importance. Our results should be interpreted with caution, given two important concerns raised at the outset. The first concern centers on the possibility that omitted variable bias affects our results, especially given that the independent variable measures we employ are incomplete. Data constraints preclude controlling for a more granular set of student-, family-, school-, and neighborhood-level characteristics that could theoretically bias school value-added estimates (e.g., continuous measures of family income and wealth, parental education and expectations). Our models include HSVA measures, as well as high school and district fixed effects, which may help mitigate these unobserved factors. Moreover, Chetty et al. (2014b) found that finer grained controls drawn from Internal Revenue Service tax data provide limited additional utility in generating unbiased value-added measures, beyond the set of controls traditionally used in such analyses. However, future research using lotteries or natural experiments in Massachusetts would help determine whether our approach over- or underestimated middle school effects in this geographic context. Second, one could counter that our estimated middle school effects merely reflect school-level differences in teaching-related factors, such as (1) the quality the of individual teachers and (2) the quality of the combined teaching workforce. If our data permitted, we would run supplementary analyses leveraging three-level models that explicitly disentangle school effects from individual teacher effects to examine this proposition directly; instead, we use two-level models and benchmark our school effect estimates with the large body of work that has generated teacher effect estimates. Yet we believe our results would remain largely consistent even in a three-level framework. As previously mentioned, prior research suggests the vast majority of the variance in individual teacher value-added quality resides within, not between, schools. Moreover, two studies employing three-level models that decompose the variance in high school and kindergarten students' end-of-year test scores into student-, teacher-/classroom-, and school-level components suggested that the school-level variance component remains similar in magnitude, regardless of whether the teacher/classroom level is explicitly modeled (Konstantopoulos, 2006, 2007a). The kindergarten-based analysis used Project STAR data—which included about 18 students per classroom, four classrooms per school, and 79 schools—and generated a two-level model suggesting that 19.5% of the total variance in test scores resides at the school level. Accounting for the intermediary classroom–level variance via a three-level model reduces the school-level variance component estimate only slightly—to 16.5%, which is nearly identical to the school-level variance estimate we generated based on our two-level model. This same study argued that the larger the number of teachers/classrooms within each school-level data cluster, the more consistent the school-level variance component within a two-level model is predicted to be when the teacher-/classroom-level variance component is excluded (Konstantopoulos, 2007a). In sum, our data set's exclusion of teacher-/classroom-level data may lead to slightly overstated school effect estimates (if conceptualized separate and apart from individual teacher quality), but the unusually large size of our data set should partially assuage this concern. In future work we plan to use three-level models to retest this study's contentions that schools exert sizable, stable, and persistent effects that are not primarily explained by individual teacher quality, and we encourage other researchers to do the same. Other limitations remain. In terms of outcomes, our test score analyses examine math rather than ELA achievement, and our non–test score analyses consider college enrollment but ignore other key milestones, such as college completion, labor market performance, and criminal justice encounters. It is also important to note that the patterns found in this study are Massachusetts-specific and cohort-specific, and they may not replicate in other places and time periods. Jennings et al. (2015) estimated that more of the variance in 10th-grade students' value-added growth resided at the school level in Massachusetts than in Texas (9% vs. 6%). This finding suggests school quality may vary to a greater extent within and between school districts in Massachusetts than in other states. If true, our estimates of school effects' size and persistence may be at the high end of the U.S. distribution. However, the possibilities that Texas is an outlier and that state-level comparisons would change if middle schools, not high schools, were assessed, cannot be ruled out. Furthermore, to reliably estimate school effects, we restricted our sample to students who progress from Grades 4 through 10 "on time" and who remained in the same middle school in seventh and eighth grades. As a result, we cannot draw conclusions regarding middle schools' effects on students who move across schools or who skip or repeat grades. Because these students are disproportionately likely to be disadvantaged, future work will be required to thoroughly analyze the implications of middle schools for race and class inequality, in general, and for disadvantaged students, in particular. Tracking the effects of middle schools on the likelihood of switching schools, repeating grades, and chronic absenteeism would add depth to our understanding of school effects. Despite its limitations, our study underscores the potential utility of shifting both researchers' analytic approaches and policymakers' strategies for creating incentives that facilitate long-term student success. The evaluation literature's orientation should expand from a primary focus on teachers' contributions over a 1-year time period to a combination of teachers'and schools' contributions over longer time horizons. Disentangling teacher and school effects becomes more difficult in this persistence framework, but we believe both types of effect estimates appear relatively stable over time. Large, ideally national, samples and multiple cohorts would increase estimate stability. Our findings also reinforce the value of expanding the dependent variables of interest from test scores to include a wide range of other non–test score outcomes, including socioemotional skills, as well as college enrollment and completion. We recognize the practical difficulties of implementing these shifts. Specifically, it may be very difficult to hold teachers and schools accountable for longer term outcomes that they do not feel they possess direct control over (e.g., college enrollment). Fortunately, our data suggest school value-added effects measured toward the end of "treatment" are predictive of longer term test score and non–test score outcomes and therefore policymakers could justify incorporating this shorter term measure into accountability regimes, at least as a first step. We echo Kane and Staiger's contention that ideally this shorter-term measure would be calculated over at least 2 to 3 years to maximize reliability and stability and ensure schools are not held "randomly accountable" (Staiger et al. 2002). If future studies confirm that school value-added effects are stable, persistent, and predictive of long-term outcomes—and meaningfully distinct from individual teacher value-added effects in three-level models—then identifying the particular school-level mechanisms responsible for middle school effects, and restructuring schools accordingly, will be critical.
10.3102_0002831220963874	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220963874	The Intergenerational Transmission of Teaching	 Parental influences, particularly parents' occupations, may influence individuals' entry into the teaching profession. This mechanism may contribute to the relatively static demographic composition of the teaching force over time. We assess the role of parental influences on occupational choice by testing whether the children of teachers are disproportionately likely to become teachers themselves and whether the intergenerational transmission of teaching varies by race or sex. Overall, children whose mothers are teachers are seven percentage points more likely to enter teaching than children of nonteachers. The transmission of teaching from mother to child is about the same for White children and for Black daughters; however, transmission rates for Hispanic daughters are even larger while those for Black sons are near zero.	 The changing racial and ethnic composition of the United States has led to a more diverse student body: As recently as 1986, about 70% of students enrolled in U.S. public schools were White, while today that number stands at about 50%. Despite such change, the teaching force remains about 80% White and overwhelmingly female (U.S. Department of Education, 2016). This is troubling, given the many benefits that non-White students receive from having a same-race teacher (Gershenson et al., 2018; Redding, 2019). Achieving a racially representative teacher workforce requires recruiting and retaining a diverse set of educators equipped with the racial/ethnic, cultural, and linguistic capital to teach a diverse student body (Dilworth & Coleman, 2014). However, despite aggressive attempts to recruit teachers of color, the demographic composition of the teaching force has yet to change. Such efforts could be improved if we better understood the reasons that individuals choose to teach. Given the importance of teachers in the schooling process, a large literature has investigated the supply of teachers (Dolton, 2020; Guarino et al., 2006). However, this literature largely overlooks a potentially important determinant of entry into the teaching profession: parents' occupation. We address this gap in the literature by investigating the extent to which the children of teachers are more likely to become teachers than the children of other college-educated women, and whether teaching is transmitted across generations at different rates than other professions are transmitted. In doing so, we contribute to the intergenerational mobility literature as well by focusing on one unique public-facing profession. The analysis relies on nationally representative survey data from the National Longitudinal Survey of Youth 1979 (NLSY) and the attached Child and Young Adult Supplement (CYA). We compare the transmission of teaching to that of similar professions, such as counseling, social work, and nursing. We find a strong relationship between maternal and child occupation in the case of teaching, but not for related occupations. This correlation is strongest for the White daughters of teachers, which likely explains some of the persistence of the demographic composition of the teaching force. Specifically, we find that the children of teachers are seven percentage points (88%) more likely to become teachers themselves. This transmission rate is about the same for White children and for Black daughters; however, there is virtually no transmission of teaching from Black mothers to sons. Limited data on father's occupation suggests an intergenerational link as well, but only between fathers and sons. Suggestive evidence indicates that this intergenerational transmission of teaching operates through a number of channels including shaping children's preferences, providing information, and investing in children's formal educational attainment. Given the general importance of teachers in the educational process, a large literature has investigated teacher labor markets, teacher recruitment and retention, and teacher labor supply (Dolton, 2006, 2020; Guarino et al., 2006; James & Wyckoff, 2020; Loeb & Myung, 2020; Sutcher et al., 2019). Much of the empirical work on the topic examines the observable characteristics of new teacher entrants and of the current teaching force as a whole using data on teachers, which limits our ability to make comparisons with nonteachers. Bacolod (2007a) is a notable exception, which shows that some aspects of the teacher workforce have changed over time, such as the academic ability of females entering the profession, which declined as other labor market opportunities for women emerged in the 1960s and 1970s. Specifically, the author finds that women in the top quartile of the ability of distribution in these cohorts were about nine percentage points less likely to enter teaching than their counterparts in the bottom quartile. Henke et al. (2000) find similar patterns in the cohort of 1992-1993 college graduates, where college graduates in the top quartile of college entrance exams such as the ACT or SAT were almost five percentage points less likely to enter teaching than their counterparts in the bottom quartile. Those in this cohort who entered teaching had higher GPAs, though, as college graduates with GPAs above 3.75 were about eight percentage points more likely to enter teaching than their counterparts with GPAs below 2.75. Part of this seeming contradiction could be due to grade inflation in schools and colleges of education (Koedel, 2011). However, more recent evidence suggests that the quality of incoming teachers is on the uptick for teachers entering the profession in the 1990s and 2000s due to new efforts to recruit talented individuals to the profession (Master et al., 2018; Nguyen & Redding, 2018). The demographic composition of the teacher force, meanwhile, has remained fairly stable: It remains female-dominated (about 70% to 80% female) and if anything is becoming more so (Ingersoll et al., 2014). The teaching force also remains about 85% White, as minority teacher recruitment has not caught pace with increasing minority student enrollments (Guarino et al., 2006; U.S. Department of Education, 2016). Redding and Baker (2019) show that racial gaps in teacher entry are primarily due to racial gaps in choosing education majors. Determinants of entry into teaching include altruism (Farkas et al., 2000), flexibility (Flyer & Rosen, 1997), and wages (Bacolod, 2007b). Flexibility, specifically the ability to exit and return, is particularly appealing to women whose labor force attachment is interrupted by childbearing (Farkas et al., 2000; Flyer & Rosen, 1997; Grissom & Reininger, 2012). Student and school characteristics affect where teachers teach, conditional on entering the field (Bacolod, 2007b; Hanushek et al., 2004). However, while an extensive literature documents the demographic characteristics and stated reasons for entering teaching of current and entering teachers, less is known about the socioeconomic determinants of entry into the profession. Specifically, does parents' occupation predict children's occupational choices? We address this question in the context of teaching and teacher labor supply by developing a conceptual model of, and then formally estimating, the intergenerational transmission of teaching. The transmission of occupations from parents to children is one channel through which the intergenerational transmission of socioeconomic status may occur (Jonsson et al., 2009). A seminal study on economic mobility showed that the intergenerational income correlation between fathers and sons was much higher than previously thought (Solon, 1992) and sparked a wave of new research on intergenerational mobility in the United States and abroad (Solon, 2002). Subsequent work showed that the intergenerational income correlation for daughters was somewhat weaker than that of sons, but still significant (Chadwick & Solon, 2002). Factors related to gender (i.e., assortative mating, sexism, and time out of the labor market) may explain this discrepancy. Using richer tax data, Chetty, Hendren, Kline, Saez, and Turner (2014) confirmed that today's children have the same chances of moving up in the income distribution relative to their parents as did children in the 1970s and Chetty, Hendren, Kline, and Saez (2014) document significant geographic variation in mobility rates. Education is also transmitted from parents to children, primarily via family characteristics and innate ability (Black, Devereux, & Salvanes, 2005; Hout & Janus, 2011). There are a number of other possible channels for the transmission of educational attainment across generations (Black & Devereux, 2010). First, highly educated parents tend to have higher incomes, which may affect educational expenditures on the child. Second, parental education may affect flexibility with time, as well as the type of experiences and child-enhancing activities that children of highly educated parents receive (Guryan et al., 2008). Third, higher levels of education may provide greater bargaining power in households, such that the more education a mother has may make her more successful in investing in the child's human capital. Greater maternal education is also associated with greater infant health, prenatal care utilization, marriage, and smoking reduction (Currie & Moretti, 2003). Given the large, established literatures on the intergenerational transmission of income and education, it is somewhat surprising that the intergenerational transmission of occupations is relatively understudied, as this is arguably an important mechanism behind the transmission of socioeconomic status (Jonsson et al., 2009). Recent exceptions include Hout (2018), who shows that occupational status, as measured by an index of average wages and credentials in an occupation, is transmitted across generations in the United States, and Aina and Nicoletti (2018), who show that sons of liberal professionals (e.g., accountants, lawyers, and architects) are more likely to enter a similar liberal profession in Italy. More specifically, Jonsson et al. find that "micro class reproduction" (transmission of specific occupations) is a key channel through which socioeconomic status is transmitted across generations in four countries: the United States, Sweden, Japan, and Germany. Early studies on occupational mobility focus on cultural and family pressure to follow in a parent's footsteps and pay particular attention to farming; see Corak and Piraino (2011) for a brief overview. However, the theoretical reasons for occupational transmission in the case of a family business like farming are unlikely to apply to a professional occupation such as teaching (Anderson, 1941; Blau & Duncan, 1967). Laband and Lentz (1983) propose a model in which children voluntarily follow in a parent's footsteps. This may occur if children seek to emulate or honor their parents. Alternatively, children may learn about, and find appealing, the characteristics of their parent's profession, especially if they cathect their parents or parents exaggerate the positive aspects of their job (Jonsson et al., 2009). The only study of which we are aware that explicitly considers the transmission of teaching is Laband and Lentz (1983), who estimate an empirical model of occupational mobility using a sample of 831 men across three generations, which includes some teachers, but find no evidence that teaching is transmitted from fathers to sons. However, given the dominance of females in the teaching profession and the small and unrepresentative sample analyzed in the study, it is unclear what to make of this null result. We discuss the channels through which teaching might be transmitted from mothers to children in the conceptual model presented below. Finally, while the intergenerational transmission of teaching has not been directly studied, scholars have investigated the parenting practices of teachers and how the children of teachers fare relative to the children of observationally similar nonteachers. For example, having a parent who is a teacher reduces the likelihood of behavioral problems in male children (McFarlin, 2007). Furthermore, more teachers engage in activities such as reading to and helping children with schoolwork at home than do nonteachers (Hansen & Quintero, 2016). However, it is unclear whether such differences lead to different educational and occupational outcomes for the children of teachers. We address this question by specifically testing the hypothesis that the children of teachers are disproportionately likely to go on to become teachers. In doing so, we contribute to both the teacher labor supply and intergenerational mobility literatures. Figure 1 presents a conceptual model of how teaching can be transmitted from mothers to children. This model is motivated by Haveman and Wolf's (1995) review of how parental resources influence children's educational attainment and the intergenerational mobility and teacher labor supply literatures reviewed above. The model articulates the four general ways in which the children of mothers who are teachers might be more likely to enter teaching themselves. There are three indirect ways in which teacher mothers might influence their children's occupational choice. First, parents can influence their children's preferences and personality. For example, they might instill a sense of altruism and a desire to help others in their children, which are antecedents to teaching (Farkas et al., 2000; Watt & Richardson, 2012). Teacher parents might also speak positively about the importance of the job to their children, leading children to prefer teaching over other professions (Han et al., 2018; Jonsson et al., 2009). Second, parents who are teachers might invest more and better-quality time and physical resources into their children's human capital development and formal education (Hansen & Quintero, 2016). This might manifest in both general and specific forms of human capital (Becker, 2009), including teaching-specific credentials, that facilitate certification and ultimately entry into the teaching profession. More generally, the relative household stability and financial security of having an employed parent might increase the odds that the child completes the formal educational and professional prerequisites to teaching (Aina & Nicoletti, 2018). Indeed, the causal relationship between household resources and children's educational attainment is well documented (Aizer et al., 2016; Akee et al., 2010). Third, parents who are teachers can provide "insider knowledge" about the profession (Corak & Piraino, 2011; Jonsson et al., 2009; Watt & Richardson, 2012). This information can affect children's occupational choice in at least two ways. For example, information on the prerequisites and how credentials can be obtained can directly increase children's likelihood of obtaining those credentials and becoming certified to teach. But information on various aspects of the job, such as working conditions, the pay scale, and the ability to take time away from the profession before reentry, can also directly influence children's occupational preferences. Finally, a direct channel through which a profession such as teaching might be transmitted across generations is via parents' professional networks (Corak & Piraino, 2011; Jonsson et al., 2009). Teacher mothers might inform their children of job openings, serve as references, or even lobby school principals and other administrators on their children's behalf. Indeed, as many as 9% of men work in the same company as their fathers (Corak & Piraino, 2011) and as many as 50% of jobs are obtained through family or friends (Loury, 2006). In sum, the conceptual model presented in Figure 1 and described here articulates several channels through which teacher mothers may increase the chances that their own children enter teaching. While our data do not permit the identification of the precise mechanisms behind the intergenerational transmission of teaching, the model described here suggests that such transmission occurs. In addition to testing the hypothesis that the children of teachers are more likely to enter teaching, we will also use the available data to provide some suggestive evidence of why this intergenerational transmission of teaching occurs. We use the National Longitudinal Survey of Youth (NLSY) 1979 and Child and Young Adult supplement (CYA). The NLSY was a nationally representative sample of 12,686 young men and women aged 14 to 22 years in 1979. The NLSY collected a host of educational, labor market, and demographic data for individuals annually beginning in 1979 and biennially from 1994 to 2014. There were 4,934 eventual mothers in the NLSY, who gave birth to 11,521 children. The subsequent CYA followed the 11,521 children of these mothers biennially from 1994 through 2014. Thus, our main analyses are limited to mothers and their children. We apply an age filter of 25 years or above in the main analytic sample to exclude children who were too young to have meaningful occupational data and show that the main results are robust to using different age cutoffs; this means that children born after 1987 are excluded from the analytic sample. Applying this age filter yields a sample of 7,060 children. The analytic sample is further restricted to the 4,572 unique children of 2,488 unique mothers for whom mom's occupation, child's occupation, basic socioeconomic information, and mom's AFQT score are observed. Individual children, nested in mothers, are the unit of analysis; accordingly, all statistical inference is corrected for arbitrary within-mother serial correlation. The key outcome is a binary variable equal to one if the child was a teacher or in an education-adjacent profession at any point during data collection and zero otherwise. Data for child and parental occupation were coded using three- and four-digit Census Industry and Occupational Codes, which varied by year, and are described in Supplemental Appendix A (in the online version of the journal). To make comparisons across generations as consistent as possible, we use the 1970 Occupational Codes for both parents and children, as suggested by the NLSY. The baseline, most inclusive definition of teaching includes primary and secondary school educators; special education teachers, early childhood education (ECE) professionals, trade and industrial school instructors; postsecondary instructors; librarians and library employees; and teacher aides and assistants. A final caveat about coding occupations is that the NLSY 1979 collects occupational information for multiple jobs held in any given year, so we take the first occupation that an individual listed as his or her primary occupation for that survey year. The NLSY contains a wealth of additional information that we adjust for in the regression analyses, as these factors might jointly predict parent and child occupation, including mother's region of residence, AFQT score, mother's birth cohort, mother's college degree, the child's birth year, the mother's marital status, mother's age at birth, total net family income when child was 10 years old, and age at the birth of the child. Similarly, the CYA contains information on the child's educational attainment and employment. Sample weights, provided by the NLSY, are used in all analyses to correct for the oversampling of certain demographic and geographic groups in the NLSY and CYA (Solon et al., 2015). The CYA also contains the child's self-report of their father's occupation. Few data are available on the fathers of the CYA children, however, as the fathers were not directly interviewed. We use the child-reported information on father's occupation to investigate the transmission of occupation from fathers to children. However, we interpret these results with caution, as these data are limited for three reasons. First, children retrospectively report on their father's occupation, so these data may be subject to measurement error and recall bias. Second, there is some ambiguity as to which "father figure" children are reporting on, especially in cases when they do not live with their biological father, which again can lead to measurement error and obfuscates the interpretation of these regressions. Finally, many children simply did not know, or otherwise failed to answer this question, leading to concerns about power and missing data. Given these limitations, we do not use the father data in the main analyses and interpret the results that do rely on these data with caution, as an exploratory analysis. Table 1 summarizes the number, and types, of teachers observed in the NLSY and CYA samples. Panel A does so for children. Column 1 shows that overall, 2.3% of children became primary or secondary school classroom teachers. This is a narrow definition of teaching, which we refer to as Definition 1 (D1). Definitions D2 and D3 expand on D1 by adding special education and ECE educators, respectively. About 4% of children were a teacher according to D3. However, 10% of children were in teaching or education-related fields according to Definition D4, our preferred and most inclusive definition, which further adds postsecondary and vocational instructors, librarians and library workers, and teachers' aides and assistants to D3. We prefer the broadest possible definition because it increases power; however, we verify below that the results are robust to the exact definition used. Moreover, comparing results across definitions will provide some clues as to the channels through which the teaching profession is transmitted across generations. Columns 2 to 6 of the top panel of Table 1 report these means separately by child demographics. Columns 2 and 3 do so by child sex, and, as expected, daughters are more likely to be teachers than sons. The sex gaps are more pronounced in certain types of teaching, most notably in ECE and special education. According to the broad definition, 15% of daughters are in educational professions compared to only 5% of sons. Columns 4 to 6 do so by race and ethnicity. Here, we see that White children are more likely to be primary and secondary teachers and special education teachers, while Black and Hispanic children are more likely to be ECE educators. Using the broad baseline definition, though, results are similar across demographic groups: About 9% of non-White and 10% of White children are in educational professions. Finally, columns 7 and 8 report the teacher means separately for children who did and did not complete a 4-year college degree, respectively. As expected, nearly all primary and secondary school teachers, special education teachers, and postsecondary instructors had a college degree. The positions that do not necessarily require a college degree seem to be ECE jobs and the "miscellaneous" jobs that are largely teacher aide and assistant positions. To understand which jobs are being transmitted we will estimate the model separately by child's and by mother's college degree status. Panel B of Table 1 presents similar means for the mothers. Column 1 shows that 2.6% of mothers were primary or secondary school teachers, which is close to the 3% figure for daughters seen in column 2. Similarly, the broad baseline definition shows that 17% of mothers were in teaching or teaching-adjacent fields, which again aligns with the 15% figure for daughters reported in column 2. Differences by race, shown in columns 4 to 6, are similar to those discussed above for children, as are differences by college degree attainment. Descriptive statistics for children in the analytic sample, the main unit of analysis, are provided in Panel A of Table 2. Column 1 summarizes the full analytic sample and shows that 23% of children completed a college degree and 10% went on to become a teacher. The sexes are approximately evenly represented. The sample is 19% Black, 72% White, and 8% Hispanic. Given the overrepresentation of women and Whites in the teaching force, columns 2 to 6 of Table 1 summarize the sample separately by sex and race/ethnicity. Columns 2 and 3 compare daughters and sons of the NLSY mothers. Consistent with the well-documented reversal of gender gaps in educational attainment (Bailey & Dynarski, 2011) daughters were six percentage points (30%) more likely to complete a 4-year college degree than sons. There are no differences in the racial makeup of the female and male subsamples, which is intuitive given that boys and girls grow up in the same households. Comparing across race in columns 4 to 6, we see that Whites have significantly higher college completion rates than Blacks and Hispanics. Columns 7 and 8 report means for children who went on to be teachers and nonteachers, respectively, using the broad baseline definition of teacher described above. The majority, but not all, of teachers obtained a college degree. 73% of the teachers were female, which is in line with other national estimates of the gender composition of the teaching force. Similarly, 75% of teachers were White, while only 17% and 8% of teachers were Black and Hispanic, respectively. Finally, columns 9 and 10 report means for the children of teachers and nonteachers, respectively. The article's main result, that teachers beget teachers, is apparent: 19% of the children of teachers go on to become teachers, compared to only 8% of the children of nonteachers. This difference is statistically significant at traditional confidence levels and consistent with the regression results presented in the "Results" section. Panel B of Table 2 similarly summarizes the unique mothers in the analytic sample. Column 1 shows that 14% of mothers completed a college degree. The racial make-up of the mothers' sample is similar to that for children; the slight differences are due to racial differences in birthrates. Columns 4 to 6 report means separately for Black, White, and Hispanic mothers, respectively. White mothers were more likely to have a college degree, to be a teacher, and to be married than their non-White counterparts. They also had significantly higher AFQT scores. Finally, columns 9 and 10 of Table 2 report means separately for mothers who were and were not teachers, respectively. Teachers were significantly more likely to hold a college degree, had higher AFQT scores, and were more likely to be married. We adapt a standard model of intergenerational mobility (Solon, 1999) as follows:[MATH](1) where i and j index children and mothers, respectively, T is a binary variable indicating whether the individual was ever a teacher, X is a vector of child and mother controls for gender, race, region of residence, mother's AFQT score, and mother's marital status at birth. ρ is the parameter of interest, which represents the intergenerational correlation coefficient. We test for heterogeneity in [MATH] by augmenting Equation (1) to include interactions between Tj and elements of Xij. We also probe the sensitivity of the results to the exact definition of T and specification of Xij. Robust standard errors are clustered by mother to account for within-family correlation in the idiosyncratic error u as multiple children may be born to the same mother. We estimate the linear probability model described in Equation (1) by ordinary least squares (OLS), though verify in Supplemental Appendix Table B2 (available in the online version of the journal) that the main findings are robust to specifying the right-hand side of Equation (1) as the linear index of a logit model that accounts for the binary nature of the dependent variable. Given the similarity between the OLS and logit estimates, we prefer the OLS estimates because they facilitate straightforward inclusion and interpretation of interaction terms (Ai & Norton, 2003). Table 3 reports estimates of the baseline model defined in Equation (1) using the broad definition of teaching (D4). Columns 2 to 4 augment Equation (1) with interaction terms that allow the intergenerational correlation coefficient to vary by child sex and race. All models control for the full set of sociodemographic controls described in the NLSY data sections. Supplemental Appendix Table B2 (available in the online version of the journal) reports analogous estimates of logit-model average partial effects, which are qualitatively similar, and confirm that the results are not driven by the choice of a linear model. Column 1 of Table 3 shows that children of teachers are seven percentage points more likely than the children of nonteachers to enter teaching. This relationship is strongly statistically significant (p < .01). It is slightly smaller than the 11 percentage point unconditional mean difference observed between columns 9 and 10 in Panel A of Table 1. However, it is arguably practically significant given that it constitutes an 88% increase from the base rate of entering teaching for the children of nonteachers observed in column 10 of Table 1. Estimated coefficients on the demographic controls indicate that conditional on their mother being a teacher, White children are slightly less likely to become teachers than their non-White counterparts and daughters are significantly more likely to become teachers than sons. The model estimated in column 2 allows the intergenerational transmission of teaching to vary by child sex. The estimate of ρ is 0.06, slightly smaller than the baseline estimate in column 1, though remains strongly statistically significant and suggests that the sons of female teachers are significantly more likely to become teachers than the sons of nonteachers. The interaction between mother-teacher and daughter is positive, and at 0.03 is large in size relative to the base effect of 0.06, suggesting that the transmission of teaching from mother to daughter is about 50% stronger than that from mother to son. However, the interaction term is imprecisely estimated and indistinguishable from zero at traditional confidence levels, which might be due to the relatively small share of sons who become teachers. Nonetheless, it is intuitive and consistent with theory that the relationship between mother's and daughter's occupational choice is stronger than that between mother's and son's. Column 3 similarly allows the effect to vary by race by interacting the mom-teacher indicator with Black and Hispanic indicators (White is the omitted reference group). This model assumes the transmission rate does not vary by child sex. Here, the base (White) transmission rate is about 0.08 and remains strongly statistically significant. The Black interaction term is negative, large relative to the base rate, though not statistically significantly different from zero. Still, this suggests a smaller average transmission rate among Black mother-child pairs of about 0.03, which while positive is less than half the size of the effect among Whites. The Hispanic interaction term is large, positive, and marginally statistically significant (p < .10), suggesting that the average transmission rate for Hispanics is twice that of White mother-child pairs. To this point, columns 1-3 of Table 3 provide evidence of positive, strong persistence of teaching across generations for both sons and daughters and for children of various racial and ethnic backgrounds. However, given the strong female presence in the teaching profession, it is possible that transmission rates vary at the intersection of sex and race. Accordingly, column 4 of Table 3 incorporates both sources of heterogeneity in a single model by including triple interaction terms that allow transmission rates to simultaneously vary by sex and race. These results identify some important nuances missed by the more restrictive models estimated in columns 2 and 3, particularly with regard to the lower average transmission rate for Black children observed in column 3. First, the daughter interaction term becomes zero, which means that the transmission rate for White sons is the same as for White daughters and that the positive daughter effect seen in column 2 was driven by race-specific daughter effects. Second, the Black interaction term remains negative, but doubles in size and becomes statistically significant. Comparing the interaction term of −0.10 to the base estimate of 0.08 implies that there is essentially zero (or even negative) transmission of teaching from Black mothers to Black sons, which is troubling given the importance and underrepresentation of Black male educators. Third, the teacher-daughter-Black triple interaction term is positive, marginally statistically significant, and about the same size as the negative teacher-Black interaction. This means that the transmission rate for Black daughters (0.08 − 0.10 + 0.12 = 0.10) is about the same as that for White children (0.08). Finally, the triple teacher-daughter-Hispanic interaction term is positive, more than twice as large as the base rate for Whites, and statistically significant (p < .05). This implies that the transmission of teaching from Hispanic mothers to daughters (0.08 − 0.03 + 0.21 = 0.26) is nearly three times that of Whites (0.08). To this point, we have demonstrated that the children of teachers, specifically White children and Black and Hispanic daughters, are significantly more likely to follow in their mother's footsteps and become teachers themselves than the children of mothers who work in other fields (or do not work at all). Given the potential ramifications of this striking result, we now conduct a series of sensitivity analyses that aim to verify its veracity. We begin by reconsidering how we define teachers. As discussed in the "Data and Method" section, the baseline models use a broad definition (D4) that includes educators of all stripes: primary and secondary school teachers, special education teachers, kindergarten and ECE educators, librarians and teacher aides, and postsecondary and trade-school instructors. There are at least two advantages to doing so. First, the broad definition leads to more mothers and children who are teachers, and thus, increases our statistical power and ability to identify the transmission of teaching. Recall from Table 2 that while 10% of mothers met the broad definition of teaching but only a quarter of those (2.3% overall) were a primary or secondary school classroom teacher. Second, focusing on a broad definition and then whittling it down provides some suggestive evidence on the reasons that such transmission occurs. For example, if we only see transmission of a particular type of teaching, but not of a more general inclination toward education and sharing knowledge with others, then we might think that specific knowledge about the profession is a more important channel than instilling a general altruistic personality or predisposition to helping others. Table 4 reports 16 iterations on the baseline model specified in Equation (1) using four alternative, progressively wider definitions of which mothers and children are teachers. Each row uses a different definition for the mother and each column uses a different definition for the child, such that each cell of Table 4 reports the correlation coefficient from a unique regression that uses a unique combination of mom and child teacher definitions. The top-left cell is the narrowest form of transmission, looking only at whether primary/secondary teacher moms beget primary/secondary teacher children. The bottom-right cell uses the broadest measure for both mothers and children, which replicates the baseline model shown in column 1 of Table 3. The overarching story from Table 4 is that the results are qualitatively robust to how teaching is defined. The four definitions are• Definition 1: primary and secondary classroom teachers only• Definition 2: those in Definition 1, plus special education teachers• Definition 3: those in Definition 2, plus kindergarten and prekindergarten (ECE) teachers• Definition 4: those in Definition 3, plus postsecondary instructors (i.e., professors), individuals who moved across categories (e.g., someone who worked as both a teaching assistant and ECE teacher), librarians and library workers, vocational school instructors, and teacher assistants/aides Column 1 of Table 4 uses the strictest definition, Definition 1, for children. Rows 1 to 4 use Definitions 1, 2, 3, and 4, respectively, for mothers. The cell in row 1 of column 1 shows a point estimate of 0.05, which is similar to the baseline estimate. However, the standard error is slightly larger than its counterpart in Table 3 and the estimate is only marginally significant (p < .10). This highlights the first advantage of using broad definitions: increased precision. That the point estimate is similar in size suggests that the main results described in Table 3 are robust to the precise definition of teaching, a theme that carries over throughout the cells in Table 4. It also shows that the acute transmission rate of being a primary/secondary classroom teacher is similar to that of the general transmission of education-related jobs, suggesting that this is a broad phenomenon that is not limited to the particulars of a specific class of job. Columns 2 through 4 of Table 4 use progressively broader definitions of whether the children became teachers (Definitions 2 to 4, respectively). Row 1 of column 3 shows a practically larger point estimate of 0.10 that is strongly statistically significant (p < .01). This suggests that the children of primary and secondary teacher mothers are more likely to become either primary/secondary or ECE teachers. The estimates in rows 2 through 4 are qualitatively similar to those in row 1, again suggesting that the intergenerational transmission of teaching is a general phenomenon that occurs for all types of educators. The patterns uncovered in Table 4 provide some suggestive evidence that both "general interest in education and sharing knowledge" and "job-specific knowledge" help to explain the intergenerational transmission of teaching. The reason is that we tend to see children of teachers go on to work in pre-K through 12 education (Definition 3), as well as in other teaching occupations (Definition 4), yet we see substantive, significant estimates throughout the table. We return to a fuller discussion of potential mechanisms in the "Discussion" section. Before moving on to other sensitivity and mechanism investigations, we should mention two additional specific types of transmission that can be studied in this framework, though our limited data and sample size prevents a full analysis of these paths. The first builds on the acute transmission of primary/secondary classroom teaching in the upper-left cell of Table 4, by thinking about the acute transmission of other types of teachers. The only group large enough to test this with is ECE educators, and so we estimate the baseline model for the transmission of being an ECE educator. We get an estimated correlation coefficient of .05 that is marginally significant (p = .06), which is consistent with the results in Table 4. It would be interesting to pursue this idea of acute transmission in a larger or more education-heavy data set to similarly look at the transmission of special education, teacher-aide, and similar niche educator positions. Another idea is to think about the socioeconomic direction of these intergenerational transmissions, in terms of the pay or security of the specific education position of the mother and child. Again, we are limited in our ability to push on this idea by the relatively small share of the sample who are teachers, but at a basic level we can consider whether nonprimary/secondary teacher (but otherwise in education) mothers produce primary-secondary teacher children. Here, the correlation coefficient is 0.02, which is marginally significant (p = .08). This weaker, but still substantive effect, foreshadows the importance of college degrees, which are also transmitted from mothers to children. Are these results simply an artifact of the children of teachers being more likely to complete a college degree, which today is a near universal requirement to be a primary or secondary classroom teacher? We proceed in two steps. First, we replicate Table 3, the main results table, using indicators for "4-year college degree or more" as the outcome. Otherwise, these models are versions of Equation (1) that include the same right-hand-side variables. The results of this exercise are reported in Table 5 and tend to mirror the patterns observed in Table 3. For example, column 1 shows that the children of teachers are five percentage points more likely to complete a postsecondary degree, even after controlling for the mother's educational attainment. This effect represents a 26% increase and is marginally significant (p < .10). Unlike Table 3, Column 2 shows that the effect on college completion is driven by sons, as the daughter interaction term is negative and fairly large, yet insignificant at traditional confidence levels. This might be due to the growing gender gap in college completion, if male completion is now more malleable (Goldin et al., 2006). Finally, columns 3 and 4 of Table 5 echo the qualitative patterns seen in the corresponding columns in Table 3. Here, column 3 suggests that the effect is driven by White children, while column 4 shows how this masks important differences at the intersection of sex and race. Specifically, Column 4 shows that Black and Hispanic daughters of teachers are more likely to complete college than Black and Hispanic sons of teachers, though overall the effects of having a mother who is a teacher on college completion for non-White children is close to zero. The results presented in Table 5 suggest that the children of teachers, specifically White children and non-White daughters, are more likely to earn a college degree than the children of nonteachers. Because college degrees are now a prerequisite to many jobs in the education sector (e.g., primary and secondary school classroom teachers, postsecondary instructors), this begs the question of whether the previously documented intergenerational transmission of teaching is merely an artifact of these effects on college completion. We probe this question by reestimating variants of the baseline model specified in Equation (1) for the subsample of children who completed a college degree. The goal here is to see if among college educated children, the children of teachers are still more likely to become educators than the children of nonteachers. These results are presented in Table 6. Column 1 of Table 6 shows that the article's main finding—that teaching is transmitted across generations—is robust to restricting the sample to college-educated children. This means that encouraging and facilitating postsecondary educational attainment is not the sole mechanism through which teachers beget teachers. Specifically, the point estimate of 0.09 is similar in magnitude to the baseline estimate of 0.07 and is marginally significant (p < .10). The decrease in precision (the standard error more than doubled in size) is likely due to the dramatic reduction in sample size from 4,572 to 847. Consistent with the college completion results reported in Table 5, columns 2 and 3 of Table 6 show that the effect is larger for sons and smaller for Black children, though these differences are not statistically significant. And column 4 of Table 6 once again shows stark differences in the intergenerational transmission of teaching between non-White sons and daughters. The transmission rates for college-educated Black and Hispanic daughters are quite large, at (0.14 − 0.10 − 0.41 + 0.56 = 0.19) and (0.14 − 0.10 − 0.25 + 0.52 = 0.31), respectively. Again, this is consistent with the main results reported in Table 3 and suggests that for all demographic groups, a higher likelihood of earning a college degree is not the sole channel through which the children of teachers are more likely to enter teaching. Finally, Table 7 probes the sensitivity of the main results to the set of mother and household characteristics adjusted for in the regression model. The primary goal here is to shed some light on the extent to which the documented relationship between mother's and children's occupation might be biased by confounders. The intergenerational correlation coefficient is fundamentally a descriptive parameter and we make no claims that we are identifying the causal effect of having a mother who is a teacher. Still, by reporting estimates from models that adjust for varying levels of potential confounders, we provide some suggestive evidence that the estimated relationship is not entirely spurious. Column 1 of Table 7 replicates the baseline estimate from column 1 of Table 3 to facilitate comparisons. Column 2 presents estimates of a naïve version of Equation (1) that contains no control variables. The estimated correlation coefficient increases a bit to 0.10 and remains strongly significant. That the estimate increased suggests that there was positive selection into teaching on the part of mothers and this can loosely be interpreted as an upper bound for the potential rate of teaching transmission. Column 3 adds mothers whose occupation was missing to the analytic sample and adds a binary indicator for missing occupation data to the set of baseline controls. Doing so does not change the baseline estimate, suggesting that missing data on mother's occupation is not biasing the results. Columns 4 and 5 selectively remove one key control variable, AFQT score and household income, respectively. Again, the estimated correlation remains practically unchanged. This is not entirely surprising, given the rich set of remaining controls for other aspects of aptitude and socioeconomic status, such as educational attainment, age at first birth, and marital status. Columns 6 and 7 of Table 7 estimate the baseline model separately by mother's educational attainment. The estimated transmission rate is quite similar for the children of college-educated and non-college-educated mothers, once again proving the main result to be quite robust. Specifically, that the effect does not vary much by mother's educational attainment suggests that an interest in education is being transmitted generally, and that the correlation is not purely driven by job-specific information. Also, the slightly larger effect of college-educated mothers is consistent with the slightly larger estimates in Columns 7 to 9 of Table 4, as the jobs included in the tighter definitions of the mother being a teacher are also the jobs that are more likely to require a college degree. Finally, column 8 estimates the baseline model using a modified definition of whether the mom was a teacher while the child was a present and impressionable member of the household (i.e., aged 2–18 years). This estimate is nearly identical to the baseline estimate, which is unsurprising because mothers very rarely became teachers after their children turned 18 or exited teaching never to return prior to their children turning 2. Nonetheless, it is reassuring to see that the intergenerational effect is driven by mothers who were teachers while their children were present in the household and who could "see their mother in action." Columns 6 and 7 of Table 7 found negligible differences in the transmission of teaching between mothers with and without a college degree. This raises the question of whether there are differences in transmission rates between other types of mothers. This sort of heterogeneity analysis is useful for at least two reasons. First, from a practical standpoint, seeing where this effect is strongest will further enhance our understanding of who enters the teaching profession. Second, and perhaps more importantly, a thoughtful heterogeneity analysis can suggest some of the mechanisms through which teaching is transmitted across generations. Table 8 tests for such heterogeneity along five dimensions by augmenting the baseline model of Equation (1) to include interaction terms. Column 1 interacts the teacher indicator with an only-child indicator. The conceptual model suggests that this interaction effect might be positive for three distinct reasons. First, the parents of only children can likely invest more time and money in their human capital, which is a necessary precursor to entering teaching. Second, parents' ability to spend more time with an only child might lead to a stronger shaping of their altruistic preferences. Third, and similarly, that additional parent-child time might also increase children's knowledge of the teaching profession. However, the estimate of ρ is unchanged and the interaction term is close to zero and statistically insignificant, indicating that the transmission of teaching is neither mediated nor moderated by the presence of household siblings. This suggests that increased one-on-one time with parents is not the primary driver of the intergenerational transmission of teaching. Column 2 again augments the baseline model to allow the transmission rate to vary by the mother's marital status at the time of their first birth. We use this definition for simplicity, as mothers' marital status changes over time, and because mother's marital status at birth is a marker of socioeconomic status and household resources. The conceptual model described in Figure 1 suggests that the direction of this interaction effect is theoretically ambiguous. On the one hand, occupational transmission could be stronger among single mothers, due to their being the sole adult role model in the household. In a two-parent household, children would be exposed to the mother's partner's occupation as well, which could diminish the role-model effect. This would influence both the knowledge and preferences channels through which mother's occupation is passed to children. On the other hand, occupational transmission would be weaker among single mothers if they have less resources and time to invest in children's human capital or if their busy schedules turn their children off from following in their footsteps by making them think ill of the profession. The interaction term is negative and fairly large at more than half the base effect, though it only approaches statistical significance. This suggests that married mothers are marginally more likely to pass along the teaching profession to their children than are teachers who are single mothers, meaning that either the resource or time constraints faced by single mothers likely inhibits the transmission of teaching to their children. Column 3 allows the effect to vary by locale. Once again, the direction of this variation is theoretically ambiguous, as the conceptual model in Figure 1 provides multiple ways in which locale might affect the intergenerational transmission of teaching. The NLSY locale codes are urban, rural, and unknown/missing, so we make do with these crude indicators. They are crude in the sense that urban is a broad category that includes suburbs and large towns. Still, there is reason to think that labor markets and job opportunities in more developed areas differ from those in more rural areas, and thus the occupational transmission rates might vary by locale as well. Specifically, transmission might be weaker in urban areas if there are more nonteaching job opportunities and influences in densely populated metro areas, which would depress children's interest in teaching and enable them to access more information about alternative careers. Alternatively, the presence of more teaching jobs and a broader professional network in larger urban areas might lead to stronger transmission rates if parental referrals are the key driver of the intergenerational transmission of teaching. However, we find no evidence that this is the case, as the urban and rural interaction terms are statistically insignificant; it could be that the positive and negative effects cancel each other out, as they need not be mutually exclusive. That said, motivated by evidence that intergenerational mobility varies by region (Chetty, Hendren, Kline, Saez, & Turner, 2014), an earlier version of this article (Jacinto & Gershenson, 2019) tests for and finds no significant differences by region of the country (the NLSY identifies four regions: South, North Central [Midwest], North East, and West). This null finding, like that for locale, might be due to the relatively crude identifiers available in the NLSY. Differences might emerge if we were able to examine differences across finer grained areas, such as commuting zones, as in Chetty, Hendren, Kline, and Saez (2014), given the prevalence of racial and socioeconomic segregation in school enrollments (Owens et al., 2016). We necessarily leave this potentially interesting question for future research. Column 4 investigates whether intergenerational transmission rates have changed over time. Once again, whether they would be increasing or decreasing over time is theoretically ambiguous. One reason to think they would be falling is that the informational role of parents might be displaced by the plentiful and accessible information on the internet and social media. But the informational and preference-shaping role of parents could just as easily be increasing over time, if parents actively share information and encourage their children to enter teaching due to perceived teaching shortages. In any case, we find no evidence that intergenerational transmission of teaching has changed over time, at least during the roughly 15 years covered in the analytic sample. This is perhaps unsurprising, as children of the NLSY subjects who are old enough to satisfy the age filter were largely born in the mid-1980s and thus cohort differences are relatively small. It would be interesting to conduct a similar analysis of the NLSY-97 once their children have come of age, or other longer time series data sets, to compare across decades. Finally, Column 5 of Table 8 separates mothers into three groups: those with 1 to 4, 5 to 9, and 10 or more years of teaching experience (0 years of teaching experience, i.e., nonteachers, are the omitted group). The conceptual model in Figure 1 provides at least three reasons to believe that the intergenerational transmission of teaching would be strongest among the mothers who have been teaching the longest. First, the children of moms who taught longer were exposed to the teaching profession over a longer time span, and thus received more information about teaching, than the children of moms who taught for only a few years. Second, the longer a mom was a teacher, the stronger and larger her professional network is likely to be, making the referral channel more relevant. Finally, mothers who taught longer presumably enjoy teaching more, and thus, are more likely to speak positively about the profession to their children. All of these point to a stronger effect for mothers who were teachers for a longer amount of time, though we are unable to identify which of these three channels is key. The data verify this intuition, as the transmission of teaching is monotonically increasing in mother's teaching experience. Having a mom with 1 to 4 years of teaching significantly increases the likelihood of their child becoming a teacher by 5 percentage points relative to a nonteacher mother, but having a mom with 5 to 9 and 10 or more years of teaching experience increases the likelihood that their child enters teaching by 12 and 15 percentage points, respectively. These two effects are not significantly different from one another, but both are significantly larger than the 1- to 4-year effect at the 95% confidence level. This experience gradient lends further credence to the idea that teaching really is transmitted across generations. Table 9 uses data on father's occupation to investigate the transmission of teaching from father to child. Recall these data are limited in two ways: Father's occupation is missing for more than half the sample and even when observed, is prone to measurement error. With those caveats in mind, we estimate several variants of the baseline model (Equation 1) using these data. Columns 1 to 3 exclude observations that are missing father's occupation. Column 1 replicates the baseline model and shows that in this restricted sample, the transmission from mother to child remains similar in magnitude (0.07) to that found in the full sample and is statistically significant (p < .05). Column 2 augments this baseline model to also include an indicator equal to one if the father was a teacher, and zero otherwise. The coefficient on the mom-teacher indicator is unaffected by the inclusion of the dad-teacher indicator. The coefficient on the dad-teacher indicator is positive and twice as large as that on the mom-teacher indicator but is imprecisely estimated. The imprecision might owe to both measurement error in the child-reported data on father's occupation and the small share of fathers who are teachers. This suggests that teaching is transmitted across generations by mothers and fathers. Finally, column 3 considers whether there is a multiplicative effect of both parents being teachers. It appears not, as the indicator for both parents being teachers is negative and imprecisely estimated. Taken at face value, the children of two teachers are 12 percentage points (0.12 = 0.07 + 0.21 − 0.16) more likely to become teachers than the children of nonteachers, which is not significantly different from the 7-percentage-point effect of the mother being a teacher, as neither the father-teacher nor the two-teacher indicator is significantly different from zero. The sample used in columns 1 to 3 of Table 9 is endogenously selected, of course, so it might not represent the full sample. Accordingly, the remainder of the table uses the full sample and adds an indicator for children whose father's occupation is missing. Column 4 replicates the baseline model Equation (1), including the dad-missing and dad-teacher indicators, and replicates the main result of an estimate of ρ = .07. Like in column 2, the dad-teacher indicator is large and positive, but imprecisely estimated. Again, this suggests that the main result for the transmission of mother's occupation is robust to controlling for the father's presence and occupation, and that having a teacher father might increase the likelihood that a child enters teaching. Column 5 allows again for a multiplicative effect of both parents being teachers. Once again, the indicator for both parents is negative and imprecisely estimated. Taken at face value, the children of two teachers are 7 percentage points (0.07 = 0.08 + 0.19 − 0.20) more likely to become teachers than the children of nonteachers, which is the same as the baseline estimate of a 7-percentage-point effect of mother being a teacher, as neither the father-teacher nor the two-teacher indicator is significantly different from zero. Finally, columns 6 and 7 of Table 9 restrict the sample to sons, to see whether the effect of the father being a teacher matters more for sons than for daughters. Column 6 shows that for sons, holding constant the mom's occupation, having a father who is a teacher increases the likelihood of the son becoming a teacher by 18 percentage points, which is three times as large as the effect of the mother being a teacher, and approaches marginal statistical significance. This suggests that any effect of the father being a teacher on the child's entry into teaching is primarily driven by the occupational choices of their male children. This runs counter to the effect of the mother being a teacher, which seems to have equal effects on sons and daughters, at least among White children. Finally, Column 7 includes the interaction term that allows for a multiplicative effect of both parents being teachers. Like in the full sample, the interaction term is negative, but is even larger and "undoes" the large and statistically significant father effect of 34 percentage points (p < .05). Specifically, sons of two teachers are 10 percentage points (0.10 = 0.06 + 0.34 − 0.30) more likely to become teachers than the children of nonteachers, which is larger than the effect of having only a mother teacher, but smaller than the father-only effect of 0.34. The reason for this counterintuitive result is unclear and merits further investigation. One possibility is that when the father is the only teacher in the household, or the only parent, this occupation is stickier for sons and the perceived "feminization" of teaching is lessened. Alternatively, this could be driven by outliers in a small cell size, as only 20 children had parents who were both teachers. We conclude our empirical analysis by estimating three separate versions of Equation (1) for the intergenerational transmission of counseling, social work, and nursing. While a thorough analysis of the transmission of occupations is beyond the scope of the current study, it is useful to conclude by presenting some analogous estimates for other professions, using the same data, sample, and model specification as in our main teacher analyses, as these estimates provide some context for interpreting the magnitude of the baseline teacher estimates. We chose these three occupations because they are similar to teaching in one way or another. Counseling and social work are classified by the Occupational Information Network (O*NET) as the most frequent professions that former teachers move to. This is an objective way to choose similar professions with similar qualifications and skillsets. We chose nursing because it is typically regarded as a "pink collar" profession like teaching in which the majority of workers are female (Liben et al., 2001). Means for these professions are reported in Supplemental Appendix Table B3 (available in the online version of the journal). Estimates of these three models are presented in Table 10. The baseline teaching estimates are reproduced in column 1 for comparison. The transmission rate for each of these three nonteaching occupations is positive, but at 0.02 they are all less than one third the size of the teaching coefficient. Moreover, only the nursing transmission rate is even marginally significant (p < .10). This suggests that there is something unique about teaching, at least compared to other similar professions, in terms of intergenerational transmission of occupation. We provide novel evidence on the intergenerational transmission of teaching. Nationally representative data from the NLSY-79 cohort shows that the children of teachers were seven percentage points (88%) more likely to become teachers than the children of nonteachers. This result is robust to a number of modeling and coding choices and is unlikely to be entirely driven by unobserved factors that jointly determine mothers' and children's entry into teaching. The magnitude of this point estimate is similar to that of other predictors of entry into teaching. For example, the current study finds that when the mother holds a college degree, children are about five percentage points more likely to enter teaching. Similar point estimates of 0.05 and 0.08 are found for being in the bottom versus top quartile of college entrance exam scores and having a college GPA above 3.75 versus a GPA below 2.75, respectively (Henke et al., 2000). The transmission of teaching from mother to child is 50% larger for daughters than sons, but this difference is imprecisely estimated and masks important racial variation in the transmission of teaching: The transmission rate is about the same for White sons, White daughters, and Black daughters. However, there is essentially zero transmission of teaching from Black mothers to their sons, while the transmission from Hispanic mothers to their daughters is even stronger than that seen for White children and Black daughters. These findings corroborate qualitative work by Schutz et al. (2001) who found that around 10% of preservice teachers cited encouragement from parents or family members in their decision to become a teacher. And while parents' occupation does not fully explain entry into teaching (the regression R2 = 0.08) it does shed light on one novel determinant. But why? Does the transmission of teaching occur because of parental pressure or role-modeling, network membership, transmission of prosocial attitudes, provision of information, investments in children's human capital, or a combination of factors? The data available in the NLSY prevent us from fully identifying the mechanisms that drive this occupational transmission, though our sensitivity and heterogeneity analyses do provide some suggestive evidence of how and why this transmission occurs. Generally, our results suggest that several mechanisms are in play. For example, we find that the children of teachers are more likely to complete a college degree than the children of nonteachers, even controlling for whether the mom completed college; this suggests that teachers' investments in their children's human capital, as articulated in the conceptual model of Figure 1, helps to facilitate the intergenerational transmission of teaching since college degrees are generally a necessary but not sufficient condition to entering teaching. However, we also see that transmission of teaching occurs among children who did not complete a college degree. This suggests that there is more to the story than the mere provision of formal credentials. Together with evidence that moms with 5 or more years of teaching experience are more likely to transmit teaching to their children than mothers who exited the profession, this suggests that other channels, such as molding children's preferences and personalities, may be important channels as well. This is admittedly speculative and we encourage future research to rigorously investigate the precise mechanisms, as they have implications for teacher recruitment and retention initiatives. Specifically, state or district longitudinal administrative data spanning several decades might allow for rigorous examination of the hypothesized professional networking and referral channels, to see whether children work in the same schools and districts as their parents, or whether they teach in similar grade levels and subject areas. Another fruitful approach would be to leverage original surveys, mixed-methods, and qualitative research to thoroughly investigate all of the potential channels through which this sort of intergenerational transmission occurs. In survey data, for example, a study of household moves could investigate the role of social and professional networks in providing access to teaching jobs or ask teachers about their parents' occupations and the professions besides teaching that they considered. The current study, and the NLSY-79 data, have some additional limitations that merit mention. The data cover the children of one cohort of mothers who were young adults in 1979, which limits our ability to generalize these results to more recent cohorts who came of age facing a different labor market, economy, and K–12 policy landscape. For this reason, it would be useful for future research to use more recent data, such as the NLSY-97 or the Panel Study of Income Dynamics (PSID). Also, the data on father's occupation is of limited quality, which again highlights the usefulness of replicating this type of research in another data set, such as the PSID. The geographic identifiers in the NLSY are also fairly crude, both with regards to the precise location of a household and the type of locale the household is in. More detailed data on specific commuting zones, as in Chetty, Hendren, Kline, and Saez (2014), would help to better understand the specific types of locales in which this transmission is strongest. Similarly, we do not observe the type of school or district that parents or children work in, which prevents a careful analysis of how work conditions moderate the intergenerational transmission of teaching; we believe that it might, since working conditions predict teacher turnover and teachers' preferences for specific schools (Loeb et al., 2005). Nor do we observe the precise teaching assignment of teachers, which once again might moderate the relationships of interest in the current study. Detailed administrative data could address some of these limitations, and we hope that future work on the topic can utilize such data. With those limitations in mind, the study adds to the literatures on intergenerational mobility and teacher labor supply by providing novel evidence that entry into the teaching profession is transmitted from mother to child. There are some general policy implications of these results as well, particularly of the finding that transmission rates are approximately equal for White sons and daughters and Black daughters in the occupational transmission of teaching. The reason is that the intergenerational transmission of teaching perpetuates the underrepresentation of males, Blacks, and particularly Black males in the teaching force. Together with the many benefits that students of color receive from having a same-race teacher (Gershenson et al., 2018; Redding, 2019), this highlights the need to further increase and support efforts to recruit and support teachers of color, and particularly, male teachers of color. Increasing teacher diversity directly will likely have knock-on effects in future generations, as the children of those newly recruited Black and Hispanic teachers are themselves more likely to enter the profession as well. This could be considered in cost-benefit analyses of such interventions. Another general implication arises from the suggestive result that mothers transmit teaching to their children by shaping their children's interest in helping others and their preferences for a prosocial profession. This is a likely channel because we see transmission of teaching as well as of teaching-adjacent professions that do not require specific credentials or college degrees. Recruiting initiatives fighting the recent downturn in teacher-training enrollments (Sawchuk, 2015) could mimic this by focusing on the unique and important opportunities that teachers have to help others and contribute to society, in addition to specific information about the unique requirements for, and flexibility of, teaching. Finally, aside from issues of demographic representation, it is not obvious whether this type of occupational transmission is good or bad for the overall quality of the teaching force, as answering this question depends on understanding the reasons that such transmission occurs (e.g., is it due to network effects or the transmission of a passion for teaching). Once again, future work would do well to explore the underlying mechanisms, and to directly test whether the children of teachers who enter teaching are more effective in the classroom than the children of nonteachers, or even whether effective teaching itself is transmitted across generations. The latter question could be tested using value-added measures in a large district or state in which administrative data cover multiple generations.
10.3102_0002831220963908	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220963908	Student Mobility and Violent Crime Exposure at Baltimore City Public Elementary Schools	 High levels of school mobility are a problem in many urban districts. Many of these same districts are also dealing with high rates of violent crime. In this study, we use 6 years (2010–2011 to 2015–2016) of administrative data from Baltimore City public elementary school students and crime data from the Baltimore Police Department to examine whether changes in violent crime at schools are associated with the likelihood of school exit. Using logistic regression with school fixed effects to adjust for constant differences between schools, we find that students are more likely to leave following years with higher levels of violent crime at their school. These associations are strongest for students ineligible for free or reduced-price meals and from safer neighborhoods.	 Turnover and student mobility are challenges facing many large districts in urban areas (Miller & Sadowski, 2017; Rumberger, 2016; Welsh, 2017). Frequent student mobility can make it difficult to properly sequence material and build the trust necessary for learning (Beatty, 2010; Beck et al., 1997; Bryk et al., 2010; Grigg, 2012; Hartman, 2002; Fiel et al., 2013). As a result, high turnover rates negatively affect both mobile and nonmobile students (Raudenbush et al., 2011; Whitesell et al., 2016). Understanding why students change schools and what, if anything, schools can do to reduce mobility is essential for providing high-quality instruction for all students. Most of the research on the sources of student mobility focuses on individual hardship and academic fit (Hanushek et al., 2004; Pribesh & Downey, 1999). However, parents frequently describe school safety as a primary concern in enrollment decisions, and many of the same districts that experience high student mobility are also are plagued by high levels of violent crime (Casella, 2001; Condliffe et al., 2015; Krivo et al., 2018; Papachristos et al., 2018). To date, higher rates of residential and school instability among populations exposed to high levels of violent crime have been noted, but not examined in detail (Alexander et al., 1996; Chen, 2008; Kerbow, 1996). Documenting that exposure to violence has an independent effect on school mobility is important for understanding both the sources of student mobility and what schools and districts could do to reduce student turnover. In this study, we use 6 years (2010–2011 to 2015–2016) of administrative enrollment data from Baltimore City Public Schools (BCPS) and incident-level crime data from the Baltimore Police Department (BPD) to test whether changes in the violent crime rate at a school are associated with an increase in the likelihood of changing schools. Using logistic regression with school fixed effects that adjust for constant differences between schools, we find that students are more likely to transfer following years with higher violent crime levels at their school. These associations are strongest for students who are ineligible for free and reduced-price meals (FRM) and students living in safer neighborhoods. These findings have important implications for our understanding of the challenges facing many urban districts. First, they add to our appreciation of the collateral consequences of urban violence and document that the direct effects of trauma on stress and cognitive functioning are only the tip of the iceberg. Changing schools is stressful under the best of circumstances (Grigg, 2012), and when motivated by safety concerns might be even more difficult for students. Second, our findings add nuance to the limited research on the reasons why students change schools. Rather than moving for reasons related to individual hardship or improved academic fit, our findings suggest that some students move in response to safety concerns at their current school. In other words, even mobility which may not appear strategic by traditional metrics of school academic quality may be a purposeful response to other metrics of quality, such as exposure to violent crime. Finally, the findings highlight one source of the instability and churning that plagues many urban districts and underscore how difficult it is for schools to function in an urban environment where many students and families are exposed to frequent violence. School changes can be divided into two broad categories: structural and nonstructural moves (Welsh, 2017). Structural moves are required by the organization of the district. For example, in many districts, students must change schools when moving from elementary to middle school and middle to high school. Structural moves also occur when the district restructures by closing a school or changing grade configurations. These moves are not made by individual students and are involuntary. While these moves may still disrupt a students' social and academic environment, they are not considered the most disruptive kind of moves. Districts, schools, parents, and children are prepared for them to happen, often years in advance, and large numbers of students make the moves together (Grigg, 2014). Nonstructural moves, on the other hand, occur when a student makes a transfer that is not required by the organization of the district. These students could have at least in theory stayed at their previous school, and they are often making the move on their own rather than with a cohort of classmates. It is nonstructural moves that attract the most attention from researchers due to their potential negative effects on individual student achievement and as the source of hard-to-manage classroom instability (Raudenbush et al., 2011; South et al., 2007; Whitesell et al., 2016). The literature on the sources of nonstructural school mobility can be further divided between two overlapping categories of moves: reactive versus strategic and school related versus nonschool related (Rumberger et al., 1999; Welsh, 2017; see Table 1). Nonschool-related moves take place due to family-specific changes that are not directly linked to a student's experience at school. These changes often occur simultaneously with a residential move to a new catchment area that forces or enables the student to change schools. Nonschool-related moves can either be strategic, purposeful, and planned in order to improve the family's circumstances, such as a new job or upgrading to a more desirable home or neighborhood, or an unplanned reaction to negative shocks, such as foreclosure, eviction, divorce, or job loss (see Comey & Grosz, 2011; Desmond, 2016). Reactive, nonschool-related transfers are often associated with lower achievement scores (Temple & Reynolds, 1999) and higher dropout rates (Gasper et al., 2012; Rumberger & Larson, 1998), but Pribesh and Downey (1999) argue that the cooccurrence of stressful events affecting children's home and family environments accounts for much of the negative effect of this kind of school mobility on academic outcomes. While the root cause of nonschool-related mobility cannot be controlled by the school or district, there are policies that can limit student instability due to these factors. The McKinney-Vento Act, for example, allows students who become homeless to remain enrolled in their current school and even provides transportation to help them do so (Fantuzzo et al., 2012; U.S. Department of Education, 2016). On the other hand, some moves are made based on school characteristics, not family circumstances. These moves can also be thought of as either reactive or strategic. In the Tiebout model of urban sorting (Tiebout, 1956), strategic school-related moves are made purposefully by students and parents to attain a preferred school placement. The presumed increase in academic quality and fit with student needs may explain why some moves result in improved academic outcomes (Hanushek et al., 2004). Reactive school-related moves often stem from disciplinary problems. Students with behavior problems may be formally expelled or counselled out of their current school (Losen & Martinez, 2013). These moves are precipitated by things that happen at school, not at home, but are involuntary and unplanned (see Table 1). Unlike most other sources of mobility, it is not immediately clear whether we should think of moves related to school violent crime as strictly reactive or strategic. On the one hand, families who change schools after an increase in violent crime at a school are clearly reacting to unexpected circumstances. On the other hand, they are making a strategic, voluntary decision to improve the perceived quality of their child's school. They are just trying to improve school safety and not necessarily academics. Therefore, we argue that safety-related moves should be thought of as strategic reactions: Students who change schools due to safety concerns are both "reacting" to their experience at the school and acting "strategically" to avoid perceived danger. This type of move is not involuntary or only due to intense social or economic stress. Instead it is another reason that families intentionally seek out better learning environments for their children. Research shows that school safety and exposure to violence are associated with parental and student satisfaction. In interviews, parents express a strong desire to limit children's exposure to violence in their school environment (Condliffe et al., 2015; Lindle, 2008). Especially in urban school districts, families report that safety is a key factor they consider in their school enrollment decisions (Goldring & Hausman, 1999; Kleitz et al., 2000). For example, Bulman (2004) finds that families are more likely to identify schools as "good" schools when they perceive them as less violent. Student absenteeism and dropout are more likely in schools that students perceive as violent and unsafe (Brookmeyer et al., 2006; Kearney, 2008). School safety is also often mentioned as a potential school-related reason for transferring (Kerbow, 1996; Rumberger, 2016; U.S. Government Accountability Office, 2010), and students are less likely to attend school and more likely to transfer to a new one after being victims or witnesses of violence (Akiba, 2008; Benbenishty & Astor, 2005; Carson et al., 2013; Dake et al., 2003; Swahn & Bossarte, 2006). However, these associations do not necessarily mean that students are willing to change schools due to changes in reported crime at their school. First, it is not clear whether safety concerns influence transfer rates after initial enrollments have been taken into account. Families may value safety and choose to enroll only in a school they consider safe, but whether changes in violent crime at a school would be enough to induce a school transfer has yet to be determined. Second, exposure to school violence is not equally distributed across the population and is highly correlated with other types of disadvantage that can lead to school mobility (Burdick-Will, 2013). Adjusting for the influence of other individual- and school-related factors is necessary to show that students are responding to the crime at or around their school. The distinction between the broad types of mobility outlined above and the ways that individuals respond to safety concerns can help generate specific hypotheses about who and when we might expect the strongest effects of school violence on student mobility. Hypothesis 1: Increases in a school's violent crime rates will predict increases in the probability of a summer school transfer. Interviews with residents of violent neighborhoods suggest that a shift in personal narrative is needed for someone to change their sense of safety enough to consider leaving (Rosen, 2017; Small, 2004). Therefore, we expect that fluctuations in the violent crime rate at a school will generate mobility. If a school with a high, but stable violent crime rate has a high mobility rate, it is more likely due to nonschool-related difficulties in the student population than reactions to perceived safety. Moreover, if we consider safety-related moves to be largely strategic, we should expect to see these kinds of transfers take place during the summer months when they are less academically disruptive (Welsh, 2017). Some midyear moves may also be a result of exposure to school violence, but there will also be parents who are willing to wait until the change can be made in the least disruptive way possible. Hypothesis 2: The relationship between school violent crime and school transfer will be stronger for students who do not receive free or reduced-priced meals. If we consider moves driven by school violent crime to be strategic, we would expect they are also more likely to take place in families with more social and economic resources. Families must not only want to change schools but must also be able to select and enroll in an alternative that they expect to be an improvement. This does not mean that lower income families are not aware of or worried by violent crime at school, but that they may feel less empowered or able to take concrete action in response (see Pattillo, 2015; Schilbach et al., 2016). If, on the other hand, we find that students with fewer economic resources are actually more likely to respond, this would suggest that we may be picking up on unmeasured stressors that confound the relationship. Hypothesis 3: The relationship between school violent crime and school transfer will be stronger for students from neighborhoods with lower violent crime rates. Students may view the same incident differently depending on their relationship to those involved or their prior exposure to violence. In other words, a single event in a relatively safe place can lead to substantial changes in perceived safety, but it may take larger changes to shift perceptions when students or residents are used to violence. Similarly, we expect that students who live in more violent neighborhoods will be less likely to respond to school violence by transferring. First, students who live in violent neighborhoods are likely to have lower levels of the social and economic resources needed navigate a school transfer. Second, they and their families are likely to be somewhat desensitized to local violence and have developed coping strategies to keep themselves safe (Harding, 2010; Rosen, 2017; Sharkey, 2006). Data for this analysis come from elementary school students in the city of Baltimore. This is a good place to study the relationship between school violent crime and student mobility for three important reasons. First, BCPS has had a high student mobility rate for decades (Alexander et al., 1996). Student mobility rates at the elementary, middle, and high school levels were between 20% and 25% for the 2015–2016 school year with significant variation in student mobility across schools and neighborhoods in Baltimore City (Maryland State Department of Education [MSDE], 2017). These mobility rates are comparable to many other high-poverty urban districts (De La Torre & Gwynne, 2009; Fantuzzo et al., 2012; Metzger et al., 2018; Raudenbush et al., 2011; Schwartz et al., 2009; Welsh et al., 2016). Second, Baltimore has a high and variable violent crime rate. Not only do crime rates vary dramatically across neighborhoods but there is also substantial variability in those rates from year to year (see Morgan & Pally, 2016). This means that students experience enough exposure to violent crime at school to detect its relationship with mobility and to compare schools to themselves over time. High rates of violent crime are not unique to this city. In 2017, Baltimore ranked third nationwide in terms of its total violent crime rate per 100,000 residents, comparable to St. Louis, Detroit, Memphis, Kansas City, Milwaukee, and Cleveland (Federal Bureau of Investigation, n.d.). Finally, Baltimore has a high out-of-zone attendance rate. While all traditional public elementary schools have residentially based catchment areas, not all students who attend those schools live in those catchment areas. In the 2016–2017 school year, the district estimates that 44% of students did not attend their local public elementary school. Only a small fraction of this is due to charter enrollment: 34% of students attending traditional public schools live outside the catchment area (BCPS, 2018). This flexibility in enrollment means that elementary students in Baltimore can use residential moves to secure a place in their desired school, but they also have the opportunity to change schools without making costly and time-consuming residential moves. These out-of-zone enrollment patterns are not unique to Baltimore. Many other districts across the country experience high rates of nonresidential enrollment (Burdick-Will, 2017, 2018; Lutton, 2014; Mikulecky, 2013; Theodos et al., 2014;). Perhaps this is why nationally only around one half of school moves involve a residential move (Gasper et al., 2012). There are some downsides to basing the analysis in Baltimore. Given the racial and demographic composition of the public schools (80% Black, 89% FRM) it is difficult to detect variation in these effects by race or assess how wealthy, suburban families might respond to changes in school violent crime. This is common for this type of high-poverty urban district, and similar demographics are found in New Orleans, Washington DC, and Chicago (Chicago Public Schools, n.d.; District of Columba Public Schools, n.d.; Orleans Parish School Board, 2019). Moreover, even in districts with higher numbers of advantaged, White students, they are rarely exposed to high enough levels of violent crime for an adequate comparison across race (see Sampson et al., 2008, for a similar discussion of exposure to neighborhood concentrated disadvantage). In sum, Baltimore is not generalizable to the entire United States, but it is typical of a large number of high-poverty public school districts in both large cities and increasingly suburban areas. (For more on a classification of urban districts facing similar issues see Milner, 2012, and Welsh and Swain, 2020). Understanding the relationship between school violent crime and student mobility in Baltimore can therefore shed light on the sources of student instability and churning in other similar places. Data for this study come from deidentified BCPS administrative records from the 2010–2011 through 2015–2016 school years that are stored at the Baltimore Education Research Consortium. The records include the date of enrollment and withdrawal for each school attended by every student in the district. Each row of the enrollment records include gender, race/ethnicity, grade level, FRM status, special education status, and English language learner (ELL) status during that time frame. Our outcome of interest is whether a student changed schools during the summer after the completion of the school year. We use district discipline records to create two measures of student behavior problems that could confound the relationship between school violence and transfer: The number of unique suspensions and the total number of days suspended for each student each year. Students who are disruptive may both contribute to violent incidents at school and be asked to leave due to their behavior. Residential address changes suggest nonschool-related reasons for a school transfer. We, therefore, compare residential addresses recorded by the district at the end of each school year to create an indicator of students' residential mobility during the prior calendar year. Since prior school mobility is also an indicator of instability at home, we include an indicator for whether the student changed schools in the previous summer. We also create an indicator for whether the student lives in the same tract as their school. This provides a proxy for out-of-zone enrollment as well as a sense of how familiar the family might be with the area around the school. School-level data are collected from the National Center for Education Statistics (NCES) Common Core of Data (NCES, 2017) and the MSDE School Report Cards (MSDE, 2017). We use the geocoded school addresses reported in the NCES data to identify the city block in which each school is located. We use annual standardized test score proficiency rates reported by MSDE to create a rough measure of school quality. In 2014–2015, Maryland adopted the Common Core-aligned test (the Partnership for Assessment of Readiness for College and Careers) and pass rates on standardized tests were lower than in previous years. To account for this discrepancy, we measure school-level achievement by ranking schools by percentile within years according to their average test scores rather than comparing raw test scores. Relative differences in test performance across schools are therefore comparable to other years in the data set. School-level racial composition and proportion of special education students, ELL, and FRM recipients and mobility metrics, such as the number of new students in each year and the number of midyear exits, are calculated by aggregating the individual-level data to the school level. Crime data for this study come from incident reports of victim-based crimes published by the BPD on the Open Baltimore Data Portal for 2010 through 2016 (BPD, 2017). This data set includes the date, time, location code, and description of all officially reported incidents during this time period. Violent crimes include all assaults, robberies, rapes, shootings, and homicides. We create two measures of violent crime exposure for every student. First, we measure violent crime exposure at school. In order to identify crimes that likely took place at school, we include all crimes that occur on either side of all streets that define each school's city block. These crimes either took place on school grounds or they took place just outside school, on the street that runs along the school grounds. Either way, police presence would have been visible from the school itself. The idea here is not to explore whether the neighborhood surrounding the school is safe, but whether the immediate location of the school is safe. There is wide variation from block-to-block in violent crime rates (see Braga et al., 2010) and even in generally violent neighborhoods the immediate area of the school may be quite safe, or vice versa. We include only violent crimes that occur during the day (6:00 am to 7:00 pm) on weekdays between the first and last days of school. These are crimes that students and their families are most likely to be aware of and to which students are most likely to be exposed. This time period is long enough that we can reasonably assume there were some students in the vicinity of the school, even if just for a one-time event, but not so short that it removes all variation in students' exposure. Our second measure of violent crime captures students' exposure in their home neighborhood. Here, we count all violent crimes that take place in each students' residential census tract at any time of day and any day of the week during the full calendar year, from the first day of school to the start of the next school year. High levels of out-of-zone enrollment and the highly spatially concentrated nature of violent crime mean that students' exposure to violent crime at school and in their residential neighborhoods are not strongly correlated (r = 0.15). Given the skewed distribution of exposure to violent crime, both measures have been transformed using the inverse hyperbolic sine (IHS) function. This transformation is frequently used when modeling wealth and has the benefit of a similar interpretation as the log transformation, but can be used when values include zero (Burbidge et al., 1988). This means that the coefficients represent approximate percent change in exposure rather than an increase in a specific number of crimes. One limitation of the administrative data is that it does not include any direct measures of family background. Instead, we rely on student addresses to capture at least some differences in socioeconomic circumstances and adjust for other aspects of students' neighborhoods that may be associated with violent crime exposure at school. Specifically, we include tract-level measures of median household income and percentage of residents with a bachelor's degree or higher from the 2011–2015 American Community Survey. Including additional neighborhood measures, such as the poverty, unemployment, or welfare rates does not add anything to the models. For each focal year, we use the enrollment records for the prior year to create measures of prior residential and school mobility and records for the following year to create our outcome of summer mobility. Therefore, we limit our analysis to the 2011–2012 through 2014–2015 school years, and we use the first and last years of available data (2010–2011 and 2015–2016 school years) to calculate mobility indicators for before and after each analytic school year. Within that time frame, we limit our analysis to students enrolled in kindergarten through fifth grade. There is substantially more mobility in these lower grades, and, although there are generally lower levels of violent crime in the area around elementary schools than around high schools, there is substantial variation in student exposure. We exclude middle and high school students because the magnet programs and open enrollment choice process available in these higher grades makes changing schools more difficult. There are not always seats available at desirable schools, and it is often difficult to move to a popular school after the initial assignment has been made. There is substantially less curricular differentiation between elementary schools, and there are no selective enrollment or vocational schools at the elementary level that provide a unique experience and would therefore be likely to retain students regardless of local safety concerns. Students making structural moves because they have reached the highest grade available at their current school are excluded from the analysis. While it is relatively rare in grades K–5, some schools do close or have limited grade offerings. These moves are not voluntary and therefore do not represent an active decision to move on the part of the family (Welsh, 2017). Finally, we limit our analytic sample to observations in which students are continuously enrolled in a single school for the entire year in order to ensure that all students in a school were exposed to the same violent crime rate. This also allows us to focus on students with only one enrollment row per academic year. Midyear mobile students contribute to school-level measures, but their individual observations are not included in the analysis. The use of administrative enrollment records means that every student has a complete record and there are no missing values in our population of stably enrolled students. Moreover, since we count a student as leaving their school regardless of their destination, leaving the district does not generate any missing values. Students may change schools or neighborhoods every year. Therefore, the resulting data structure leads to annual observations that are simultaneously cross-classified by student, school, and neighborhood. The analytic time frame allows for up to four observations per student; however, some students leave the district and others are too young or old to be observed in all 4 years. In practice, there are only an average of 2.2 observations per student. Since our exposure of interest is at the school-level and we have relatively few observations per student, we treat the data as having two levels: annual observations nested within schools. Assessing the relationship between exposure to school violent crime and school mobility is difficult due to the selection of different types of students into different schools. Most important, exposure to school violence is correlated with other sources of student disadvantage that lead to school mobility for nonschool-related or reactive reasons. In order to adjust for as much of this selection as possible our models include student demographics, neighborhood characteristics, reports of student discipline problems, and indicators of prior residential and school mobility. Adjusting for residential mobility during the school year removes confounding by nonschool-related factors that influence housing, such as foreclosure, foster care, or divorce. Prior summer school transfers capture unmeasured sources of family instability that could also lead to a nonschool-related transfer. Individual suspension records allow us to adjust for involuntary school-related moves that are related to formal expulsion or informal counseling out. In addition to bias from individual selection, our models must also account for school-level characteristics that correlate with school violence but are an independent source of student instability. Therefore, we adjust for time-varying measures of school size, demographics, special program use, and midyear mobility. School fixed-effects adjusts for unobserved, constant differences between schools that might be related to both safety and mobility, such as proximity to transit and commercial areas (Cohen & Felson, 1979) or structural features of the school building that limit adult supervision (Sánchez-Jankowski, 2016). With these fixed effects in the model, the coefficient for the school violent crime represents the estimated relationship between mobility and year-to-year changes in school violent crime. In other words, they allow us to compare students in the same school but different calendar years to see if students are more likely to leave in years with higher violent crime rates. The school fixed effects also account for the nesting of observations within schools by calculating robust, clustered standard errors. The formal model is as follows:[MATH](1) where Ytijk is an indicator for whether or not student i living in neighborhood k made a nonpromotional exit from school j in the summer following school year t; Vtj is the IHS-transformed measure of violent crime at school j in year t; Xti are the individual-level characteristics of student i during school year t (including gender, race/ethnicity, special education status, ELL status, FRM, grade level, living in the school tract, number of suspensions, and total days suspended); Mti are additional mobility indicators for student i during year t, including whether the student changed addresses in the prior calendar year or changed schools last summer; Ntik are the characteristics of the student i's census tract k in during year t, including the IHS-transformed violent crime count, median household income, and percentage of residents with a bachelor's degree or higher; Stj are time-varying characteristics of school j in year t, including total enrollment, total number of student entries and exits during the school year, and percentages of students who are identified as Black, Hispanic, ELL, FRM, and special education eligible; dt are dummy variables for each school year; sj are fixed effects for each school; and εtijk are the observation-level error terms. All standard errors are robust and clustered at the school level. Table 2 reports the distribution of the types of crimes that occur at Baltimore City elementary schools during the 2010–2011 through 2014–2015 school years. In the average school year there are around seven reported assaults and one robbery, leading to a total of approximately eight violent crimes. However, these distributions are skewed with a few schools reporting more than 50 violent crimes in a single school year. Most schools do not have any of the most serious reported crime types, but there are a few schools that have up to three homicides and two rapes or shootings in one school year. Table 3 describes the analytic sample and compares it to students who are excluded from the analysis due to midyear mobility. Around 13% of all observations and 12% of stable enrollments result in a summer move. Midyear moves are slightly less common. Only around 10% of all observations are excluded due to a midyear move. As expected, midyear movers are generally more mobile by other measures as well. They are substantially more likely than stably enrolled students to have changed residences in the last year and to change schools during the previous or following summers. They are slightly more likely to be Black and are more disadvantaged than the rest of the population in terms of special education, ELL, FRM, behavior problems, tract demographics, and violent crime exposure in both their neighborhoods and schools, but these differences are relatively small. The number of students in the stable and midyear mobile groups does not add up to the total number of students because some students are mobile in some years and stable in others. Table 4 presents school characteristics in years with low, medium, and high levels of school violent crime. School years in the bottom third of violent crime (fewer than three reported crimes) are considered low, schools in the top third of average violent crime (more than eight reported crimes) are considered high, and all other schools fall in the medium category. Exposure to violent crime is somewhat associated with school size, with highest exposure school years enrolling approximately 48 students more than the lowest exposure schools on average. The biggest differences between high and low violent crime school years is reflected in their racial composition. More than 90% of the students in the high violent crime school years are Black compared to 75% in the low violent crime years. Only 5.5% and 2.7% of students in high violent crime school years are White or Hispanic, respectively. These numbers show that while not all students in predominantly Black schools in Baltimore are exposed to violent crime, those who are exposed are much more likely to be Black. Schools in the highest exposure category also serve disadvantaged students in terms of FRM and special education, although they serve fewer ELL students than schools in the other two exposure categories. Test scores are also somewhat lower in the highest exposure years. Higher violent crime school years also have somewhat higher turnover rates with larger numbers of new students (123 in the lowest violent crime years vs. 152 in the highest) and midyear movers (34 vs. 53). The last row of the table shows how many schools are represented in each of these groups. The numbers in each column do not sum to the total because many schools change categories from year to year. Figure 1 shows the geographic distribution of school violent crimes across Baltimore for the 2013–2014 academic year. Tract-level violent crime is shown in the background as a reference. The schools with no reported violent crimes are marked in gray. Larger black circles represent schools with larger numbers of reported violent crime. Two characteristics of the spatial distribution of school violence are worth highlighting. First, while the most violent schools are generally closer to more violent neighborhoods, there are quite a few reported school crimes in what otherwise appear to be relatively safe areas of the city and very safe schools in otherwise dangerous neighborhoods. For example, Eutaw-Marshburn Elementary School did not experience a single violent crime in the 2013–2014 school year, but 134 violent crimes were reported in the surrounding tract during the same period. Second, schools that are very close to one another geographically can have dramatically different violent crime rates. For example, Empowerment Academy is located across the street from Calverton Elementary. During the 2013–2014 school year, there were 17 violent crimes reported at Calverton, but none at Empowerment. This spatial variability is due to the highly concentrated nature of violent crime. Even in the most dangerous neighborhoods in Baltimore and elsewhere, most crimes take place on a relatively small number of specific block faces (Braga et al., 2010; Cohen & Felson, 1979; St. Jean, 2008). There is also quite a bit of temporal variability from year to year, especially in schools on the higher end of the violent crime distribution. Figure 2 illustrates the violent crime rate for 12 randomly selected schools over time. Violent crime rates in some of these schools vary dramatically from year to year, with the most violent school year in this figure peaking near 60 incidents but dropping below 20 in other years. Even in the lowest crime schools there is variability. In fact, only one elementary school in the city (a charter school in the northwest part of the city) reported no violent crimes in any year. Table 5 presents results for the logistic regressions of student mobility on school violent crime. Coefficients are reported in odds ratios. Model 1 includes only student-level covariates. Unsurprisingly, the largest predictor of school mobility is prior residential mobility. The odds of school mobility for students with an address change in the prior calendar year are 8.6 times higher than those who did not change addresses. The odds that students who changed schools during the previous summer move again are 24% greater than those who enrolled in the same school as last year. On average, Black students are more likely to change schools than non-Black and non-Hispanic students. Hispanic and ELL students are less likely to change schools. In this model, the number of unique suspensions does not predict school mobility, but each additional day suspended increases the odds of changing schools by 1%. With all of the student- and school-level adjustments, percentage of residents with a bachelor's degree in the student's tract does not predict school mobility, but one standard deviation increase in median household income predicts a 6% reduction in the odds of changing schools and living in the same neighborhood as the school reduces the odds of changing schools by 12%. When violent crime in students' neighborhoods doubles, the model predicts a 5% decrease in the odds of school transfer. The negative relationship between neighborhood violence and school mobility may come as a surprise since neighborhood violence tends to be positively associated with residential instability. However, since baseline tract violent crime rates are much higher than school violent crime rates (108 on average), it is much more difficult for these crime rates to double in a single year, and the magnitude of this effect is quite small. Moreover, since many students do not attend school near their home neighborhood, residential tract violent crime is unrelated to violence exposure or perceptions of safety at school. Instead, this coefficient captures the association between residential violence exposure and student mobility above and beyond the effects of social and economic neighborhood disadvantage, student poverty, residential instability, prior school mobility, and school enrollment that might lead to a reactive move. In other words, the direction of the relationship suggests that after adjusting for all of these factors, high rates of violence near home may impede families' available bandwidth to engage in a strategic, voluntary school transfer process for any reason (Schilbach et al., 2016). Alternatively, when families experience high rates of neighborhood violence they may seek out more stability in their school environments as a counterbalance to disadvantage at home (Crosby et al., 2019). With the IHS transformation, the coefficient for school violent crime represents predicted change in the odds of mobility when violent crime rates at a school double (100% change). Since school violent crime rates are relatively low, in many cases the rate could double with just a few additional crimes. In this first model, when adjusting for student characteristics, doubling the school violent crime rate increases the odds of a student leaving the school by 14%. Model 2 adds observed school characteristics. Students are more likely to leave schools with higher midyear mobility rates, lower test scores, and higher proportions of Black, ELL, and FRM students. Including these covariates reduces the magnitude of the school violence coefficient. Now a doubling of school violent crime predicts a 4% increase in the odds of school transfer. The next model (Model 3) adds school fixed effects to account for any unobserved, constant differences between schools that might be related to both mobility and violent crime. Interestingly, the coefficient for school violent crime remains essentially the same as the previous model: When school violent crime doubles, the school fixed-effects model again predicts a 4% increase in the odds of school mobility. Converting the odds ratios to predicted probabilities means that, holding all else constant, the predicted probability of a student with average characteristics transferring from a school with no violent crimes is 11.4%, with average violent crime exposure (eight violent crimes in a year) is 12.4%, and with exposure one standard deviation above the mean (17 violent crimes in a year) is 12.7%. This means that approximately 5.8 additional students are expected to leave an average-sized school (approximately 443 students) after a high violent crime year than a year with no reported violent crimes. There is no evidence of differential associations by grade, race, gender, ELL, special education status, prior mobility indicators, living near the school, or school proficiency levels (results not shown in tables). However, there is evidence that the relationship between violent crime exposure and school transfer is stronger for more advantaged students who are more likely to have the resources to make a strategic move (Hypothesis 2). Model 4 shows the interaction between exposure to violent crime near school and FRM status. Non-FRM eligible students are much more likely to leave their school following a year with relatively high violent crime. For non-FRM students, doubling school violent crime predicts an 11% increase in the odds of transfer, while for FRM students the same increase in school violent crime only predicts a 3% increase in the odds of transfer. The relationship between school violence and mobility also varies by exposure to neighborhood violence: Students from safer neighborhoods are more sensitive to exposure to violence at school (Model 5, Hypothesis 3). This is true whether or not we include the FRM interactions. In the most extreme case, students in neighborhoods with zero reported violent crime are expected to increase their odds of school transfer by 31% when school violent crime doubles. However, there are very few students who actually live in such safe neighborhoods. More realistically, for students who live in the safest decile of neighborhoods (with fewer than 27 violent crimes in a year), doubling school violent crime predicts an 11% increase in the odds of changing schools. In neighborhoods with average violent crime rates (approximately 100 violent crimes per year), doubling school violence only predicts a 5% increase in the odds of changing schools. For students in the most violent neighborhoods (approximately 250 crimes per year), it drops to a less than 2% increase in the odds of mobility. This suggests that students who are exposed to violence near their homes are desensitized to exposure at school. Despite the rigorous adjustments for potential confounding, it is still possible that our models are not capturing the true relationship between school violent crime exposure and summer mobility. Below we describe three different robustness checks used to examine potential bias in our measures and estimates. First, our measure of school violence relies on the fact that crimes that take place on school grounds need to be reported with addresses on the streets that surround the school campus. To test whether these crimes are just capturing the general area around the school and not what is going on inside the grounds, we also created measures of violent crime in the census tract in which the school is located and within a half-mile buffer of the school. The correlation between these measures and the crime rates at school is relatively low (0.27 and 0.16, respectively) and these measures do not significantly predict student mobility (results not shown). This suggests that families are responding to—and our school violent crime measure is capturing—something about the school itself rather than the larger area around the school. One potential explanation for this difference is that general safety in the area is likely already taken into account when selecting the school and, therefore, is less likely to influence subsequent student mobility decisions. Second, individual-level standardized test scores are only available for students in the third grade or higher. In order to include as many elementary school students as possible we do not include measures of individual achievement in the main models. However, analysis using only students in Grades 3 to 5 and including test scores shows that test scores are not predictive of mobility after adjusting for the other covariates (β = 1.03, SE = 0.02) and the coefficients for exposure to violence are essentially the same with or without the controls for achievement (β = 1.06, SE = 0.03). This suggests that our results are not biased by excluding achievement scores in the main models. Finally, despite the rigorous school- and student-level adjustments, it is possible that unobserved differences between students account for some of the association between school violent crime and student mobility. One way to assess whether there are still unobserved differences between schools and students that drive the association between school violent crime and mobility is with placebo tests. First, we use a measure of violent crime that took place at the school in the year after observed student mobility. If there is something about the school or student that is generating bias in our estimate, we might expect that this "future crime" measure would also be as strongly related to the likelihood of school mobility as our main measure. On the other hand, if timing matters and this "future crime" measure has a weaker relationship with prior mobility, it is more evidence that our estimates are not driven entirely by bias. The results of this placebo test are clear: Measures of school violent crime in the following year are half the size of the main coefficients and not precisely related to student mobility (β = 1.02, SE = 0.02). Second, we exploit the timing of the school day to test whether violent crimes that take place at night, when students are not on campus, have the same relationship with mobility. If they do, it is a sign that there is unmeasured confounding. Again, the results are clear: Nighttime and weekend violent crime have a much weaker and more imprecise relationship with student mobility (β = 1.02, SE = 0.02). One might wonder what kind of schools students exposed to school violence transfer to and whether their school change leads to an improved social or academic environment. To assess whether leaving schools with high violence levels influences the quality of the receiving school we use our main models, including the sending school fixed effects to predict the proficiency rates and school violent crime rates of the schools that movers attend. The results indicate that there is no significant relationship between the level of violent crime at a school and the characteristics of destination schools (full results in Supplemental Table S1 in the online version of the journal). This suggests that students who move for safety reasons are more concerned with leaving a particular environment and are not able to harness that move to improve objective measures of school quality. Student mobility and exposure to violence are both well documented problems in many urban school districts. In this study, we show that these two phenomena should not be considered in isolation and are in fact related. Specifically, we show that in academic years with higher levels of reported violent crime at school, students are more likely to transfer from that school at the end of the year, even after adjusting for a large number of student, school, and neighborhood characteristics. For the average student, when school violent crime doubles, we predict an approximately 4% increase in the odds of school transfer. Moreover, these predicted associations are substantially larger for students who do not qualify for FRM and for those from safer neighborhoods. For these students, doubling violent crime predicts an 11% increase in the odds of school mobility. This suggests that it is relatively more advantaged students who are most sensitive and able to respond to changes in violence at their school. Therefore, school violent crime may not only influence individual students' transfer rates but may also shape the composition of the student body by pushing out some of the most advantaged students. Alternatively, exiting students may be replaced by new students with similar demographics who are leaving their own schools for any number of reasons. Longer time frames with more statistical power than are available in this study are needed to test this school-level hypothesis directly. Doubling a violent crime rate may seem like a high benchmark for the marginal effect, but since school violent crime rates are generally low, this is actually quite common. Remember that it takes just one additional crime to double the rate from one to two. Of the 352 school years where it is possible to calculate the change from 1 year to the next, around 23% of school years experienced a change from one year to the next of at least 100%, and many reported even larger changes. Given the baseline mobility rates, student populations, and the estimated increase in mobility due to school violence, the average school in the district can expect an additional 1.5 students to transfer per year due to safety concerns, with some excess mobility as high as nine students in some schools in a single year. At the district level, 665 students are estimated to have changed schools during this period due to Baltimore's high levels of school violence. Baltimore's violent crime rates have only increased since the years analyzed in this study (Morgan & Miller, 2020), suggesting that even more students could be changing schools due to safety concerns today. Any loss of students can have serious financial and existential problems for a school. In an era where losing the competition for students can also lead to closure, principals are in a constant battle to attract and retain students (McWilliams, 2019). With per-pupil spending at approximately $16,000 (U.S. Census Bureau, 2018), the loss of just a few students can lead to reduced staff and program cuts (Cossyleon & Schock, 2019). Even if new students end up enrolling in the fall, the uncertainty and effort that goes into recruitment can cause organizational problems (Jabbar, 2015). Turnover in the student population can also undermine a school's effort to create a trusting, positive climate for students (Bryk et al., 2010). It is also important to remember that these models include a large number of covariates, including prior school and residential instability and students' disciplinary records. Controlling for so many possible reasons for student mobility makes it notable that the influence of school violent crime can be detected at all. Most important, this study controls for prior school and residential mobility, which are proxies for general family instability and known to be strong predictors of school transfer. While the effect size for school violence exposure does not compare to these individual predictors, it is comparable to the effect of standardized test score proficiency rates in our models. One standard deviation change in these pass rates predicts approximately a 6% reduction in the odds of school transfer. This suggests that students are about as sensitive to changes in safety as they are in changes in academics. Furthermore, this study likely underestimates the total effect of school violent crime on mobility. First, we only measure parents' sense of safety at school indirectly, through reported violent crime. Many fights and disruptive events do not get reported to the police and therefore do not show up in these data. The relationship between objective measures of violence and perceived safety varies across students in the same school (Burdick-Will, 2013; Lacoe, 2020; Steinberg et al., 2011). What we estimate is likely an effect that is averaged across families who have intimate knowledge of what happened during those events and those who may know little about them. This means that our estimated effects are only capturing the most extreme tip of the iceberg. Were we able to pinpoint more direct exposure to or knowledge of violence at school, we would likely find even larger effects for a subset of students (Carson et al., 2013). Second, in order to simplify the analysis and pinpoint the timing of violence exposure and transfer we focus only on summer transfers. Assuming that some proportion of midyear movers are also influenced by safety concerns (in fact, safety is one of the only reasons a student can request a nonresidential midyear transfer [BCPS, 2020]), the total effect of school violence on student mobility and churn is likely to be much larger. Finally, violent crime in and around schools is not only known to students but also to administrators. Research shows that with the right social and emotional supports, it is possible for a school to make students feel safe regardless of what is going on outside the building (Bryk et al., 2010; Crosby et al., 2019). Doing so is not only likely to lead to lower levels of violence from within the building but also make families more inclined to stay, since research shows that student transfer rates can be reduced when schools reach out to families to build strong social relationships (Fiel et al., 2013). It is possible that principals are already working to reduce the effect of safety concerns at schools by engaging in strategic retention efforts. Given these realities, it is important that researchers, schools, and districts take even these small estimated effect sizes seriously. The relationship between exposure to violence and student mobility has important implications for educational theory and policy. First, these findings expand our understanding of the sources of student mobility. The existing literature tends to focus on the relationship between student mobility and personal hardship, and in particular, how residential instability leads to school instability (Comey et al., 2012; Cordes et al., 2019; Hanushek et al., 2004). This focus on reactive, involuntary moves implies that student instability is a reflection of students' home lives and therefore not controlled or influenced by schools or districts. In contrast, we argue that students and their families may be behaving with more agency than this literature gives them credit for, and in response to factors that are in fact school related. In other words, by demonstrating that changes in violent crime near school help predict student mobility, we provide evidence that student mobility is about more than just students' own social, economic, or residential instability. Importantly, this means that schools districts could take action to promote a safe learning environment and reduce at least some of that instability. Second, the results provide even more evidence of the collateral damage of violent crime on urban areas (see Harding, 2010; Sharkey, 2010) and remind us to look beyond individual stress and trauma as a causal mechanism. Strategic school transfer is a form of avoidance for protective purposes. Similar to teachers who leave unsafe schools (Boyd, 2011) and students who stay home rather than brave a dangerous commute (Burdick-Will et al., 2019), families likely seek to protect themselves by leaving schools that they no longer feel are safe. This means that the consequences of an unsafe environment ripple across cities and districts in ways that are often underappreciated when we focus only on direct victims or witnesses. Changing schools is stressful under the best of circumstances (Grigg, 2012). When transfers are motivated by the stress of exposure to violence, they are likely to be even more difficult for students. However, the overall academic effect of these safety-driven moves for individual students is difficult to determine and likely depends on where the movers end up. On average there is no relationship, positive or negative, between leaving a school with high levels of violence and the characteristics of the receiving school. This fits with the existing literature that shows that for most mobile students receiving schools are very similar to sending schools (Welsh, 2017). The lack of improvement may be a sign that the structural constraints that lead students into one type of school are hard to overcome when a student changes schools (Burdick-Will et al., 2020). Nevertheless, the average null effect likely hides substantial variability for individual students. As with any strategic transfer, if families use the move to improve their child's social and academic environment, these moves may result in long-term academic benefits (Hanushek et al., 2004). On the other hand, if a student's enrollment is disrupted without a gain in school quality, the move may be detrimental. Unfortunately, given the year-to-year fluctuations in school violent crime rates, some students may leave one school because they experienced violence for another that they think is safer, only to find that in the next year their new school experiences more violence than they expected. In this case, students might be inclined to move again in the following year, leading to even more instability in their academic trajectories. Further research is needed to unpack the consequences of safety-related school transfers and the tradeoffs that families face when changing schools due to safety concerns. Understanding the complex and multifaceted relationship between exposure to violence and academic outcomes is especially important because it is an often-hidden disadvantage for Black students. Research repeatedly finds that Black students are more likely to change schools than White students with similar background characteristics (Welsh, 2017). Our results show that even more advantaged, non-FRM Baltimore public school students of color face substantially higher levels of violent crime exposure than their White peers. These findings suggest that this exposure is a source of instability in these students' lives, leading to school transfers that may not have happened in safer environments. Perhaps just as importantly, the interaction with FRM-eligibility suggests that students with more resources are better able to move in response to school violent crime. This means that large numbers of students without those resources are likely to feel unsafe and want to change schools, but are unable to do so. These students are also at a substantial disadvantage given the negative and cumulative effect of exposure to violence on concentration and learning (Burdick-Will, 2013, 2016; Sharkey et al., 2012). Finally, the results of this study suggest that to reduce student turnover rates, policymakers in urban districts must think beyond their own walls and consider the larger urban context. Since violent crime reported just outside of the school building can significantly affect families' decisions about school enrollment, districts may be able to reduce student turnover by expanding their definition of school safety. Although many districts have begun to place security guards inside school buildings, schools often ignore the areas just outside or across the street from their buildings (Sánchez-Jankowski, 2016; Steinberg et al., 2011). More could be done to protect students and provide a safe zone in these border areas. Our results show that this does not mean that a school needs to make the whole neighborhood around the school safe. Instead, focusing on reducing exposure to violent crime on the streets surrounding the school can potentially increase stability in enrollment patterns at the district level. Instability and churning in urban school systems is a serious problem. Shifting student populations pose an instructional and organizational problem for schools. Not only do individual students tend to do poorly after a school transfer but schools and classrooms with high mobility rates are harder to teach (Raudenbush et al., 2011; South et al., 2007; Whitesell et al., 2016). Stability in enrollment and staffing is also necessary for building the trusting and supportive environment necessary for learning (Bryk et al., 2010; Fiel et al., 2013; Grigg, 2012). Understanding the sources of instability are key to developing policies to fight it. This study shows that high levels of churning are not just a reflection of student disadvantage but also of structural and social problems in our cities as a whole. Therefore, devoting resources to improving safety in our cities should be considered not just a good in and of itself but also a necessary step to improving educational outcomes in large urban districts. Julia Burdick-Will https://orcid.org/0000-0002-3670-4179 Kiara Millay Nerenberg https://orcid.org/0000-0002-2282-4647 Jeffrey A. Grigg https://orcid.org/0000-0003-1975-3232
10.3102_0002831220968947	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220968947	Are Donations Bigger in Texas? Analyzing the Impact of a Policy to Match Donations to Texas' Emerging Research Universities	 We examine the impact of the Texas Research Incentive Program (TRIP), a state policy that offers matching funds to incentivize private-sector donations to certain public universities. We use a national dataset and employ a generalized difference-in-differences approach with matching procedures to estimate the treatment effect of TRIP on revenues at eligible institutions. Results show that TRIP is associated with increases in revenue from private gifts and state grants/contracts, which suggests that policymakers can leverage public investment to incentivize private donations. We do not detect a statistically significant relationship between TRIP and endowments, so donations are likely used for short-term funding and do not create long-term dividends. We consider potential social consequences of selecting certain universities to benefit from incentive policies.	 American universities are highly stratified. In 2017, 75% of higher education endowment assets belonged to 12% of universities (Sherlock et al., 2018). Out of more than 4,500 degree-granting colleges and universities in the United States, 30 universities accounted for 42% of university-based research and development (R&D) expenditures, which totaled more than $75 billion (Gibbons, 2018; National Center for Education Statistics, 2018). Distributional inequalities are validated through status hierarchies. In other words, U.S. universities that are wealthier and that spend more on R&D tend to be highly regarded by global rankings systems, such as the Academic Rankings of World Universities (Cantwell & Taylor, 2013). Research suggests that rather than trying to reduce or minimize funding inequalities and stratification in higher education systems, policymakers around the globe are implementing initiatives to further stratify their higher education systems. Almost two dozen countries have implemented policies that select a subset of universities and award them additional funding—all with the hope that financial incentives will lead those universities to produce more research and gain reputational status in the country, throughout the region, and around the globe (Fu, 2017; Fu et al., 2020). On one hand, observers might say that already privileged universities are selected for further advantage, while the rest of the higher education system is left behind. On the other hand, some policymakers might suggest that in a globally competitive knowledge economy it makes sense to invest in a small cadre of promising and aspiring universities to achieve national goals for research and economic competitiveness. In this study, we examine the impact of a state policy that supports cumulative advantage among a group of state universities. In 2009, Texas adopted the Texas Research Incentive Program (TRIP) to leverage public funding and encourage private-sector (industry) donations to certain public universities. Similar to research policies adopted around the globe, TRIP aimed to help the state's "emerging research universities" achieve national prominence. Based on TRIP, eight public universities were eligible to receive matching funds from the state for gifts or endowment donations received from private sources, while the state's less resourced and less selective schools were ineligible for matching grants. The matching-fund formula used a sliding scale, so larger gifts were eligible for greater percentages of matching monies. For example, gifts between $100,000 and $1 million were eligible for 50% matches, and gifts above $2 million were eligible for 100% or dollar-for-dollar matches (House Bill [HB] 51: Texas Higher Education Coordinating Board [THECB], 2009; University of Texas System, 2018). Our analyses expand on prior research by examining a government policy that linked public and private financing of higher education. We draw on research on donor motivations and apply a principal-agent framework to conceptualize campus responses to the incentive policy. Then we use quasi-experimental methods—a generalized difference-in-differences (GDiD) analytic strategy, paired with coarsened exact matching procedures—to analyze the effects of TRIP on institutional revenue. Our guiding research question is: Did a state policy that incentivized private-sector donations by offering matching public funding positively influence university revenues among emerging research universities? As we address our research question, we test whether the policy achieved its intended goal while considering the broader social consequences of awarding additional funds to emerging research universities. Public universities in Texas are highly stratified. The University of Texas at Austin and Texas A&M University at College Station sit atop the hierarchy of public universities in Texas. These two campuses are more prestigious than the campuses of the state's other public university systems (i.e., University of Houston System, University of North Texas System, Texas State University System, and Texas Tech System). The University of Texas and Texas A&M were founded and endowed in the 19th century with large tracts of public land. The state invested revenue from those lands (leases, grazing fees, mineral rights) into what was called the Permanent University Fund (PUF). The state required that interest from PUF be moved into a separate Annual University Fund, which distributes PUF money. In 1923, prospectors discovered oil on university lands, and throughout the 20th century, the value of PUF dramatically increased. The Texas Constitution required that PUF funds can be disbursed only to The University of Texas and Texas A&M System campuses. The University of Texas System was allocated two thirds of PUF revenues in perpetuity, and the Texas A&M System was allocated the remaining third (Richardson, 2005). As members of the selective Association of American Universities (n.d.), The University of Texas and Texas A&M University Systems have continued to benefit from their early monopoly on PUF revenues. In the 2015 fiscal year (FY), the University of Texas System had the third largest endowment in the country—and the largest for public universities—valued at $25.4 billion. Additionally, The University of Texas at Austin (as a single campus independent from the system) had the 26th largest endowment in the country ($3.4 billion). Texas A&M University at College Station had the ninth largest endowment or the third largest endowment for a public university, which was valued at $10.5 billion (National Center for Education Statistics, 2017). Just like the discovery of oil made enduring fortunes for independent prospectors (Burrough, 2009), oil financed the stratification of Texas higher education. As an alternative to PUF, in 2009 the 81st Texas legislature adopted HB 51 and created revenue streams to support public emerging research universities in "achieving national research university criteria" (THECB, 2009, p. 4). When the legislature adopted HB 51, seven state universities were designated as "emerging research institutions," including four campuses in the University of Texas System (Arlington, Dallas, El Paso, and San Antonio), along with University of Houston, University of North Texas, and Texas Tech University. THECB granted Texas State University "emerging research" status in 2012. Among the 117 public colleges and universities in Texas (not including University of Texas at Austin and Texas A&M at College Station), the eight universities named above were granted "emerging research" status based on a set of metrics, such as endowment value, number of doctoral degrees awarded, faculty quality, and quality graduate programs (THECB, 2018a). In other words, before universities were designated as "emerging research institutions" by the state, they had to have already taken a gamble on their ambitions to move up the stratified higher education hierarchy by investing in costly endeavors like graduate education and faculty hiring. In fact, across the country "striving" research universities have invested their resources to improve infrastructure and expand administrative support to hopefully draw external research funding and institutional prestige (Morphew & Baker, 2004; O'Meara, 2007). The alternate funding mechanisms that support emerging research institutions are the National Research University Fund (NRUF), the Research University Development Fund (RUDF), and TRIP—the last of which is the focus of this study. All three policies were meant to increase research productivity, but they were designed to do so in different ways. NRUF is a state-run endowment (similar to, but smaller than, PUF), which distributes 3.5% of its earnings to emerging research universities that have more than $45 million in restricted research expenditures (THECB, n.d.). The purpose of RUDF was to award additional funding for recruiting and retaining "highly qualified faculty" who can increase institutions' research productivity (THECB, 2010, p. 1), but it was never funded or distributed to the universities (R. Cornelius, personal communication, February 14, 2020). Unlike NRUF and RUDF, TRIP is designed to "provide matching funds to assist emerging research institutions in leveraging private gifts" (THECB, 2018b, p. 3). The state determined that gifts or endowments that are eligible for TRIP matching funds include cash, endowments, securities, property, commodities (e.g., minerals), and life insurance proceeds. Universities were directed to submit information about gifts and to apply for matching grants from THECB. The enabling legislation stipulated that when all TRIP (state) funds are expended, eligible gifts that are not matched remain eligible for potential TRIP matching grants in future years if funds are available (Texas Administrative Code, Section 15.10). Anecdotally, universities have successfully used TRIP to solicit large grants from the private sector. For example, The University of Texas at Dallas received $3 million from Texas Instruments, which was doubled by TRIP, to create the Texas Biomedical Device Center. Similarly, the Valero Energy Foundation invested $2.5 million with University of Texas at San Antonio (doubled by TRIP) to pay for fellowships, assistantships, and conference travel for graduate students in Business and Engineering (University of Texas System, 2018). Between 2010 and 2019, approximately $326 million was distributed through TRIP (THECB, 2018b). Yet TRIP has not kept pace with university applications for matching grants. As of December 18, 2019, TRIP-eligible universities had applied for more than $210 million in unfunded matches. Some applications were found not eligible for TRIP funding, and others were still pending review. Yet more than $157 million of private-sector donations were approved as TRIP-eligible and were ready for payment, but they were not funded based on limited state allocations to TRIP. Despite the backlog, the state only appropriated $17.5 million to TRIP for FY 2020 and FY 2021. In fact, the entire FY 2020 allocation went to matching approved private-sector donations that were deposited before October 2015 on a first-come, first-served basis (THECB, 2019). The University of Texas System (2018) has asked the state legislature to use general revenue dollars from the state budget to clear the backlog and adequately fund TRIP to match future private-sector gifts to eligible universities. In this study, we focused on examining TRIP's influence on three measures of university revenues: private donations, endowments, and state contracts/grants (i.e., matching funds awarded by TRIP). Based on our research question, we measured outcomes in terms of revenue and not research production or faculty hiring. Focusing on donations and grants also has a methodological benefit because it does not confound the effects of TRIP with NRUF and RUDF. China was among the first countries to implement a research incentive policy in 1996. Since then, at least 22 additional countries have implemented research incentive policies. These policies have been implemented in various forms in Europe, Africa, Asia, and the Americas (Fu, 2017). In this section, we review rigorous empirical analyses of research incentive policies where, like TRIP, a government selected a subset of its public universities and awarded them additional funding to try to increase their research profiles and institutional prestige. Using time-series data, scholars have shown that university outcomes changed after governments adopted and implemented research incentive policies. For example, in the 1990s, Chinese policymakers adopted the 211 Project with the goal of cultivating "100 elite higher education institutions" (L. Zhang et al., 2017, p. 154). Ultimately 116 universities received support from the 211 Project. From the 211 Project universities, a subset of 39 universities were selected for additional grants for the purpose of "constructing world-class universities" through what was called the 985 Project (L. Zhang et al., 2017, p. 169). The 985 Project universities were chosen because they exhibited strength or monopoly in specific academic areas. L. Zhang et al. (2016) analyzed research production by Chinese universities, including universities that "receive preferential treatment in public funding allocations" (p. 885) through the 211 and 985 programs, and found that 985 Project institutions became more efficient at producing scholarship in science and engineering fields over time. In a second, similar study focused on a subset of China's 985 universities, the authors found the lower tier or less prestigious universities saw the greatest increases in research production in foreign journals. The authors noted that "massive investment in university research appears to have benefited second- and third-tier universities more than those in the first tier" (H. Zhang et al., 2013, p. 774). Analyses of a research incentive policy in South Korea also showed that when universities received targeted governmental support, they improved institutional outcomes. Seong et al. (2008) examined the first phase of the Brain Korea 21 (BK21) project, which invested around 1.4 billion U.S. dollars "to nurture globally competitive research universities" (p. 1). Seong et al. found that there was a positive effect between grants and research output for BK21 institutions at the bottom funding quartile. Similar to H. Zhang et al. (2013), the evaluation of the BK21 project found that "BK21 has the strongest positive impact on groups that received the least funding" (p. 94). Unlike the studies for China and South Korea, an evaluation of a research incentive program in Taiwan found less evidence that the government could increase university research activity. Fu et al. (2020) used a GDiD analytic framework to analyze Taiwan's World Class University Project. While Taiwan's recipient universities increased their number of science, technology, engineering, and math (STEM) publications and the share of those articles that were published in prestigious journals, those increases did not outpace advances made by universities that did not receive additional funding by the World Class University Project. Fu et al. complemented their statistical findings with qualitative findings, which suggested a diffusion effect. In other words, administrators at untreated universities increased their commitment to research and adopted their own initiatives using institutional resources to encourage faculty to increase research production. For example, Taiwanese institutional administrators explained that they offered monetary bonuses to research productive faculty; the administrators hoped that by demonstrating an ability to increase research production, their universities might be included in future rounds of research incentive funding (Fu et al., 2020). Surprisingly, limited research has been conducted on the relationship between U.S. state government funding and research productivity. Husted and Kenney (2018) noted that a decrease in state appropriations is associated with reductions in the number and value of National Institutes of Health grants obtained by public universities and the number of publications in economics. A working paper of the Federal Reserve Bank of Boston found that state appropriation cuts have a negative impact on the number of approved patent applications and research expenditures for public research universities (Zhao, 2019). The more robust literature on performance-based funding in states also has little to say about research productivity. Among the 13 PBF states with research metrics for at least some public 4-year institutions, the relevant metrics in PBF models focus on increasing research expenditures rather than increasing revenues from private donations for research (Boelscher & Snyder, 2019). We are not aware of research evaluating the effects of research-related metrics in PBF models. In summary, this study fills an important gap in the literature. Whereas international studies of research incentives have examined direct grants from governments to universities, we focus on a policy that disbursed funds only if TRIP-eligible universities first secured private-sector donations. This study addresses a void in the literature in the U.S. context by examining whether governments can effectively use a different mechanism for awarding research incentive funding, one that incentivizes institutions to secure private donations. Revenues for emerging research universities in Texas could have changed following the implementation of TRIP either because the policy influenced behavior at the institution (e.g., greater resources devoted to development) or because TRIP signaled to potential donors that their gifts were good investments, since they would be matched by the state. These two potential explanations are guided by principal-agent framework and literature on donor motivations, respectively. We discuss these components of the conceptual framework in turn. We anchor this study in a principal-agent framework, which seeks to explain how agents (in this case, eligible universities in Texas) might respond to incentives from principals (e.g., the state). The principal-agent framework posits that principals rely on agents to execute activities that the principal is ill-equipped to perform, due to factors such as a dearth of knowledge, capacity, or time (Eisenhardt, 1989; Moe, 1984). Government officials and actors in the public sector constitute one classic principal-agent relationship; legislators rely on public employees (bureaucrats) to implement legislation on behalf of elected officials, who cannot carry out the work themselves. For example, state policymakers turn to their public universities to advance scientific discovery. Conflicts in principal-agent relationships can result from information asymmetry or from competing goals between principals and agents (Moe, 1984). With respect to information asymmetry, the agents (e.g., university officials) know more than the principals (e.g., state officials), which hampers the principals' ability to monitor the agents' work. The problem of conflicting goals can also occur in higher education, including when public universities are driven by incentives that differ from state officials' goals (Kivistö, 2008). University officials could be driven by revenue incentives, as a resource dependency framework might predict (Pfeffer & Salancik, 1978), as well as by a quest for prestige maximization (Santos, 2007). To address perceived principal-agent problems, such as misalignment between state preferences and those of higher education institutions, state policymakers craft incentives to encourage university officials to pursue state policymakers' goals. This approach is evidenced by the proliferation of PBF models, by which states tie institutions' funding to their performance on certain measures (e.g., Gándara & Rutherford, 2018; Hillman et al., 2015; Hu & Villarreal, 2019; Umbricht et al., 2017). TRIP is one example of an incentive program designed to address a potential principal-agent problem: by dangling a carrot (i.e., the possibility of additional matching funds from the state), the state seeks to encourage eligible universities to raise private funds for research. This study examines, in part, the degree to which (1) eligible universities respond to the TRIP incentive by seeking greater private donations and (2) the state awards the promised financial incentive in exchange for institutional performance. However, TRIP is different from other research incentive policies in that it rewards universities only if administrators can recruit donors to support research efforts. Donors are indeed driven by solicitation (Bekkers & Wiepking, 2011). If institutions exert greater effort in requesting external research funds in response to TRIP, this could lead to higher revenue from gifts (and from state matching funds). Yet, in addition to solicitation, there are numerous reasons why people donate (Bekkers & Wiepking, 2011). Thus, we complement our conceptual framework with insights from the literature on donor motivations to inform this study. Beyond university officials, the second set of actors that policymakers aimed to incentivize via TRIP are potential donors, particularly from the private sector (HB 51, 2009). Donors are motivated by a number of factors, including social pressure (Bekkers & Wiepking, 2011). Most relevant for our study, previous research suggested that donors may be driven by perceptions of "quality" or worthiness of the organization to which they are giving (Connolly, 1997). This motivation is one reason for "crowding in," whereby increases in revenue from one source yield increases in revenue from another source (Connolly, 1997; Gottfried, 2008). Numerous studies in higher education support this contention, which stands in contrast to earlier studies in economics and public finance that suggested that government grants to nonprofit organizations crowded out private donations to those organizations (e.g., Kingma, 1989). For example, Cheslock and Gianneschi (2008) analyzed 11 years of panel data observations of 4-year public colleges and universities and found a positive relationship between state appropriations and gifts received from donors. Otherwise stated, reductions in state appropriations were related to decreases in gifts. Based on their review of the literature coupled with their own findings, the authors argued that donors to higher education "prefer institutions of high quality . . . and state funding helps to provide the basic infrastructure that helps to determine quality" (p. 212). Similar to Cheslock and Gianneschi's study, TRIP's underlying rationale of selecting emerging research universities that are most likely to achieve national or global recognition may signal to donors that those institutions are of high quality and worthy of donations. Similarly, studies of government support for university research buttress the idea that donors derive information from observing which institutions and organizations are selected as grantees. For instance, federal research grant revenues were positively associated with revenue from other research grants (Blume-Kohout et al., 2009, 2014; Connolly, 1997), albeit to varying degrees based on institution type (Blume-Kohout et al., 2009; Payne, 2001). As Payne (2001) explained, when certain universities received funding, this indicated that the government had vetted the research universities and that "the government through its grant awards may provide a signal of quality of research or other information to donors that is less noisy than that available to private donors" (p. 731). This phenomenon is consistent with the "Matthew effect" (Merton, 1968, 1988). High recognition leads to even more recognition, leading to a phenomenon of the "rich get richer" (Van Looy et al., 2004). Similar phenomena exist in the private sector. For instance, Kleer (2010) tested a signaling model of information asymmetries regarding the quality of proposed projects. Specifically, firms could propose projects for funding from banks and a government agency that awarded grants. Kleer found that government subsidies could be useful to private investors when subsidies were paired with quality signals. Like Payne (2001), Kleer found that actors in the private sector were willing to make investments because they found that government signals implied a certain kind of due diligence to review data and assess quality before investing public resources. Alongside Cheslock and Gianneschi's (2008) study, the Payne (2001) and Kleer (2010) studies suggested that donors were not discouraged from contributing when governments provided funding to a university or organization. Instead, donors may be more motivated to give to an emerging research university that has the government's imprimatur. Previous research on the relationship between government support and private donations, coupled with the principal-agent framework, informed our analysis. Given data constraints, we could not parse the degree to which any funding increases were due to greater fundraising efforts (e.g., greater investment in development staff) or to greater donor generosity. We could only observe whether revenues changed following TRIP implementation. However, taken together, the principal-agent framework and scholarship on donor motivations suggested that revenue from gifts should increase under TRIP, both from increased university efforts to fundraise and from a heightened motivation among potential donors to give, in light of the matching incentive. The panel data for this study were drawn from multiple sources between 2006 and 2017, with institutional-level data from the Integrated Postsecondary Education Data System (IPEDS), and state-level data from the Census Bureau, the National Conference of State Legislatures, and the THECB. We chose 2006 as the start year to avoid the confounding effect of the Research Development Fund (RDF), which was first funded to support public universities to promote increased research capacity in FY 2006 (THECB, 2018b). We chose 2017 as the end year based on availability of IPEDS data. Restricting the study period between 2006 and 2017 keeps RDF (which was later replaced by the Core Research Support Fund and the Texas Comprehensive Research Fund in 2016) as a constant factor contributing to state research funds for eligible research universities. We followed the recommendations by Jaquette and Parra (2014) to collapse institution-level data to the parent level for institutions that reported IPEDS finance data in a parent-child relationship. Based on the amount of private gifts and endowments for research activities received by each eligible institution, the state may award TRIP matching funds in the form of state contracts/grants (E. Buchanan, personal communication, February 5, 2019). In other words, the financial incentives from the state are not guaranteed, and it is not clear whether eligible institutions have received additional funds even upon receiving increased research donations. Thus, we included three dependent variables in this study: the amount of private gifts, the value of endowment assets, and the amount of state contracts/grants received by each institution. All dependent variables were adjusted for inflation and transformed using a natural logarithm to model a linear relationship. A log transformation on dollar value corrects for the dependent variable's skewed distribution and provides clear interpretation of the effect of a one-unit change in an independent variable on proportional changes in the dependent variable (Mincer, 1974). The treatment variable is a binary indicator of TRIP eligibility at the institutional level, and it was coded as 1 for institutions that are eligible for TRIP matching funds and 0 otherwise. As indicated in Supplemental Appendix A (available in the online version of the journal), the treated group includes eight institutions that were eligible for TRIP funds. As TRIP funds were first provided to eligible institutions in FY 2010, post is a binary variable that indicates the time of TRIP eligibility for individual institutions. For universities that were eligible for TRIP since its initial adoption in 2009–2010, this variable was coded as 1 for observations during and after 2010, and it was coded as 0 before 2010. For Texas State University, which was granted emerging research status and became eligible for TRIP in 2012–2013, post was coded as 1 for observations during and after 2013, and 0 otherwise. Based on prior literature (e.g., Bekkers & Wiepking, 2011; Hunt et al., 2019), we controlled for both state- and institution-level variables that could affect the levels of private gifts, endowment values, and state contracts/grants. At the state level, confounding factors included political variables (e.g., legislative control, governor political party) and economic variables (e.g., Gini index, unemployment rate). Institution-level variables included institutional characteristics (e.g., enrollment, Carnegie classification, affiliated hospital), revenues (e.g., public appropriations, revenue from tuition and fees per full-time equivalent [FTE] student), and institutional selectivity (e.g., ACT/SAT score, undergraduate graduation rate). Supplemental Appendix B (available in the online version of the journal) lists all variables used in our analyses. To estimate the average treatment effect of TRIP adoption on eligible universities' revenues, we used a GDiD model. GDiD, which allows the treatment period to vary by the individual group, has been applied in a host of higher education studies (Belasco et al., 2015; Daun-Barnett & St. John, 2012; Kramer et al., 2018; Rosinger et al., 2019). This method is appropriate for the study since seven institutions became eligible for TRIP in 2009–2010, and one university became eligible in 2012–2013. Allowing flexible treatment durations, GDiD ensures that the analysis can account for varying treatment phases (Angrist & Pischke, 2009). Specifically, the GDiD estimator (δ1) was used to compare the difference in the outcomes between treated and untreated units after TRIP adoption and then subtract the difference in outcomes before TRIP adoption.[MATH] Formally, we used ordinary least squares regression in the model, holding covariates constant:[MATH] where yit represents the dependent variable at institution i in year t. β0 is an institutional-specific intercept. Treatment is an indicator variable for TRIP adoption during any year in the panel, and post is an indicator variable that indicates the time of TRIP eligibility. δ1 is the coefficient of interest, ci represents the time-invariant institution-level fixed effect, and ht represents the year fixed effect. By incorporating institution and year fixed effects, the model controlled for potential state-specific effects over time—both within the institution as well as any time effects that were common across institutions in each year (Allison, 2009). Zit represents the matrix of covariates described in the previous subsection. We also included institution-specific time trends (δit) to improve internal validity by allowing outcomes to take different trajectories across institutions during the pretreatment period (Angrist & Pischke, 2009; Belasco et al., 2015). Specifically, we created the institution-specific trend variable by regressing each dependent variable on each year for each institution, using data from the period before TRIP was adopted for individual institutions. The trend covariates incorporated into our models multiply the resulting coefficients by year and are unique for each institution-year observation. To correct for serially correlated error terms and heteroscedasticity in panel data (Bertrand et al., 2004), we estimated robust standard errors in each model by clustering at the institutional level. Because eligible institutions and their donors may have a delayed response to TRIP adoption and it takes time to process funds, we ran additional specifications modeling a 1-year lag and a 2-year lag. Because all public universities in Texas received a variety of state funds to develop research capacity or build programs for national recognition (THECB, 2009), they were excluded from the comparison group. Additionally, based on the 2010 Carnegie classification, two treated institutions were categorized as "master's colleges and universities" and six treated institutions as "research universities with high research activities" in the base year, and several treated institutions have become "research universities with very high research activities" between 2006 and 2017. Thus, we included only institutions with Carnegie classifications as public research universities in the comparison group given their comparable institutional mission and research capacity. To the best of our knowledge, the states other than Texas had not adopted a statewide program to encourage private-sector donations to universities at the time of this study. To create a counterfactual of institutions closely resembling TRIP-eligible universities in observable ways, the comparison group was selected using Coarsened Exact Matching (CEM). This procedure controls for pretreatment heterogeneity and temporary shocks that could influence the probability of being treated (Blackwell et al., 2009; Cerulli, 2015). Matching on pretreatment outcomes can improve residual balance and reduce bias from the subsequent regression model on matched samples instead of whole samples of treated and nontreated units (Ho et al., 2007; Rubin, 1974). Previous work has combined CEM procedures with regression models to condition for observable differences between the treated and untreated institutions to select groups of matched units in studies (Hillman et al., 2014; Ortagus & Hu, 2019). Unlike propensity score matching, which matches institutions based on similar propensity scores that were generated based on a set of covariates, CEM exactly matches institutions on selected characteristics to improve balance for each variable in isolation. In other words, CEM is less model-dependent and it allows comparison between treated and untreated observations that are comparable for each variable separately without reducing balance in other covariates (King & Nielsen, 2019). CEM also approximates a fully blocked experiment with a preset degree of the balancing of covariates, avoiding repeated assessment of covariate balance and reestimation of the propensity score (Cerulli, 2015). According to King and Nielsen (2019), the CEM algorithm leads to "lower levels of imbalance, model dependence, and bias than PSM [propensity score matching]" (p. 8). Specifically, to select a group of matched institutions, we first estimated a logit model predicting a college's eligibility for TRIP, given the set of observable institutional-level covariates measured at baseline (i.e., the year of 2006). To determine the inclusion or exclusion of conditioning variables in the CEM model, we applied a data-driven approach based on a t statistic and its associated p value (Rosenbaum, 2002; Rosenbaum & Rubin, 1984). The logistic regression model for CEM thus included only pretreatment covariates whose group differences met a threshold of |t| > 1.5 for significance, including institutional size (p = .024), undergraduate graduation rate (p = .046), and total amount of federal, state, and local appropriations (p = .097). Second, we temporarily coarsened the two continuous variables (i.e., graduation rate and total federal, state, and local appropriations) at quartile breaks for each unique observation. Finally, the institutions were matched exactly on institutional size and the two coarsened variables. This procedure created 38 strata with unique numbers of cases. Six strata were matched yielding eight treated institutions and 35 untreated institutions, and other observations in unmatched strata were discarded (Iacus et al., 2009). We used the original values of the coarsened variables in our subsequent analysis. The L1 statistic, which is a measure of multivariate imbalance (Iacus et al., 2009), for the full sample was 0.801 whereas the matched sample yielded an L1 statistic of 0.475. By comparing the prematching imbalance results with the postmatching imbalance results for the original values of the coarsened variables (Table 1), a substantial reduction in imbalance in the means and the marginal distributions suggested improved covariate balance (Blackwell et al., 2009). The coefficient for all variables in the logit model predicting a college's eligibility for TRIP is also smaller and statistically nonsignificant for the matched sample, which indicates that CEM was successful in making the groups more similar and reducing bias in the estimations. Supplemental Appendix C (available in the online version of the journal) presents a descriptive summary of the treated and untreated groups. We used several approaches to check robustness of the analyses. First, the consistency of the results was examined over additional comparison groups with multiple pre- and posttreatment periods included in the analyses (Furquim et al., 2020; Meyer, 1995). The first additional comparison group was restricted to 10 public research universities in states neighboring Texas (i.e., Arkansas, Louisiana, New Mexico, and Oklahoma), due to adjacency and a high level of similarities (Cook et al., 2008). The second additional comparison group included 60 public research universities in the member states of the Southern Regional Educational Board (SREB), which share similar demographic, social, economic, and labor market conditions. As indicated by Supplemental Appendix D (available in the online version of the journal), the consistent pattern in most model specifications suggested that the average treatment effects of TRIP adoption were robust regardless of comparison samples. Second, because individual institutions may have had different trajectories before TRIP implementation, our GDiD model accounted for institution-specific trends during the pretreatment periods (Angrist & Pischke, 2009). In addition, we followed the recommendations by Furquim et al. (2020) to conduct a series of event studies for all comparison groups to examine the presence of pretreatment trends. Specifically, we included a series of binary variables denoting leads and lags of TRIP adoption, ranging from 5 years before the adoption to 5 years after. Untreated institutions were coded as 0 for each of these variables. In addition to providing a means to identify any pretreatment trends, the event study approach also serves as a falsification test to ensure estimates reflected the impact of TRIP adoption rather than any differences in the outcomes between treated and untreated institutions. Coefficients from event study analyses were generally nonsignificant before TRIP adoption, suggesting that pretreatment trend was not a major concern. The CEM procedure and the inclusion of institution-specific time trends have further reduced pretreatment biases, suggesting that the parallel assumption is met for the CEM comparison group. Finally, because time-varying treatment can bias the estimator, we performed the Goodman-Bacon (2018) decomposition test to check which comparisons weight the most in the aggregated treatment effect. The results showed that the treatment effect was largely driven by comparisons between treated and untreated institutions. In particular, 67% of the weight comes from comparisons between treated institution and untreated institutions in neighbor states, and it weighted 83% in the comparison with untreated institutions in SREB states and 76% in the CEM group. Though there is some evidence that the time-varying treatment can bias the estimator, its effect was limited. We also included the Goodman-Bacon weights in our main analyses to account for any biases caused by time-varying treatment as a robustness check. Specifically, instead of including the unit-specific time trend, we regressed the outcome on all the fixed effects and control variables and the interaction of timing group indicators, which was generated by the bacondecomp Stata command, with a linear time trend for the pretreatment periods. Then, the residuals from this regression from the whole sample were used as the outcome in GDiD regression. The combination of different approaches of robustness checks strengthened our conclusions. This study is subject to several potential limitations. First, while we controlled for various institution- and state-level covariates and used institution-level fixed effects to account for these confounding factors, unobservables have the potential to bias the GDiD estimation. For example, because donor motivations can also be influenced by the long-term viability and quality of life of their community, such as civic and education concerns (Kim et al., 2011), and the goals of inserting their agendas into colleges and universities (Boyce, 2013), unobservable community-specific variations can confound the treatment effect of TRIP adoption on the outcomes. Though TRIP matching funds were only awarded based on private gifts and endowments for research activities, we were unable to isolate the amount of private donations that was specifically devoted to research productivities and faculty recruitment due to data limitations. The results could have been upwardly biased if private gifts and endowments that were not restricted to research capacity–building also increased for the treated institutions. It is also possible that there was some spillover effect of the broad Emerging Research University Initiative in Texas. While we selected multiple comparison groups closely resembling the TRIP-eligible universities, we interpret the causal effect of our models with caution, given the potential for omitted variable bias. Second, the relatively small number of treated institutions and comparable institutions tend to produce greater standard errors, which give a less precise estimate of the parameters. Given the sample, the statistical power is low, so there is always a probability of making a Type II error. With the sample size in the preferred model, at a .1 level, the statistical power for the treatment coefficient needs to be 1 to reach an effect size of 0.1, which is the threshold for a small effect size in Cohen's f (Cohen, 1988). Because the true effect size of TRIP adoption on endowment changes is unknown, we tested different thresholds for the original scale of the endowment outcome to understand the effect size benchmark in practical contexts. Even with a hypothetical 20% annual increase in endowment assets for treated institutions after TRIP adoption, the detectable effect size of TRIP adoption remains low at 0.004 (p = .103). Thus, a substantively meaningful increase in endowments from the policy perspective can remain a small effect size and may not be statistically detected by the current model given the sample size and power. We were not entirely surprised by the nonsignificant finding for TRIP adoption on endowment assets, because endowments take longer to grow and are a function of other factors, such as market volatility. However, given this limitation, we urge caution in the interpretation of null results. On the other hand, we are more confident in our significant results given the relatively small sample size. Third, our data have some inconsistencies in the level of reporting, which affects a modest number of institutions in our analytic sample (Jaquette & Parra, 2014). In particular, two institutions in the neighbor comparison group are parent institutions with partial child reporting, representing 7% of all observations. In the SREB comparison group, only 2% of observations are affected by the parent-child reporting issue. In the CEM group, 12% of observations were influenced by the parent-child relationship. By collapsing institution-level data to the parent level (Jaquette & Parra, 2014), we cannot separate the portion of endowment changes for the parent institutions from the ones for the children institutions, so the estimation of the effect of TRIP adoption on this particular outcome can be biased. Thus, we ran model specifications excluding institutions in a parent-child relationship. The results of our outcome model specifications are highly consistent, indicating that the point estimates were not sensitive to this data limitation. Finally, based on our theoretical framework, the effect of TRIP could have been due to either campus officials' greater effort or effectiveness in fundraising or donors' increased willingness to donate. The current study does not differentiate how stakeholders' behaviors mediated TRIP's effect on the outcomes. Table 2 presents the means and standard deviations of the outcome variables for the treated and untreated group. After performing the CEM procedure, the matched treated group had a larger amount of all three outcome variables than the matched untreated group. Specifically, the average amount of private gifts received per FTE for TRIP-eligible institutions ($861) was higher than untreated institutions ($803). Similarly, the average amount of endowment assets value of TRIP-eligible institutions per FTE ($13,808) and the average amount of state contracts/grants per FTE ($1,051) were both higher than the average of CEM-untreated institutions with an average of $12,278 and $623, respectively. Figures 1 to 3 illustrate the longitudinal trend of each outcome for the treated and untreated group. Descriptive differences in the outcome variables suggested a need for further inferential analyses that controlled for both state- and institution-level covariates using our GDiD approach. Given the consistency of findings across comparison groups, Table 3 presents only the GDiD coefficients of effects of TRIP adoption on the outcome variables for the matched group, accounting for institutional- and state-level covariates and the institutional specific trend. In the 1-year lag model specification, TRIP adoption was associated with a 37.2% increase (p < .05), or an additional $6.7 million ($18,084,000 × 0.372) in private gifts for TRIP-eligible institutions during each year following TRIP implementation, when comparing with non-TRIP institutions in the matched group. In the 2-year lag model, TRIP adoption was statistically significantly associated with a 50.9% increase in private gifts (p < .1) when comparing TRIP-eligible institutions and non-TRIP institutions. In the no-lag model, TRIP adoption was not statistically associated with changes in private gifts for TRIP-eligible institutions. Turning to endowment assets (as opposed to annual gifts), the effect of TRIP was nonsignificant on the proportional change of the endowment values in any of the model specification, holding institutional- and state-level covariates constant. Table 3 also reveals a positive and statistically significant relationship between TRIP adoption and state contracts/grants received by TRIP-eligible institutions, accounting for institutional- and state-level covariates. Following the same calculation, TRIP adoption was associated with a 65.4% increase in state contracts/grants (p < .01) when comparing TRIP-eligible institutions and matched non-TRIP institutions. On average, a 65.4% increase would result in an additional $14 million ($21,727,000 × 0.654) of state contracts/grants across all postprogram years after program implementation. For the 1-year lag model, TRIP adoption was associated with a 66.3% increase in state contracts/grants (p < .01) for TRIP-eligible institutions 1 year after TRIP adoption. The relationship between TRIP and state contracts/grants was not statistically significant 2 years after TRIP adoption. As indicated in Table 4, the event study results are largely consistent with the findings reported in Table 3. Figures 4 to 6 provide a graphical representation of event study estimates for each of the outcomes. These figures demonstrate a discernible increase in the coefficient estimates coinciding with TRIP adoption for the amount of private gifts and state contracts/grants received. The graphics suggest potential preexisting trends for our outcomes at the .1 level with large standard errors. Indeed, the event study estimates for our preferred model (Table 4) indicate a potential differential trend for a single outcome variable (i.e., state contracts/grants) 3 years before TRIP adoption. Therefore, we urge greater caution with the interpretation of this particular result. Though an immediate positive effect of TRIP adoption on the amount of private gifts was not found for treated institutions in the CEM group, there seems to be a significantly positive relationship two years after TRIP adoption (δ = 0.471, p < .05), indicating a 58.4% increase of private gifts for TRIP-eligible universities. The event study also indicated no statistically significant findings of TRIP adoption's effects on the value of endowment. Finally, our point estimates indicated that TRIP adoption increased the amount of state contracts/grants by 43.9% a year after policy adoption (p < .1), when compared with untreated institutions. As a robustness check, since Goodman-Bacon (2018) suggested that including institution-specific trends is not sufficient to address potential biases caused by the time-varying treatment, we incorporated the Goodman-Bacon weights into the GDiD model. Table 5 presents the coefficients of model specification incorporating the Goodman-Bacon weights, and they are highly consistent with the findings from the main GDiD analysis. In this study, we set out to examine whether a state can craft a research incentive policy that used relatively scarce state resources to encourage private donations to a subset of public universities. We analyzed a panel data set that covered a 12-year period using a GDiD framework with institution-level and year fixed effects. Our findings suggested that TRIP positively influenced revenues from private gifts and state contracts/grants for Texas' emerging research universities. We found evidence that the emerging research universities received more money from the state that paralleled increased donations. TRIP appeared to be successful at stimulating short-term revenues but did not have a statistically significant relationship with changes in endowment assets. The statistical findings are consistent with qualitative findings from a separate study. In late 2019, the second author interviewed university leaders, including a Vice President for University Advancement at a TRIP-eligible university. While talking generally about the university's development and messaging strategies, the participant stated, "We use TRIP as a match a lot." Additionally, the participant acknowledged: "TRIP has been a big help . . . we often talk about matching [funds]" when communicating with potential donors "and that's what it was designed for." Even though this article does not report findings from a mixed-methods study, we recognize that the anecdote supports both parts of our conceptual framework. The advancement officer recognized that state policymakers (i.e., principals) adopted TRIP to help state employees be more effective fundraisers (i.e., agents). In this narrative, the advancement officer frequently uses TRIP to encourage donors' motivations. This study contributes to multiple areas of scholarship, including studies on the dynamics of public and private funding for higher education, studies of research incentive policies, and the application of principal-agent frameworks to study education policy. This study provides evidence that a state can encourage giving by providing matching funds to the recipient of the donation. The findings in this study were consistent with prior research on the dynamics of public-private funding (e.g., Cheslock & Gianneschi, 2008), while contradicting previous studies (e.g., Oster, 2001), which cautioned that donors may be reluctant to write checks when they perceive universities as becoming wealthier. Our findings supported the position that donors perceived the government's willingness to provide additional money to a university as a signal of quality and were more likely to donate (Cheslock & Gianneschi, 2008; Kleer, 2010; Payne, 2001). Research incentive policies are not new, but TRIP represents a new take on an old policy challenge. National governments around the world have developed policies that awarded scarce public funds to a targeted group of universities, which were deemed as having the best chance at improving their national and international research profiles and reputations. When researchers evaluated those policies, they found that research incentive policies influenced the volume of research publications and resulted in larger shares of publications landing in more prestigious journals, particularly among lower tier research universities (e.g., Seong et al., 2008; Shin, 2009; H. Zhang et al., 2013; L. Zhang et al., 2017). However, prior scholarship tended to focus on research incentive policies that were developed and implemented by relatively strong central governments with more nationalized higher education systems. In those countries, governments awarded funding directly rather than trying to encourage private investment. Yet private investment was an increasingly important element of the triple-helix model of R&D at the beginning of the 21st century. The triple-helix model posited that in this era, economic progress required sophisticated research and human capital development, but policymakers and firms were unable or reluctant to bear the full costs of supporting R&D; therefore, governments, universities, and the private sector were required to work together to support scientific advancement (Leydesdorff, 2018; Leydesdorff & Meyer, 2006). Compared to other research incentive policies that were identified by Fu (2017), TRIP more closely approximated the triple-helix model. Unlike prior research, this study did not examine whether the research incentive policy increased scholarly output. Instead, this study provided new evidence to the scant research base that considered whether increased government support—and the signal provided by that support—could stimulate monetary donations or help build universities' research capacity. Additionally, the analyses in this study offered implications for our understanding of how a principal-agent framework may be applied in education policy. We use a principal-agent framework to consider whether a state policy encourages universities to pursue and obtain additional donative revenue. Findings suggested that TRIP adoption was positively related to emerging research universities receiving private gifts, which can greatly contribute to research capacity. While other private nonprofit organizations, such as social welfare services, reduced their own fundraising efforts upon increased government support (Andreoni & Payne, 2003), TRIP universities appeared to increase (or at least maintain) their ability to raise private gifts, and they were more successful fundraisers than comparable institutions. The emerging research universities (agents) were responsive to the interest of their principals (state policymakers) when incentives for securing donations were relatively high, ranging from 50% to 100% matching grants. In future work, perhaps with game theory frameworks, scholars could analyze the cost of pursuing large donations and consider whether a state could calculate more optimal matching rates. Furthermore, it was unclear from our study whether TRIP universities were investing donations to earn future dividends, since we did not detect statistically significant increases in university endowments. Qualitative interviews could reveal whether using donations in the short term versus investing them to build long-term endowments (which is also a metric for maintaining "emerging research status" and for achieving national and international recognition) is a preference of university administrators or donors. Future research could also examine whether a state could better achieve its goal of supporting emerging research universities by incentivizing short-term expenses compared to long-term endowment investments. Finally, we identify additional areas of future research based on TRIP's potential unintended consequences and lateral effects, the latter of which occur in the long term when TRIP universities are also affected by other state policies (Mettler, 2016). Scholars may consider whether incentivizing universities to pursue private funding could lead to a narrowing of the institution's mission to fulfill various state needs. For example, we expect public universities to fulfill a public good that includes offering social mobility, training workers in a variety of fields, and educating citizens. Can public universities be incentivized to pursue private gifts in ways that detract from their other public goals? This study raises numerous questions that warrant investigation. Future work may explore whether the effects we identified contributed to further institutional inequality over time, including by estimating Gini coefficients (e.g., Halffman & Leydesdorff, 2010). Additionally, researchers should consider whether the pursuit of private funding can dictate the types of research universities support (Slaughter & Leslie, 1997). In other words, to what extent should public universities focus on pursuing the research priorities of the private sector versus pursuing essential but perhaps less profitable research areas? Finally, scholars and policymakers may consider whether TRIP had lateral effects that may enhance or dilute the effects of the many other initiatives in Texas (e.g., NRUF, RUDF, RDF) or whether TRIP reduced state funding, especially in grants and contracts, for nonemerging research universities. Although these topics are beyond the scope of this study, they are important questions to address in future research. Taken together, our results suggested that policies like TRIP may be an effective strategy for states to encourage greater support from private sources. During times of fiscal austerity, an incentive program like TRIP challenges organizational stakeholders to engage private donors to support public higher education. While we found significant and relatively consistent findings across comparison groups and model specifications, it is important to remember that TRIP is underfunded. TRIP had a $157 million backlog of unfunded but approved applications for matching grants in December 2019. Texas policymakers will have to decide whether to double down on TRIP and reduce the backlog of applications for unfunded but eligible gifts or whether to continue to allow universities to use the promise (on paper) of a matching grant to solicit donations from private companies and individuals. Our evaluation of TRIP showed that a state can successfully choose a small set of universities and offer them "preferential treatment in public funding allocations" (L. Zhang et al., 2016, p. 885) to expand their research capacity. However, we leave unanswered the question of whether states should do so. By their nature, research incentive policies around the world have sought to increase stratification within and across higher education systems. Universities can rise only if they move up in rankings or research output relative to their peers. Policymakers have justified selecting some universities over others by using metrics and by invoking state or national interests for educational, economic, research, and national security competitiveness. Generally education researchers see stratification of colleges and universities as having negative effects on postsecondary equity (Posselt et al., 2012; Shavit et al., 2007; Stephan et al., 2009). In Texas, there was widespread support for reducing the gap between The University of Texas at Austin and Texas A&M University at College Station on one hand, and at least some public universities in the state on the other. One perspective is that TRIP is reducing inequality between the emerging research universities and the two universities that benefit from PUF. Another perspective is that the ascendance of emerging research universities could allow other public institutions in Texas, including minority-serving institutions, to fall further behind in terms of public funding and research capacity. TRIP did not to help other public universities that were not selected for additional funding but could have benefitted from additional state support to expand college access, increase affordability, and improve student success (Perna & Finney, 2014). Finding the balance between efficiency and equity remains a priority as higher education decision makers develop and implement research incentive policies. Xiaodan Hu https://orcid.org/0000-0002-8648-0601
10.3102_0002831220969138	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831220969138	The College Preparatory Pipeline: Disparate Stages in Academic Opportunities	 The rise in college preparatory coursework across American high schools appears not to affect college enrollment and graduation rates. This study uses the Civil Rights Data Collection to evaluate three stages along the college preparatory pipeline: access to, enrollment in, and mastery of Advanced Placement® and International Baccalaureate® coursework to understand the cumulative academic opportunities shaping students' college readiness. Leaks in the pipeline divert out historically marginalized students. An adaptation of the Herfindahl-Hirschman Index captures the magnitude of these racial and ethnic disparities. Social context explains where school and district resources alleviate disparities to provide more equitable (i.e., proportionally representative) academic opportunities. These findings offer substantive direction to improve equality in students' college readiness opportunities.	 Since the late 1980s, high school students have gained more and more access to Advanced Placement® (AP; Geiser & Santelices, 2004; Kolluri, 2018) and, more recently, International Baccalaureate® (IB) course offerings (Donaldson, 2017). Access has widened in the sheer number of schools offering these college preparatory (prep) courses, and the course choices offered are increasingly varied (Geiser & Santelices, 2004; Judson & Hobson, 2015; Kolluri, 2018). The increased access and normalization of college prep offerings has "increased the average" of high school course-taking expectations (Schneider & Stevenson, 1999). The rise in access to prep coursework has, unexpectedly perhaps, not translated to an increased number of college admissions or baccalaureate degree completions (Snyder et al., 2016). Considering the rhetoric surrounding AP and IB courses, one would expect the increased access to yield stronger results (College Board, 2018; Evans, 2019). Since 2000, undergraduate college enrollments rose 20% overall (National Center for Education Statistics, 2018), but this enrollment gain was not evenly distributed. The rapid rise of students from Latinx backgrounds—who rose from the lowest enrollment rate to nearly the rate of White students—drove the overall increase in college enrollment. Otherwise, when we look deeper into college enrollment trends, prominent gaps remain, as shown in Figure 1. Persistently, Asian students enroll at rates 10 to 15 percentage points higher than White students; Black students enroll at a rate approximately 8 percentage points lower. Despite the increase in college enrollments, graduation trends remain fairly flat by comparison. College graduation rates among Asian and White student subgroups persistently remain twice those of Black students and 1.5 times the rate of Latinx students (Snyder et al., 2016). College graduation is inching upward at 1 to 3 percentage points across most enrolled students, although Black and multiracial college students are experiencing a drop in graduation (Snyder et al., 2016). The differences in these college enrollment and completion trends may be explained in part by the wide variation in high schools preparing students for college. To assess this explanation, this study captures the level of students' exposure to the college prep coursework pipeline and the racial/ethnic disparities in exposure using a version of the Herfindahl-Hirschman Index (HHI). A framework of organizational resource allocations associated with social context then explains where schools and districts alleviate or exacerbate these disparities. This framework, the prior literature, and the conceptualization of these disparities are explained next. In the United States, academic opportunities are not supposed to be predetermined by the education level or wealth status of students' parents. Rather, the ideal of equal opportunity shapes the delivery of U.S. K–12 education. Although this ideal is far from the reality, schools and districts use their available resources to expose students to an array of academic opportunities. With that said, the amount of resources varies vastly from district to district and even between schools within districts. Coleman (1990) referred to the diseconomy of resources when describing the resource constraints that school administrators confront when allocating resources to schools in a district, which involves (1) the quantity of resources (how much and how many are available to allocate); and (2) the quality of resources (new or tattered buildings or textbooks, accredited or not, etc.). The diseconomy involves the allocation of the quantity and quality of resources to different schools and the experiences of the students receiving those resources. When both the allocation and the experience tilt in the direction of "loss of input," there is a diseconomy of resources. For example, if teacher salaries within an urban district are equal between schools and some of the schools serve lower income neighborhoods where many students experience poverty (Title I schools) and some are located in more affluent neighborhoods in the city (not Title I schools), then the district is not competitive in salary and will lose teachers to the more affluent suburbs (Coleman, 1990). When thinking of academic opportunities, this diseconomy of resources offers a perspective to understand where and why leakage occurs along the college prep pipeline to ready students for college. Drawing on Coleman's diseconomy of resources perspective to examine the allocation of resources by administrators and the experience of those resources by students in schools, this study assesses how schools and districts use their organizational resources to shape AP and IB curricular opportunities for students by decomposing the pipeline into three interlocked stages:1. Access: Do students attend schools with the curriculum offerings?2. Enrollment: Who in the school with the curriculum participates in these particular courses?3. Mastery: Which students in the classrooms acquire the credential to demonstrate that they learned the expected content knowledge? To examine the allocation of resources, this study focuses on "where" aspect of the diseconomy of resources by focusing on this social context of schools and districts. Social context is partitioned into three elements: curricular policy, spatial location, and peers. This study analyzes how organizational resources induce educational equality or inequality as experienced by students in the same educational setting. Unlike many studies of AP or IB, this study does not take an individual, student-level perspective to understand the differences between students' success in their coursework. Instead, this study assesses the group-level perspective in relation to the organizational resources. The study thus gives direction to future policy discussions on the loss of input between disbursement and reception of academic opportunities that supports student preparation for college success. Research on the three stages in the curricular pipeline explain the persistence of inequality via students' differential exposures to academics. At the most basic foundation, equality involves the notion of the absence (compared with the presence) of disparities in schooling opportunities: Whether there exist measurable sociodemographic differences between students who have and those who do not have the ability to engage in academic opportunities. The following literature section reviews research on persistent academic opportunity inequality in American high schools. Both between-school and within-school tracking research identifies structural differences in access and enrollment. Differences in levels and tracks of courses between schools contributes to stratified access to rigorous courses, such as whether or not a high school offers calculus. In addition, differentiation in levels and tracks of courses within schools contributes to stratified enrollment in the courses, such as who and how many students enroll in the calculus class. The most recent review of this literature as well as empirical testing of access and enrollment upholds the continued pervasiveness of stratified differences and shows how the disparities accumulate over students' educational trajectories (Domina, McEachin, et al., 2016). Between-school tracking research addresses the first stage in the pipeline: access. This literature answers the question: What are the differences in schooling opportunities for students? During the decades of Jim Crow and residential redlining, the answers were clear: students of different racial and ethnic backgrounds attended separate and segregated schools (Tyack, 1974). Ending de jure segregated schooling after Brown v. Board of Education altered between-school tracking on access to curriculum to de facto policy. De facto tracking between schools is often addressed by demonstrating the spatial differences in district locale on students' access to differentiated curriculum (Klopfenstein, 2004). Rural districts often do not have enough numbers of students to offer differentiated sections of a subject course, such as an honors history and a regular history course (Cisneros et al., 2014; Gagnon & Mattingly, 2016; Iatarola et al., 2017). This especially impacts students of American Indian heritage who predominately attend schools in overwhelmingly rural areas (Apthorp, 2016; Christenson, 1996; DeVoe & Darling-Churchill, 2008). Courses in these schools adopt a "teach to the middle" approach (Iatarola et al., 2017; Solórzano & Ornelas, 2004). In urban-core districts, the student numbers are larger, but academic priorities are not geared to prioritize accelerated courses, as these are considered optional offerings among other mandated academic services (Iatarola et al., 2017). Although resources differ between the rural and urban-core districts, implications on educational inequality are similar between these spaces (Roscigno et al., 2006). Both types of districts struggle to attract and retain highly qualified teachers (Carver-Thomas & Darling-Hammond, 2017). Rural and urban-core districts inordinately staff their classrooms with more novice and fewer advanced credentialed teachers than urban-ring or suburban districts (Gagnon & Mattingly, 2016; U.S. Department of Education Office for Civil Rights, 2014). Within-school tracking research addresses the second stage in the pipeline: enrolling in the course. The research on within-school tracking in the late twentieth century assessed the educational outcomes of assignment of students to a program of courses; typically, students were assigned to college prep, general, or vocational (Bryk & Thum, 1989;Gamoran & Berends, 1987; Hallinan, 1996; Rosenbaum, 1976; Sørensen, 1970). Due to the detrimental effects of tracking on inequality (Oakes et al., 1992), tracking as a program of courses evolved into subject-by-subject ability grouping (Lucas, 1999). The change from rigid programmatic tracking a more flexible approach still theoretically allowed students to enroll in courses per individual abilities, needs, and wants. In practice, the more flexible subject-by-subject ability grouping approach continued to reproduce parental, social, cultural, and educational advantages where parents' assets of social networks and know-how advance their children into accelerated courses (Eaton, 2001; Kelly, 2004; Lareau, 2000; Lareau & Horvat, 1999; Lewis & Diamond, 2015; Oakes & Guiton, 1995). Tracking and ability grouping are persistently "sticky" in that students rarely advance beyond their initial track or ability group assignment (Gamoran & Mare, 1989; Kelly, 2009; Lucas, 1999; Lucas & Berends, 2002; Oakes & Guiton, 1995). This stickiness acts to de facto segregate high school course taking (Lucas, 1999; Lucas & Berends, 2002; Tyson, 2011). Together, this research on disparate access and enrollment highlights the differences in the allocation and use of schools' and districts' organizational resources. Although many advanced courses have similar titles, titles do not guarantee students' exposure to the same level of rigor, content, or instructional quality (Domina, Hanselman, et al., 2016; Hallett & Venegas, 2011; Kelly & Price, 2011). Course quality is difficult to determine and certify in American schools due to the lack unified curricula and standards. Content considered advanced or accelerated in rural or urban-core districts may be considered the standard course of study in a neighboring urban-ring or suburban district (Domina, Hanselman, et al., 2016; Remillard et al., 2017). Examining the credentialing related to course mastery in the American primary and secondary school context is often relegated to course grades since no national course exams exist (Graham, 2005). State-based testing assesses general grade level proficiencies in subjects, but this does not link back to the nuanced differences of content between classrooms, such as between an honors and regular U.S. history course. Most of the research on differences in quality of curricula comes from qualitative comparative work. These qualitative studies often cite differences in teacher training, social-structural school resources, and family resources as reasons underlying differences in course rigor and mastery (Cisneros et al., 2014; Gagnon & Mattingly, 2016; Lareau, 2000; Oakes, 2005; Palmer, 2016). Recent research on eighth-grade algebra exemplifies the idea of varied rigor in course content. Courses may share the same name, but content can and will vary. Domina et al. (2015) find that California's algebra-for-all-eighth-graders policy induced schools to assign all students to eighth-grade algebra. In practice, teachers explain that they are only able to teach pre-algebra in many of their classrooms due to the preponderance of below grade-level math skills of students where teachers need to begin with more remedial teaching at the start of the course (Domina et al., 2015; Penner et al., 2015). On transcripts, all California students earn a grade in "eighth-grade algebra" yet mastery varies from general eighth-grade math to pre-algebraic to algebraic content (Domina et al., 2015; Penner et al., 2015). It is only the eighth-grade math exam that assesses proficiency in algebra across these different courses titled "algebra." A course exam offers a credential that attests to the mastery of the subject and provides a transferrable educational good (Bourdieu & Passeron, 1977). Without the exam credential, a course title on a transcript simply exchanges for lesser value because no third-party appraised the course quality (Bourdieu & Passeron, 1977; Evans, 2019). A transcript with an AP or IB course listed may provide a signal to college enrollment recruiters about students' potential to succeed in college, but the externally certified AP exam score or the IB diploma legitimates the course quality in exchange for college credit (Evans, 2019; Hallett & Venegas, 2011). In the case of AP, university policies of "3 or higher" exam scores set the exchange rate for mastery that bypasses three credits of college tuition (Evans, 2019; Kolluri, 2018). AP and IB classes provide curricula with third-party recognized quality and extends beyond state variations in grade-level standards. In addition, colleges and universities legitimate the quality of these college prep courses by including them in admissions criteria (College Board, 2018; Kolluri, 2018). This study thus uses data on AP and IB coursework to represent a substantial portion of college prep curriculum across the United States. With these data, the following research questions are asked, Research Question 1: To what extent do racial/ethnic disparities exist in American high schools along the three stages in the college prep pipeline, from access to enrollment to mastery? Research Question 2: Which social contexts related to resource allocations exacerbate or alleviate racial/ethnic disparities in the college prep pipeline? Districts and schools are the units of analyses in the study since these organizations disperse the curricula among their students. The Civil Rights Data Collection (CRDC) gathers population data from all public schools on AP and IB course enrollments and test taking (U.S. Department of Education Office for Civil Rights, 2014). Unlike other school curricula that are state regulated, these courses are nationally monitored by agencies external to the local, state, or federal education agencies. The CRDC is the first of its kind to collect a census inventory that counts all AP and IB student enrollments and pass rates from every public traditional and charter school in the United States. The CRDC also provides data specific to each AP subject, but given the premises of this study, the data for IB and all AP subjects are combined to assess organizational-level disparities in educational opportunities. In each year of the biannual data collection, the CRDC collects subgroup-level data on the gender and racial/ethnic enrollments for each course in each school. In the 2011–2012 and 2013–2014 school years, the racial and ethnic classifications were as follows: Latinx, American Indian, Asian, Native Hawaiian or Pacific Islander, (non-Latinx) White or Black, and multiracial. For the districts where more than 1% of their students were excluded from the racial/ethnic subgroup counts, this study adds a "no answer" subgroup to capture disparity among the students whose race or ethnicity were left unclassified. The data from schools managed by state or federal agencies, such as juvenile detention facilities and hospitalization schools, are excluded from these analyses. This CRDC also gathers data on school enrollment totals, Title 1 school status, and school type (charter, magnet, traditional, state managed). District and school data from the Common Core of Data from the National Center for Education Statistics supplement the CRDC data to measure locale and teacher certifications. Disparity measures provide metrics to understand the extent to which groups dominate a "market" (a form of opportunity hoarding, see Kelly & Price, 2011). This study uses a version of the HHI to measure the amount of disparity along the three stages in the college prep pipeline (Taagepera & Ray, 1977). The more equitable the market share across all groups, the closer the HHI gets to zero. The higher the HHI value, the greater the disparity in market share between groups. An extremely high value indicates a single group monopolizing a market. This HHI focus on concentration and clustering differs from the evenness assumptions of many segregation-based indices common to education research (Reardon & Firebaugh, 2002). Social movement research and political social science research commonly use the HHI measure to represent degrees of segregation (Boydstun et al., 2014; Taagepera & Ray, 1977). The HHI allows for multiple group representations in a single measure rather than a set of dichotomous relations (White-Black, White-Latinx, etc.), which is the formulaic basis of the segregation indices of Gini, Theil, and others. If, for example, a measure with dichotomous relations was used, there could be up to 21 combinations analyzed for each of the three stages if all seven racial and ethnic affiliations were represented in the school. This study therefore adopts an adjusted HHI to estimate the amount of monopolization by subgroup affiliations over the college prep curriculum within school markets (Price, 2019). The standard HHI assumes that each group has the same chance as any other group in the market (one group, one chance). Since district and school student subgroups are not the same sizes, this study normalizes the HHI estimate to adjust for the varied proportional representation of students in districts and schools at each stage in the pipeline (Price, 2019). The HHI measure of market concentration also adjusts well to the "moving denominator" between stages in the college pipeline trajectory, whereas most segregation indices assume a single, static event (Price, 2019). As an example of this dynamic nature across the three pipeline stages, imagine a district where all students attend one high school. In this school, 90% of the students in AP Calculus are 12th graders and the other 10% are 11th graders. This school has a quartiled student body of ninth, 10th, 11th, and 12th graders and so all students in the school would have access at Stage 1 (HHIaccess = 0), but the 12th graders hold a "corner on the market" of enrollment at Stage 2 (where HHIenrollment = 2.3). This score above 1 indicates a near monopoly in the market of enrollment. At Stage 3, if all of the 11th graders demonstrate mastery (receive a 3 or higher score) but only half of the 12th graders do, then the disparity is greater than Stage 2 where HHImastery = 8.3. This extremely high score shows that one group overwhelmingly dominates the Stage 3 market compared to the other group. Put another way, this extremely high HHI shows that Stage 3 heavily favored one group (11th graders) while diverting out a substantial proportion of the other group (12th graders), all of whom were in the market for a mastery credential. The adapted HHI calculations for measures of racial/ethnic disparity along the three stages in the pipeline are as follows: Access disparity measures the disproportionate racial/ethnic representation of students who attend the schools that offer college prep curriculum compared to all the high school students in the district; a between-schools, within-district measure.[MATH](1) where Π is the proportion of the district high school population, π is the proportion of high school students attending the district schools with AP or IB, and j is the subgroup designation. Enrollment disparity measures the disproportionate racial/ethnic representation between students enrolled in the college prep curriculum as compared to their representation among the general student body within the same school; a between-classrooms, within-schools measure.[MATH](2) where π is the proportion enrolled in district schools with AP or IB, t is the proportion enrolled in AP classes, r the proportion enrolled in IB classes, and j is the subgroup designation. Mastery disparity in college prep coursework measures the amount of disproportionate racial/ethnic representation among those students in AP or IB classes who pass the exam; a between-students, within-classroom measure.[MATH] where t is the proportion enrolled in AP, q is the proportion passing at least one AP exam with a score of 3 or higher, and j is the subgroup designation. All calculations are restricted to high school age students only (Grades 9–12); population proportions exclude preK–Grade 8 students. Proportionate representation (HHI = 0) could result when the student population comprises of only one racial or ethnic group. While 100% of a single racial or ethnic group composition is rare in U.S. districts, hypersegregation within schools and classrooms is less uncommon. Table 1 highlights the prevalence of districts (Stage 1) and schools (Stage 3) that serve diverse student populations that succeed at proportionate representation along the pipeline route (HHIs = 0). Social context shapes where resources are allocated and thus how students experience learning. Three elements of social context are measured in this study: curricular policy, spatial locale, and peers. Curricular policies on college preparation can be understood using measures of course availability to students and teacher availability to teach the courses. Course availability is measured using the proportion of classroom AP and IB seats in relation to the total student population., Teacher availability is loosely measured using the number of certified secondary education teachers in the district. A more ideal measure would link teacher qualifications to courses taught, but this information is not nationally collected. Resources associated with spatial locale are measured using four categories of rural, town, suburban, and urban. In addition to the teacher qualification differences discussed earlier, Roscigno et al. (2006) discuss the family and school economic differences by locale impacting students' schooling experiences. Measures of school sociodemographics capture the peer context element. Some research stresses how the Whiteness of the American schooling system biases schooling experiences and expectations toward White students (Cherng, 2017; Fisher et al., 1996; Orfield & Eaton, 1996; Tilly, 1998). However, in cases of schools with only a few White families enrolled, bias dampens and the voices of people of color can be heard (Anyon, 2014; Yosso, 2005). Other research stresses socioeconomic class dominating the schooling expectations and student experiences (Bourdieu & Passeron, 1977; Dietrichson et al., 2017; Kelly, 2004; Lareau, 2000; Lareau & Calarco, 2012; Salloum et al., 2017). Moreover, contemporary American studies call on the inextricable intersectionality of class and race in American schools (Carter, 2005; Cipollone & Stich, 2017; Ellison & Aloe, 2018; Hallett & Venegas, 2011; Lareau & Horvat, 1999; Ndura et al., 2003; Posey-Maddox, 2012). Given these complementary ideas, this study assesses the prevalence of the schools' Whiteness, majority minority composition, class (Title 1 status), and the intersection of race and class. Last, resource allocations in nontraditional schools differ. Charter and magnet schools operate under more independent funding and looser teacher certification regulations (Berends et al., 2009; Cannata & Penaloza, 2012). The self-selection to enroll in a charter or magnet school preemptively sorts social context (Berends et al., 2009; Riel et al., 2018; Sattin-Bajaj & Roda, 2018; Renzulli & Roscigno, 2005). For all these reasons, charter and magnet schools are flagged in the analysis to isolate the effect on the three focal social context elements. Table 2 details the descriptive statistics of the data used in this study. The descriptions show 13% of schools with college prep offerings show no disparity between peer groups of passing test scores among their enrolled college prep students. Also, more than 40% of districts evidence no disparity among students' access to these college prep offerings among their schools. However, five of six of these districts with equitable access only have one high school and so it is impossible for disparity to exist. Thus, this study adds a flag for districts that operate only one high school. Additional controls in the models that mitigate resource allocations are size of the student population and special education needs. Schools and districts with 95% to 100% homogeneous racial or ethnic student populations are also controlled since the chance of racial/ethnic HHI disparity is arithmetically low when there is nearly no variation on racial or ethnic composition. This study only speaks to organizational-level patterns. Using transcript data, Domina, Hanselman, et al. (2016) recently studied the longitudinal impact of curricular opportunities at the individual student level. They found clear evidence of divergent effects on individual student achievement and the reproduction and accumulation of inequality in educational opportunities within only 3 years based on students' initial start course. With that said, the organization-level patterns tested here are important to understand loss of input impacts college readiness. Unless they are titled with AP or IB the CRDC data do not ask about other advanced courses such as those addressing specialized science or literature topics. This limitation implies that these estimates narrow the definition of college prep from the broad scope of offerings that may expose students to college-level curricula. In addition, the CRDC does not track the credentialing of the IB program so this analysis imputes these few students across the exam measures to assume the acquisition of the credential. Extant analyses did test the sensitivity of the estimates by excluding these students; there were insignificant differences. To address the first research question, simple proportions of racial and ethnic subgroups of American students who remain in, compared with those who leak out, of the three stages of the college pipeline are presented. Since this study used population/census data, differences in subgroup populations are real and do not require sample-based statistical tests. Nonetheless, all t tests of the proportional differences between and within ("in" vs. "out") subgroups are statistically significant, p < .001. Data are pooled across the two school years because it smooths averages for the smaller and more volatile subgroups of American Indian, Hawaiian or Pacific Islander, and multiracial student subgroups. To address the second question, negative binomial regression models are used since the dependent HHI variables follow a Poisson distribution with a truncated left end and a long right skew. For access and mastery, 40% of districts and 13% of schools, respectively, demonstrate proportionately equal representation (HHI = 0). In these two models, zero-inflated negative binomial regression models are estimated in order to parse the analysis to describe the social context of the organizational resources that (a) explain variation in disparity (an exponentiated outcome estimate) and (b) the likelihood to associate with no disparity (HHI score = 0, a binary logit outcome estimate). Social context could work differently since (a) predicts estimates of disparity and (b) predicts the likelihood of an absence of disparity. Logistic regression analyses (not shown here) precede these disparity models to support the necessary warrant that these social context elements relate to schools' and districts' likelihood to establish each pipeline stage (contact author for details). Although these data are population data with no selection issues because there is no sampling, there may be differences related to resources associated with state-regulated funding formulae. The models therefore fix effects for the 50 states and the District of Columbia. On average, seven per 100 high school students attend districts that do not offer any AP and IB curricula. Among the students with district access, only 30% of American students attend the schools that offer AP or IB courses while the others attend traditional or charter high schools that do not offer any AP or IB. Six per 100 American high school students enroll in AP and fewer than one per 100 are enrolled in at least one IB course. Between the start and the end of the pipeline, only three per 100 American students enrolled in regular high schools earned at least one IB or AP course credential of mastery. Put differently, 97 per 100 high school students leak out of the pipeline at some stage. Although it could be the case that all high school students have access to AP or IB in their school, a substantial number of high school students are likely too early in their high school education to enroll in AP or IB. The IB Diploma Programme requires students to be at least 16 years old to enroll (International Baccalaureate Organization, n.d.). For AP, most schools require substantial prerequisite courses to accumulate to be eligible to register for the AP class (Kelly & Price, 2011). Between the typical high school Grades of 9 to 12, we would expect nearly zero students in Grade 9 and 10 to enroll in any of the three dozen AP courses—this immediately pushes the ratio to 50 per 100. Although all seniors could theoretically enroll in AP or IB if their school offered courses, it would be unlikely under our current educational norms. Similarly, fewer juniors than seniors would be assumed to enroll in these courses. Among high schools that enroll all seniors in at least one AP course, their enrollment ratio would be 25 per 100 and their exam mastery would not likely be 100%. Figure 2 shows how the pipeline selection process systematically differs by racial and ethnic affiliation. Given the decades-long discussion on tracking, this analysis only focuses discussion three historically marginalized groups—American Indian, Black, and Latinx students—compared to White students. Although most traditional public high school students attend districts that offer college prep curriculum, only 87% of American Indian students attend districts with AP or IB courses, leaving American Indian students more than twice as likely as other students to not have access to AP or IB anywhere in their district. Moreover, fewer than 24% of American Indian attend the schools that offer AP or IB, whereas 27% of Latinx and 30% of Black and White students attend these schools. When using the moving denominator among those students attending schools with college prep courses, 18% of students in these schools are enrolled in at least one college prep course. The gap is wide, however, from 20% of White students to only 11% of Black, 13% of American Indian, and 15% of Latinx students enrolled. Included in these proportions are the small enrollments in IB courses, but there is little difference between the 1.5% of students from each of these four racial and ethnic groups enrolled in IB. Demonstrating mastery on AP exams differs widely by racial and ethnic group affiliation. On average, 20% of students enrolled AP courses never take an AP exam, while another 54% students attain a passing score to demonstrate mastery of the course material. Approximately 55% of White students pass an AP exam, while 39% of Latinx and 30% of American Indian and Black students attain the mastery credential with a passing exam score of "3" or higher. These results provide evidence in response to the first research question about the prevalence of racial and ethnic disparities along the three stages in the college prep pipeline. American Indian students leak out at every stage. After Stage 1, Black students leak out at Stages 2 and 3. Latinx students leak out disproportionately at the last Stage, 3, by failing to attain a passing exam score. Table 3 shows the series of models that identify the social context elements associated with students' experiences of racial/ethnic disparities along the pipeline route (Research Question 2). HHI columns estimate factors that exacerbate (positive coefficient) or alleviate (negative coefficient) the magnitude of disparities when they exist. The two HHI = 0 columns (Models 2 and 5) estimate factors that increase or decrease the likelihood of proportionate representation when no discernable racial/ethnic disparities exist at that stage in the school. Disparity in access at Stage 1 regards whether, in the same district, students who attend schools with AP or IB curricula affiliate with racial or ethnic identities differently than their peers in other high schools that do not offer these college prep curricula. Models 1 and 2 therefore restrict the estimates only to the multischool districts because single-school districts and charter schools will immediately score HHI = 0 since the school population is the arithmetically identical to the whole high school population in the district or charter school. Model 1 shows that curricular policy on the volume of AP or IB availability does reduce disparity; districts with more AP and IB seats per pupil average lower rates of racial disparity in access to college prep curricula. The availability of secondary-certified teachers does not matter one way or the other regarding disparity in access. Spatial location advantages students attending urban schools: Students attending urban schools experience less racial disparity in access than other students. Urbanicity stands independent of the peer context in the school where the predominance of poverty in schools does not relate to differentiated access. Students attending a hypersegregated school serving students of color experience higher levels of disparity, however, compared to their peers in other district high schools. Model 2 shows that policy on the volume of AP or IB availability improves the chances of racially proportionate representation in access to college prep curricula (Model 2, an HHIaccess = 0). Suburban schools are the most likely to equalize access, while rural and township schools are the least likely. The peer context element works quite differently than disparity in Model 1: The predominance of student poverty reduces the chance of proportionate representation in access and hypersegregated schools serving White students are less likely to allocate access equally to their students even though less than 5% of their students are not White. Students of color attending hypersegregated schools do not experience this worsened chance of proportional representation. Disparity in enrollment at Stage 2 identifies the differences in racial or ethnic representation between students who sit in the AP or IB classrooms compared to the students who sit in the lunchroom. Model 3 shows that policy on the sheer availability of AP or IB seats in a school reduces enrollment disparity; schools with greater AP or IB availability average lower rates of racial disparity in enrollment to college prep curricula. Greater availability of secondary-certified teachers, however, associates with higher levels of disparity. By spatial location, rural schools average lower enrollment disparities while urban schools average slightly higher disparities compared to other locales. Peer context elements of poverty or hypersegregation levels do not play a significant role in racial/ethnic disparity in AP or IB enrollment. Racial/ethnic disparity at Stage 2 occurs ubiquitously (in 99.4% of schools). Thus, no model estimates where HHIenrollment = 0. At the final stage in the pipeline, disparity in mastery at Stage 3 assesses the extent of racial/ethnic disproportionality between students earning a passing score of 3 or higher on their exam. Students can fail to earn mastery two ways: they either not take any exam one or they take it and earn a low score of 1 or 2. Again, the results show that the volume of AP or IB offerings in the school reduce disparity, albeit a much smaller influence at this stage. The availability of secondary-certified teachers does not sway mastery disparity levels. The magnitude shrinks for the spatial location element of social context; students attending urban schools experience slightly more disparity than their suburban counterparts, but less disparity than if they attended a rural or township school. What matters the most at Stage 3 is peer context. Schools receiving Title I funding average greater racial/ethnic disparity in mastery among their students than other schools. Hypersegregated schools serving 95% or more students of color also average greater disparity in mastery between their students who enrolled in AP or IB. The two characteristics do not compound, however, Title I schools that predominantly serve students of color overcome much of the disparity that otherwise occurs. Title I schools serving 95% to 100% White students do not act the same way. Only a few social context factors relate to proportional representation in mastery. Policies related to offering more volume of college prep offerings lessens the chance of proportionate racial/ethnic representation on mastery. Students enrolled in college prep in urban schools also experience slightly greater likelihood of proportionate representation. Students enrolled in AP or IB who attend hypersegregated White schools or students who attend Title I schools that predominately serve students of color are both more likely to experience proportionate representation on mastery. These social context elements stand independent of school type, school or district size, the proportion of special education or limited English proficiency needs, single high school districts, state education funding, or whether or not the district Grades 9 to 12 student body is composed of 95% or more of all the same race or ethnicity. Thus, findings such as the impact of students in rural schools experiencing less enrollment disparity is not because those students all attend a single high school where nearly all of the students are White. Instead, the spatial location of rural schools stands independent, no matter the district size, design, or student composition. Together, these models show that some social context elements at an earlier stage continue to impact later stages in pipeline while other elements are more stage-specific. The prevalence of curricular policies to make more available college prep curriculum to students reduces disparity throughout the pipeline stages of access, enrollment, and mastery. Additionally, the policy of hiring teachers with a basic secondary certification does not help reduce disparities. Spatial location is stage-specific. It particularly distinguishes proportionate representation in access and disproportionate representation in mastery. In both cases, students attending suburban schools fare better than students attending other schools. However, suburban locale is not always advantageous: Students attending urban schools experience less discrimination in access and students attending rural schools experiences less discrimination in course enrollment. The social context question of whether the racial composition or economic status of peers affects the level of monopoly in the college prep marketplace proves quite nuanced. First, attending a hypersegregated White school does not reduce disparity at any stage. This means that the few (less than 5%) students of color attending these hyper-White schools do not experience more or less discrimination than if they attend a more diverse school. However, attending a hypersegregated school that overwhelmingly serves students of color with few to no White students does widen discrimination in access and mastery. In addition, high schools serving many students experiencing poverty substantially widens discrimination at Stage 3 of mastery. However, the schools' resource uses associated with these peer contexts work independently of each other as evidenced by the finding that hypersegregated Title I schools serving nearly all students of color do not compound disparity on mastery but instead overcome much of it. Last, these models account for prior disparity in the pipeline. Results show that disparity in-and-of-itself does not substantially accumulate from one stage to the next stage in the pipeline. This study explores a new 21st-century high school tracking issue: The rise and newly established prevalence of college prep curricula in high schools and the disparate opportunities that result from it. Before this study, the extent to which this new tracking system differentiated academic opportunities among American students throughout the pipeline stages was not understood. Prior studies on AP access and enrollment for one group of students or another proved invaluable to hypothesize on this larger structural issue of the complex, interlocked stages in this pipeline. This study traced the disparities along each of the three stages using two guiding questions: Which groups of students experience disparities at which stage in the pipeline and where do social contexts exacerbate or alleviate disparities for those students remaining in the pipeline? Focusing the results on the Latinx, American Indian, Black, and White student subgroups spoke to the intersection of the urban/rural divides and different experiences with racial discrimination in schooling. Answers to both questions clearly evidence unequal opportunities. The findings related to the first research question show stark racial/ethnic differences in college prep opportunities among students. American Indian students are disproportionately diverted out of the pipeline early with relatively low access to college prep curricula anywhere in their district or their schools. These students are also disproportionately underenrolled and do not pass the exams at higher than average rates. Black students are disproportionately diverted from enrolling in courses and, among those one-in-ten who enroll, two thirds leak out without a passing score. Latinx students tow the average along the pipeline route with White students until Stage 3 where they experience disproportionately low pass rates on exams. These findings coincide with case studies (Ndura et al., 2003; Solórzano & Ornelas, 2004; Tyson, 2011) and state (Cisneros et al., 2014; Klugman, 2013; Moore & Slate, 2008) and nationally representative (Gagnon & Mattingly, 2016; Judson & Hobson, 2015; Moore & Slate, 2010) studies that look at single stages along the AP course-taking pipeline. Put together, this study's interlocked pipeline analysis points to the accumulation of structural inequality. Understanding where these inequalities occur can inform policy discussions to address these academic juggernauts that particularly disenfranchise students from historically marginalized groups. The diseconomy of resources theory provides a means to test how organizational resource allocations associated with social context structurally exacerbate (Coleman's "loss of input") or alleviate these persistent racial/ethnic inequalities. The administrative decision to allocate resources to add college prep curriculum holds promise to reduce racially and ethnically disparate opportunities. Adding in-name-only titled AP courses, however, do not deliver on mastery of the course content (Cipollone & Stich, 2017; Domina, Hanselman, et al., 2016; Hallett & Venegas, 2011) although these findings suggest that even if courses were added in-name-only, the declines would not be unequally shouldered by some students but rather all students would equally experience poorer quality. Focusing policies on the most basic threshold of staffing secondary-certified teachers is not adequate enough to benefit students' experiences in college prep. The social context of spatial locale especially differentiates mastery at Stage 3. The limitations of resources in non-suburban areas create disparate outcomes. This finding may point to the differences in teacher quality across locales that impact the rate of learning (see Carver-Thomas & Darling-Hammond, 2017), or it may point to the differences in parental educational advantages in the rate of learning (see Roscigno et al., 2006). These data cannot speak to the mechanism related to spatial locale, but future research would want to gather national data to further inform policy. The peer context findings speak less to the premise that class (income) of peers associates with differentiated resource allocation and instead speaks much more to the research strand regarding Whiteness hindering learning opportunities for students of color (see also Davies & Rizk, 2018; Lewis & Diamond, 2015; Tyson, 2011). If disparity was truly an underlying class-based mechanism, the measure of Title 1 schools serving 95% to 100% White students would stand out, but it does not. Moreover, the impact of concentrated poverty (Title I) on disparity levels pales in comparison to racial or ethnic hyper-segregation measures. This finding coincides with a recent test of the "school socioeconomic status effect" on learning by Armor et al. (2018) who find the impact of free or reduced-price lunch rates are mostly an artifact of other school contexts, including racial composition. To push these social context ideas further, this study also controlled for specialty schools with different regulatory standards since some scholars propose that these schools offer a condition where the norms and expectations around schooling are shared from the onset with selection to attend the school (Ellison & Aloe, 2018; Riel et al., 2018; Sattin-Bajaj & Roda, 2018). Independent of the homogeneity of the specialty schools' peer composition, attendance selection appears to pan-out in charter schools to reduce the disparity between students within schools. Magnet schools, however, follow an inverse effect where racial/ethnic disparity magnifies in these schools. Further research will want to understand why resource allocations differ among these nontraditional school types. In conclusion, the diseconomy of resources framework argues that it is not just about the availability of resources but also their allocation. These findings lend support to the notion that administrators can differentially allocate their resources to improve the chance of equal opportunities for students, reduce discriminatory educational opportunities, and stop leakage from the pipeline. Even if a district is fraught with disparity, the finding that disparity at one stage along the pipeline has no substantial impact on the next stage holds promise because it means that school administrators' resource allocations can work internally to alleviate in-school disparity. The prevalence of disparate opportunities along this new college prep pipeline indicates that the current system fails to deliver on the promised exchange value of college prep coursework. Students can be short-changed on the exchange rate on the educational good they think they are acquiring in high school by taking college prep courses but not actually gaining mastery of the content. The evidence here shows that this educational pipeline disparages historically marginalized students at rates higher than their advantaged peers and this disparity structurally exists between schools and districts. For some students, like American Indian students, the pipeline is leaky at all stages. For other students, the sediment in the pipeline slowly builds up over time and eventually creates a significant and detrimental effect on their learning. These CRDC data allow the mapping of the AP and IB from access to enrollment to mastery in efforts to assess the extent to which social context elements compound disparities in academic opportunities. Although college prep curriculum is in-place for the overwhelming majority of American high school students to access, there are still a number of districts with no access (the first sequence in the diseconomy of resources regarding resource quantity). If students are lucky enough to be in a district with access, they then need to attend the schools with these courses, get enrolled in them, and master the content. Currently, all resources allocated for these stages vary by the school or district and depend on the elements of social context (the second sequence in the diseconomy of resources). Historically marginalized groups of American Indian, Black, and Latinx students are disenfranchised and experience a loss of input under the current pipeline structure. These findings build on years of equality research to evidence the cumulative divergence of curricular opportunities in the U.S. educational system. The findings for research question one on racial and ethnic disparities in access, enrollment, and mastery dovetail with the educational studies on resegregation of educational opportunities (Clotfelter, 2004; Frankenberg & Orfield, 2012; Mickelson, 2003; Orfield & Eaton, 1996; Orfield & Lee, 2005, 2006; Quillian, 2014) as well as the current research regarding the unequal learning experiences of students in American high schools (Frankenberg & Orfield 2012; Judson & Hobson, 2015; Lewis & Diamond, 2015; Mickelson, 2003; Tyson, 2011). The findings complement studies on within-school segregation by moving the discussion forward to understand the diseconomy of resources that stratify academic opportunities and learning. This study brings to the forefront the prevalence of the allocation differences that have been theorized (Coleman, 1990; Lucas, 1999), but seldom tested with national population data. Where students attend high school shape their opportunities. Differences in how high schools ready students for college provides some explanation to the persistent gaps in college attainment. It is imperative to prepare students in high school to be "college ready" in this era where college enrollment is common, but completion still evades many college students (Snyder et al., 2016). Understanding why "where" matters for loss of input in the leaky pipeline is the next step in this research. A strength of this study is the comprehensiveness of the population data so that no racial or ethnic subgroup might go unaccounted due to small counts. The gaps measured for the dependent variable are therefore not subject to sampling error. Using these data, however, limit the resource measures to that which could be matched in other federal databases of the population of American schools, like the Common Core of Data. Further studies would benefit from richer resource measures to better understand why leakage occurs. For example, variables on teacher training, social-structural resources, family resources, and links of teachers to the students in their courses could deepen the conversation about resource quality in the leaky college prep pipeline. To be sure, this study cannot test the intentions of school administrators or teachers. These data cannot reveal if this stratified pipeline of college prep curricula opportunities is due to intended actions of a few in power to maintain segregation and inequality, if this is a product of unintended consequence of school and district policies, and/or if this stratified pipeline is a function of historical and systemic racism in America. Although intention is important to study in order to induce social change, this study begins by evidencing the social problem in order to drive research to understand it more so that students do not endure continued discrimination in their readying for college. Heather E. Price https://orcid.org/0000-0001-5359-3231
10.3102_00028312211001810	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/00028312211001810	Coming Soon to a Neighborhood Near You? Off-Campus Recruiting by Public Research Universities	 Scholarship on college choice largely focuses on how students search for colleges but less is known about how colleges recruit students. This article analyzes off-campus recruiting visits for 15 public research universities. We Web-scrape university admissions websites and issue public records requests to collect data on recruiting visits. Analyses explore the similarities and differences in off-campus recruiting patterns across universities in the study. Results reveal socioeconomic, racial, and geographic disparities in recruiting patterns. In particular, most universities made more out-of-state than in-state visits, and out-of-state visits systematically targeted affluent, predominantly White localities. We recommend that future research should exploit new data collection methodologies to develop a systematic literature on marketing and recruiting practices in higher education.	 College access is an outcome that depends on interactions between students looking for colleges and colleges looking for students. How do students and colleges find one another? Most scholarship analyzes the process by which students search for colleges. A massive "college choice" literature highlights many factors that influence students' decisions about where to apply and where to enroll, including financial costs (Avery & Kane, 2004), sociocultural predispositions (Hossler & Gallagher, 1987; McDonough, 1997), information asymmetries (Hoxby & Avery, 2013), and proximity to home (Hillman, 2016; Turley, 2009). By focusing on the characteristics and behaviors of students, families, and K–12 schools (the "demand side" of higher education), the college choice literature largely views colleges and universities (the "supply side") as passive recipients of students. However, analyses of organizational behavior show that colleges and universities are very purposeful about which prospective students they search for. A literature within the economics of education analyzes enrollment management behavior, with a particular focus on selective private institutions (Clotfelter, 1996, 2017; Winston, 1999; Zimmerman, 2003). This literature highlights financial aid as a lever to influence student decisions but ignores marketing and recruiting interventions that target prospective students at earlier stages in the college choice process. A case study literature in sociology analyzes the full range of enrollment management behaviors (Cottom, 2017; Posecznick, 2017; Stevens, 2007) but does not systematically document which prospects are being targeted by which marketing and recruiting interventions from which postsecondary institutions. Developing such a literature is an important task for organizational analyses of college access. Postsecondary institutions expend considerable resources on marketing and recruiting, just as they do on admissions and financial aid (Clinedinst et al., 2015). These behaviors affect student access and reveal insights about the processes by which colleges and universities "also choose students through strategic enrollment management practices" (Rhoades, 2014, p. 920). However, marketing and recruiting are understudied relative to scholarship on admissions and financial aid in part because data on marketing and recruiting behavior are often difficult to collect. This study takes an initial step toward a systematic literature on recruiting by investigating off-campus recruiting visits of public research universities. Off-campus recruiting visits are one of the most widely used enrollment management practices to identify prospective students (Ruffalo Noel Levitz, 2018). Admissions representatives travel across the country to host hotel receptions, attend college fairs, and visit high schools in efforts to generate student interest. Recruiting is not only a costly enrollment management practice but also one of the most efficient in soliciting admissions applications and persuading students to enroll (Ruffalo Noel Levitz, 2018). University admissions websites often post off-campus recruiting schedules advertised as colleges and universities "coming soon to a neighborhood near you." We utilized Web-scraping and public records requests to track off-campus recruiting visits for the 15 public research universities analyzed in the study. We draw on academic capitalism (Slaughter & Leslie, 1997; Slaughter & Rhoades, 2004) to ground recruiting visits as part of the marketization of public research universities and explore whether recruiting efforts "serve more privileged segments of the student market" (Slaughter & Rhoades, 2004, p. 292). Given that more privileged segments of the student market tend to be made up of affluent and predominately White students, we analyze the socioeconomic and racial characteristics of schools and neighborhoods that receive a recruiting visit in comparison with those that do not. Additionally, we investigate broader similarities and differences in off-campus recruiting patterns across the 15 universities in the study. The study's findings suggest that universities focus their efforts on recruiting out-of-state, affluent, White students. These findings exemplify what Slaughter and Rhoades (2004) characterize as public universities moving away from broadening access for underserved student populations and toward devoting more enrollment management energies on serving already privileged students. The study contributes to the discussion of how universities also choose students and lays the foundation for new scholarship that investigates how colleges search for and recruit prospective students. We position analyses of off-campus recruiting vis-à-vis scholarship from economics and sociology that describe behaviors of postsecondary institutions designed to exert control over enrollment. Within the economics of education, a supply-side literature theorizes the goals of (selective) colleges and universities, the contribution of student characteristics to these goals, and how universities attract students with desired characteristics (e.g., Brewer et al., 2002; Clotfelter, 1996, 2017; Doyle, 2010; Epple et al., 2003; Epple et al., 2006; McPherson & Schapiro, 1998; Winston, 1999, 2004). The most influential account of organizational goals is Bowen's (1980)"revenue theory of costs," which states that universities generate as much money as they can and expend all the money they make on the pursuit of "educational excellence" or prestige. Winston (1999) develops insight about the mechanisms linking student characteristics to the revenue theory of costs. He describes higher education as an industry that relies on a "customer-input technology" (Rothschild & White, 1995) in that student "quality" is an essential input to the pursuit of educational excellence by colleges. For example, SAT and ACT scores are a determinant of rankings and, because students educate one another, "the quality of the education any student gets from college depends . . . on the quality of that student's peers" (Winston, 1999, p. 17). Amid this customer-input technology, the "student-as-customer" pays a price for education and the "student-as-supplier" is awarded a "subsidy" for contributions to educational excellence. Subsidy is defined as the difference between university expenditure on the student and the net price paid by the student. Universities that offer students a greater subsidy are more successful in the competition for "high-quality" students. Therefore, universities have an incentive to make as much money as they can in order to attract the "best students" and, by consequence, move up the prestige hierarchy. This notion of subsidy as a means of attracting desired students helped motivate scholarship on "financial aid leveraging," which found that universities can strategically allocate institutional aid offers in order to influence application and enrollment decisions (e.g., DesJardins et al., 2006; Ehrenberg & Sherman, 1984; Epple et al., 2006; Hurwitz, 2012; Van der Klaauw, 2002). The burgeoning enrollment management profession applied these findings to develop institutional aid formulas that served university enrollment goals. Historically, selective institutions used institutional aid to meet the need of low-income admits (Ehrenberg, 2000; McPherson & Schapiro, 1998). As rankings became important in the 1990s, scholars found that selective institutions increased expenditure on merit aid—disproportionately awarded to high-income admits—as a means of increasing academic profile (e.g., Doyle, 2010; McPherson & Schapiro, 1998). By contrast, nonselective private institutions depended entirely on tuition revenue and used "tuition discounting" to fill the class (Lapovsky, 2001). As public universities became tuition reliant in the 2000s following declines in state appropriations, scholars analyzed financial aid models designed to maximize tuition revenue (e.g., Bosshardt et al., 2010; Hillman, 2012). Because nonresident tuition price is often more than double the price of resident tuition, many public universities developed institutional aid policies designed to attract nonresident students who are denied admission to their own state flagship university (e.g., Burd, 2015; DesJardins, 2001; Leeds & DesJardins, 2015). A limitation of the supply-side economics literature is the near-total focus on subsidies (i.e., financial aid) as a lever to attract students and an indicator of the value universities place on students. By contrast, the "enrollment funnel"—a hueristic used by enrollment managers, depicted in Figure 1—decomposes the enrollment process into the stages of prospects, inquiries, applicants, admits, and enrolled students (Campbell, 2017). The purpose of the enrollment funnel is to inform interventions at each stage. Financial aid is an intervention that primarily targets the enrollment decisions of admitted applicants. However, the enrollment management industry expends substantial resources on marketing and recruiting interventions that target earlier stages of the funnel. Universities host off-campus recruiting visits to identify prospects and to convert prospects and inquiries into applications (Ruffalo Noel Levitz, 2018). These recruiting interventions are the means by which universities search for students and craft the composition of their applicant pool. Although several analyses by economists evaluate the enrollment effects of specific recruiting interventions (e.g., Andrews et al., 2020; Dynarski et al., 2018; Miller & Skimmyhorn, 2018), few describe recruiting behaviors in the wild (but see Hanson, 2017; Hill & Winston, 2010). Therefore, the supply-side economics literature provides limited insight about which students universities choose to pursue. Within sociology, qualitative case study literature contributes holistic analyses of enrollment management that highlight the importance of recruiting (e.g., Cottom, 2017; Holland, 2019; Karabel, 2005; Khan, 2010; Killgore, 2009; Posecznick, 2017; Stevens, 2007). Analyses of selective institutions often build on "Max Weber's core insight that education has a dual character" (Stevens et al., 2008, p. 129), serving social mobility by redistributing access to privileged positions and simultaneously serving social reproduction by granting wealthy families access to colleges that confer privilege. Stevens's (2007) ethnography of the admissions office at a selective private liberal arts college reveals the mechanisms by which enrollment management serves social reproduction. Charged with moving up the rankings, the admissions office attempted to enroll students who raise academic profile and contribute to the tuition and donation revenue necessary for the pursuit of prestige. During the autumn "travel season," admissions officers visited selected high schools across the country "to spread word of the institution and maintain relationships with guidance counselors" because "the College's reputation and the quality of its applicant pool are dependent upon its connections with high schools nationwide" (Stevens, 2007, p. 53). The high schools they visited tend to be affluent schools—in particular, private schools—that enroll high-achieving students who can afford tuition and had the resources and motivation to host a successful visit. The college tended to visit the same schools year after year because recruiting depends on long-term relationships with high schools. Additionally, students are more likely to attend a college if their high school has a strong "feeder legacy" of enrolling students in the college (Wolniak & Engberg, 2007). Several studies analyze enrollment management at open-access institutions (e.g., Brint & Karabel, 1989; Clark, 1956; Cottom, 2017; Posecznick, 2017). Posecznick (2017) studied the "Ravenwood College," a private nonprofit college founded to serve working Black women. Consistent with analyses of tuition discounting by Lapovsky (2001), the survival of Ravenwood depends on the success of admissions officers in identifying prospects (e.g., through on- and off-campus recruiting events) and "converting" these prospects to enrolled students. Cottom (2017) studied for-profit institutions. Her book is simultaneously an ethnography of for-profit enrollment management and a political economy analysis of how the contemporary for-profit sector emerged vis-à-vis the broader higher education industry. Cottom (2017) argues that for-profits found a niche because "traditional" colleges and universities largely ignored communities of color and working adults. The for-profit sector viewed tight labor markets and military expansion—not traditonal colleges—as their primary competition for enrollment growth. The expansion and deregulation of federal student loans also enabled for-profits to earn considerable profit by expending substantial resources on recruiting and little on education. Complementing these analyses of recruiting from the perspective of colleges, Holland (2019) analyzes recruiting from the perspective of students at two racially and socioeconomically diverse public high schools. High school visits—including college fairs, instant decision events, and small-group representative visits—influenced where students applied and where they enrolled. This finding was weaker for affluent students with college-educated parents, who tended to be more concerned about college prestige and less taken with overtures from colleges, but was strong for first-generation students and students of color, who often said that high school counselors had low expectations for their college prospects and were too quick to recommend community college. This trust vacuum created opportunities for colleges that made these students "feel wanted." Therefore, Holland (2019) suggests that underserved student populations are particularly affected by which colleges and universities take the time to visit their high school. Despite generating important insights, the sociological literature on enrollment management has not systematically analyzed which student populations receive which recruiting interventions from which postsecondary institutions. Our study takes an initial step in this direction by investigating off-campus recruiting events by public research universities. Jaquette and Curs (2015) found that public research universities dramatically increased nonresident enrollment from 2002 to 2012 in response to declines in state appropriations. Apart from the adoption of "merit" aid programs for nonresident students (e.g., Burd, 2015), little is known about what public research universities do to attract nonresident students. Compared with resident students, nonresident students are more likely to be affluent, non–first generation, and identify as White or Asian (Jaquette et al., 2016). However, prior research has not investigated whether out-of-state recruiting efforts are explicitly targeting these student populations. Furthermore, prior research has not investigated how extensive or how equitable in-state recruiting efforts are. We draw on academic capitalism (Slaughter & Leslie, 1997; Slaughter & Rhoades, 2004) to frame our analysis of university recruiting visits within higher education's shift toward an academic capitalist knowledge regime and the push to "serve more privileged segments of the student market" (Slaughter & Rhoades, 2004, p. 292). Slaughter and Rhoades (2004) define academic capitalism as a higher education regime in which the logics of profit, privatization, and marketization influence the behavior of colleges and universities. Originally addressing how colleges and universities aim to generate revenue via patenting and technology transfer (Slaughter & Leslie, 1997), the theory has developed to explain how market and market-like behaviors "permeate almost all aspects of colleges and universities" (Slaughter & Rhoades, 2004, p. 305). The shift toward an academic capitalist knowledge regime is partly a function of changes in the external environment. Following World War II, during the height of the "national service" political economy (Stevens & Gebre-Medhin, 2016), higher education experienced decades of growth, government investment, and public support that expanded access for historically underserved students and encouraged the free flow of knowledge produced by scientific research (Stevens & Gebre-Medhin, 2016). This stability of student demand and government-provided resources contributed to a higher education system characterized by providing social mobility opportunities and serving the public good. However, the 1980s saw the emergence of external forces—including the decline in the college age population, erosion of state funding, and the expansion of federal student loan programs and federal legislation that commercialized university research—associated with the "market" political economy (Stevens & Gebre-Medhin, 2016). This configuration incentivizes colleges and universities to prioritize revenue concerns and engage in market activities to offset financial losses. Scholarship on the economics of education reviewed above (e.g., Clotfelter, 1996; Winston, 1999) suggests that elite colleges engage in self-interested admissions practices in order to remain competitive in the higher education positional arms race, such as leveraging institutional aid in order to attract high-achieving students that can increase institutional prestige. This prestige orientation toward students perpetuates the historical social reproduction function of higher education by granting privileged students disproportionate access to elite universities (Stevens et al., 2008). Consistent with this scholarship, academic capitalism suggests that public colleges and universities have also adopted a market orientation toward students as the result of economic, political, and social changes in the "new economy" (Slaughter & Rhoades, 2004, p. 280). While the prestige orientation targets high-achieving prospective students for institutional prestige, academic capitalism suggests a market orientation targets prospective students for "the extraction of revenue" (Slaughter & Rhoades, 2004, p. 279). The expansion of enrollment management as a core university structure has developed since the early 1990s primarily as an effort to craft incoming classes (Duffy & Goldberg, 1997; Hossler & Bean, 1990). This core structure integrates and leverages the activities of admissions, financial aid, marketing, and recruiting to achieve desired student enrollment outcomes (Cheslock & Kroc, 2012; Hossler & Bean, 1990). The economics literature describes many of these enrollment management practices (e.g., tuition discounting, "need-conscious" admissions, targeted recruiting) as part of selective colleges' efforts to enhance institutional prestige by seeking out and "buying the best" students (Clotfelter, 1996). However, these practices are increasingly used to craft incoming classes that contribute to the revenue interests of universities by commodifying and marketing the higher education experience toward the consumption culture of students from upper-middle-class families (Armstrong & Hamilton, 2013). In turn, Slaughter and Rhoades (2004) argue that the revenue concerns and the consumer-focused marketization that come with academic capitalism push universities to "serve more privileged segments of the student market" (Slaughter & Rhoades, 2004, p. 292). In their efforts to increase tuition revenue, universities seek out affluent prospective students with the ability to pay full tuition without financial aid (Armstrong & Hamilton, 2013). These affluent students are already privileged in the college search process by virtue of having the financial resources to purchase advantages such as private counselors and test preparation services that can increase their probability of admission (McDonough, 1997). The move toward an academic capitalism knowledge regime further advantages affluent students by pushing colleges and universities to also actively search for them. Academic capitalism suggests that race and ethnicity are also related to the ways in which student markets are segmented. Slaughter and Rhoades (2004) acknowledge that more privileged segments of the student market tend to be composed of predominantly White students. However, they do not foreground how the shift to an academic capitalist knowledge regime drives universities to focus more enrollment management energies on White students and less energies on Asian, Black, Latinx, and Native American students. Rather, academic capitalism presumes that raced effects in the pursuit of tuition revenue are primarily due to the differential distribution of wealth among racial and ethnic groups. We ground off-campus recruiting efforts within the revenue concerns and marketization central to an academic capitalism knowledge regime by analyzing whether and to what extent recruiting visits serve more privileged student markets. Drawing on Slaughter and Rhoades's (2004) conceptualization of which students make up privileged segments of the student market, we develop broad expectations about the socioeconomic and racial characteristics of schools and communities that are likely to receive a recruiting visit across universities. We expect public research universities to focus recruiting efforts on affluent schools and communities in order to pursue prospective students who contribute to institutional revenue goals. We also expect universities to make a substantial number of out-of-state recruiting visits because nonresident students typically pay 2 to 3 times the amount of tuition that resident students pay (Zinth & Smith, 2012). Universities are also likely to disproportionately visit private high schools as these schools tend to enroll high-achieving, affluent students who can afford to pay full tuition (Stevens, 2007). Although schools in affluent communities tend to perform well on traditional metrics of academic achievement (e.g., Dixon-Roman et al., 2013), we expect that universities will recruit in affluent schools net of achievement as they seek full-pay students who do not require need-based or merit-based financial aid. The extent to which universities visit affluent and out-of-state schools is also likely to vary across states with differing state funding generosity and demographic changes, as these are important factors triggering a market orientation toward prospective students. Following prior research on the relationship between state population trends and resident enrollment demand (Grawe, 2018; Winters, 2012), we expect that universities in states with small or declining high school graduate cohorts may focus recruiting visits to out-of-state schools and communities given lack of student demand within their respective states. Universities with generous state appropriations and state grant aid are likely to make more in-state visits without preferences for affluent schools and communities. On the other hand, universities with weak state financial support are likely to make more out-of-state visits to recruit nonresident students and visit more affluent schools and communities to recruit full-pay students. However, we expect that universities in states with nonresident enrollment caps (e.g., California, North Carolina) will likely make fewer out-of-state visits. We also expect universities to focus recruiting visits in predominantly White schools and communities. Academic capitalism suggests that the focus on visiting predominantly White schools and communities may be the result of universities concentrating efforts on recruiting affluent students. While schools and communities are stratified by both income and race; the racial and ethnic composition of schools and communities is a critical and independent factor from socioeconomic factors as it relates to educational opportunities (DeCuir & Dixson, 2004). To investigate whether universities may be recruiting in predominantly White schools and communities in response to the socioeconomic characteristics rather than the racial and ethnic composition of those localities, we examine whether visits are more likely to be focused in predominantly White schools and communities even after controlling for income. Given state pressures for public research universities to provide equitable access for all state residents (Gerald & Haycock, 2006), we expect out-of-state recruiting to be more disproportionately focused on White schools and communities than in-state recruiting. This study uses a quantitative case study design to explore off-campus recruiting visits by public research universities. While often considered a qualitative research design, case studies can focus exclusively on either quantitative or qualitative data analysis (Korzilius, 2010; Yin, 2014). A "quantitative case study" refers to a case study where quantitative data are the primary source of evidence (Korzilius, 2010). More specifically, we utilize a multiple case study design, which treats each university as a separate analysis, rather than as one observation within a large-N analysis (Yin, 2014). Many universities advertise off-campus recruiting events on their admissions websites (e.g., "coming to a neighborhood near you" links). Python, a general purpose programming language, was used to collect recruiting visit data by Web-scraping university admissions websites. Python programs were automated to "scrape" all information from URLs containing recruiting events once a week from January 1, 2017, to December 31, 2017, thereby capturing recruitment of spring juniors and fall seniors. As part of a broader project, our data collection sample is drawn from the population of public research-extensive universities (2000 Carnegie Classification). Out of all public research–extensive universities (N = 101), the project collected data for those that posted off-campus recruiting events on their admissions websites (N = 49). For each university in this data collection sample, we investigated the entire university website, searching for URLs that contained data on off-campus recruiting events. Since URLs containing data on off-campus recruiting events often change (e.g., a university creates a new URL or changes the formatting of an existing URL), we completed this investigation process for each university every 2 months. We also scraped data about participation in national college fairs from the National Association for College Admission Counseling website and data about participation in group travel tours. We categorize off-campus recruiting events based on event type, host, and location. Event type includes college fairs (in which multiple colleges attend), day-time high school visits, group travel visits, formal admissions interviews, admitted student events, and committed student events. Event hosts include paid staff, paid consultants (e.g., independent "regional recruiters" contracted by universities), alumni, and current students. Event locations include high schools, community colleges, hotels, conference/convention centers, and other public places (e.g., cafes). Our research defines off-campus recruiting events as those that focused on soliciting undergraduate admissions applications and were hosted by paid personnel or consultants at any off-campus location. This definition excludes admitted and committed student events, but includes guidance counselor events. We excluded formal one-on-one interviews because these events are focused on determining the admission of one particular student rather than open events aimed at soliciting applications from many prospective students. We excluded events hosted by alumni or student volunteers as practices allocated to paid staff are better indicators of organizational priorities than those allocated to volunteers (Thompson, 1967). We draw on purposeful sampling to identify and select the case study sample. Purposeful sampling is a strategy used to yield cases that are "information-rich" (Patton, 2002). This most often involves sampling individuals or cases that provide high-quality data about the phenomenon of interest. We selected cases from the broader data collection sample (N = 49) based on the completeness and quality of recruiting data collected. From prior research (Holland, 2019; Ruffalo Noel Levitz, 2017, 2018; Stevens, 2007) and conversations with admissions professionals, we find that all universities convene three broad types of off-campus recruiting events: (1) receptions or college fairs at hotels and convention centers, (2) evening college fairs at high schools, and (3) day-time visits at high schools. Fifteen of the 49 universities we scraped recruiting data on posted all three broad types of off-campus recruiting events on their website. These 15 universities make up our case study sample. Table 1 shows the 15 universities in this study across various institutional characteristics. While our aim is not to generalize the population of public research universities, this sample does reflect a distribution of institutional characteristics generally representative of the many different "types" of research universities. Our sample includes public research universities that are highly prestigious and selective (e.g., UC Berkeley, Georgia), as well as those that are relatively open access (e.g., Kansas, CU Boulder, Cincinnati). Some universities, like Alabama, exemplify extremes of national trends in declining state appropriations and increases in nonresident enrollment. On the other hand, other universities are located in states that provide generous state funding for public higher education (e.g., NC State) or have relatively low proportions of nonresident students (e.g., Georgia, Rutgers). Based on principles of case study research, we triangulate Web-scraped recruiting data using recruiting data requested from universities. Data triangulation in case studies attempts "to collect information from multiple sources but aimed at collaborating the same finding" (Yin, 2014, p. 120). We issued requests to universities for information on all their 2017 off-campus recruiting events. As a courtesy, we first requested data from the Office of the Vice President for Enrollment Management. If our request was denied or ignored, we issued a formal public records request under the Freedom of Information Act. Public records requests were denied by some universities due to state statutes that only permit public records requests from state residents. Of the 15 universities analyzed in this study, we received requested data from 11 universities. The other four universities were either not required by state statutes to provide data or refused to provide data. For universities that sent us data, the analyses below use "requested" rather than "scraped" data given we consistently found that "requested" data had more events. Broad patterns were similar across requested data versus scraped data, and results based on scraped data are available on request. Appendix A details the data sources collected and used for analyses across universities in the study, as well as data quality checks conducted across all sources. Following scholarship on multiple case study methodology (e.g., Eisenhardt, 1989; Yin, 2014), we conducted within-case and cross-case analyses. Our within-case analyses consist of quantitative descriptive analyses, including univariate statistics of recruiting visits, bivariate descriptive statistics comparing the characteristics of schools that received a visit with those that did not, and linear probability regression models that investigate whether relationships between the probability of receiving a visit and independent variables of interest persist after controlling for other factors associated with receiving a visit. After identifying broad patterns in the characteristics of schools that received a visit versus those that did not within each case, we compared results across cases to identify similarities and differences in recruiting efforts across universities in the study. Decisions about which variables and relationships to examine were driven by our academic capitalism conceptual framework. In particular, Slaughter and Rhoades (2004) suggest that the more privileged segments of the student market tend to be made up of affluent and predominately White students. Additionally, recent research highlights nonresident enrollment growth as a revenue generation response to diminished state support (Jaquette & Curs, 2015). Therefore, we focus our analyses on understanding the economic and racial characteristics of localities that receive recruiting visits versus those that do not, with separate analyses for in-state and out-of-state recruiting. We defined nonvisited schools for in-state analyses as all schools that did not receive a visit within the state where the university is located. Nonvisited out-of-state schools include all schools in states that received at least one visit to a public or private high school from the university. For example, Nebraska visited high schools in 13 different states besides their home state. In our out-of-state public high school analyses, only the 6,423 public high schools in those 13 states are included in the analysis sample for Nebraska, and an indicator is used to identify which schools received at least one recruiting visit by the university. Thus, the total number of out-of-state high schools differs across universities in our analyses. We made this decision because it is unhelpful to compare visited out-of-state schools with nonvisited schools in states where the university did not make a single visit. We also conducted a robustness check of results to explore whether a more conservative approach to constructing the sample of nonvisited out-of-state schools would yield similar patterns in the characteristics of schools that received a visit in comparison with those that did not. Rather than including all out-of-state schools in a state where at least one high school visit occurred, the robustness check only included schools in the Core-Based Statistical Area (CBSA) where at least one high school visit occurred. CBSAs encompass both metropolitan (urbanized area of 50,000+ residents) and micropolitan areas (urban cluster of at least 10,000 but less than 50,000 residents; U.S. Census Bureau, 1994). This conservative approach would reason that recruiters visiting one CBSA could visit other schools in that same area but it may not be reasonable to assume that the recruiter could visit schools throughout the entire state. Broad patterns in the CBSA robustness check were largely consistent with our state-level approach to constructing the nonvisited out-of-state sample for each university. However, the CBSA analysis excluded some rural recruiting visits in localities that are not part of a metropolitan or micropolitan area. For this reason, we present main results for the state-level analysis and briefly describe notable differences between our main analysis and the CBSA robustness check within the Findings section. We also provide all figures and tables of results for the robustness check at the CBSA-level as supplemental material. We draw on various secondary sources for data on school and community characteristics. We obtained data on public high schools from the Common Core of Data, data on private high schools from the Private School Universe Survey, and data on the universities in the sample from the Integrated Postsecondary Education Data System. Achievement data for public high schools is from EdFacts Assessment Proficiency data collected by the National Center for Education Statistics. Demographic and economic data on communities were collected from the U.S. Census Bureau's American Community Survey. Our dependent variables are visit counts and dummy indicators for whether a locality (i.e., public high school, private school, or ZIP code for nonschool visits) received a recruiting visit by each of the universities in the study. Our primary independent variables of interest measure the economic and racial characteristics of schools. We measure income for public high schools by using the average median household income of the ZIP code where the school is located., Race variables include the percentage of student enrollments from each racial–ethnic group, as well as the cumulative percentage of Black, Latinx, and Native American students for public high schools. Other variables are used to account for factors that would affect whether or not a school is likely to receive a visit. Because prior research on access inequality suggests that low-income students and students of color are less likely to attend college due to lower academic achievement (Howard, 2015; Reardon & Galindo, 2009; Sheperd & Sutcliffe, 2011), we include a school-level measure of achievement for public high schools. Achievement is measured by the number of students scoring at or above proficient levels in mathematics on state high school exit exams. Prior research also suggests that recruiting efforts are likely to target larger schools, which have a larger pool of prospective students (Hoxby & Avery, 2013; Ruffalo Noel Levitz, 2018). We measure school size using the number of 12th grade students. Because some populations of students are more likely to attend colleges in closer proximity to home (Turley, 2009), we include measures of distance in miles between the location of the university and the high school. We include a categorical measure indicating whether a school is located in a city, suburb, or rural area because traveling to rural communities may be more expensive and time-consuming. Last, we include an indicator that categorizes school type as a regular school, charter school, or magnet school because magnet schools may be more likely to have a larger number of high-achieving students and the capacity to host recruiting events. We acknowledge several data limitations. First, off-campus visits encompass only one university recruiting effort. Universities may also be recruiting students via other interventions (e.g., direct mailings, emails, specific outreach programs). Second, despite our best efforts to collect and triangulate off-campus recruiting data from more than one source to validate quality and completeness, our data may not capture all off-campus recruiting events by each university. Third, the National Center for Education Statistics collects high school finance data and personnel data on guidance counselors at the district level rather than at the school level. Therefore, our analyses do not account for differences in high school–level capacity to host recruiting visits. Last, we are limited in using the number of students scoring at proficient levels on state math assessments as a crude measure of school-level academic achievement because no other comprehensive, national data set reports student achievement measures at the school level. Figure 2 presents the number of off-campus recruiting visits by visit type and by in-state versus out-of-state location for each university in the study. In 12 of the 15 cases, universities made more out-of-state recruiting visits than recruiting visits within their respective states. Seven universities made more than twice as many out-of-state visits (Alabama, Arkansas, CU Boulder, Kansas, UMass Amherst, Pittsburgh, South Carolina). Alabama showcased the upper extreme of making more out-of-state than in-state recruiting visits. Alabama's 3,900 out-of-state visits accounted for 91% of the university's total visits, a proportion higher than any other case. This coincides with shifts in the university's funding and enrollment that also represent an extreme example of trends happening at many public research universities. We describe trends in revenue sources, enrollments, and state population demographics in detail for individual cases throughout our analyses of off-campus recruiting visits and provide figures of these changes over time for each university in Supplemental Appendices D through H. Since 2008, Alabama's state funding has declined substantially, while tuition revenue has increased dramatically. These shifts are partly explained by a shrinking proportion of 18-year-olds in the state of Alabama and substantial growth in the university's nonresident enrollment over the past 10 years. Three other cases also made a relatively large number of out-of-state visits that account for roughly three quarters or more of total recruiting efforts: Arkansas (78%), Pittsburgh (74%), and South Carolina (81%). NC State, UC Berkeley, and UC Irvine were the only cases in the study to deviate from making more out-of-state than in-state recruiting visits. NC State made the fewest total visits across all universities, 33% of which were to schools and communities outside of North Carolina. UC Berkeley's 471 and UC Irvine's 172 visits to states outside of California accounted for 42% and 18% of total recruiting efforts, respectively. It may not be a coincidence that the three universities with more in-state than out-of-state recruiting visits are the only cases in the study that face strong legislative pressures to cap nonresident enrollment. The University of North Carolina system has an 82–18 enrollment cap that has been in place since the late 1980s. In order to avoid a reduction in operating budgets, at least 82% of new freshmen at the University of North Carolina system schools must be state residents. Along with a cap on nonresident enrollment, state funding for public colleges and universities in North Carolina has kept pace with growing student enrollments over the past 10 years. More so, North Carolina is one of the top-ranked states in per-student funding (Leins, 2020), providing nearly $19,000 per full-time equivalent (FTE) student in 2017. UC cases in the study also face strong legislative pressures to cap nonresident enrollment, although these pressures are coupled with dramatic declines in state funding over the past 10 years. After the California legislature introduced a bill in 2017 to cap nonresident enrollment, the UC system voluntarily implemented an 18% nonresident enrollment cap for campuses that enrolled less than 18% nonresident undergraduates and a nonresident enrollment freeze for campuses with more than 18%. With the highest nonresident enrollment cap (24%) and most dramatic declines in state funding per FTE student over the past 10 years (from $22,000 in 2004 to $12,000 in 2017), UC Berkeley made more than twice the number of out-of-state visits than UC Irvine, which has a lower enrollment cap (19%) and experienced less dramatic declines in state funding per FTE student (from $13,000 in 2004 to $10,000 in 2017). Out-of-state visits by all universities in the study were concentrated in metropolitan areas. Figure 3 shows maps of national recruiting patterns for each university. Out-of-state recruiting visits were clustered in large metropolitan areas around the country, such as New York City, Los Angeles, Chicago, Dallas, and Washington, D.C. Rural areas within these states were largely ignored. For example, UMass Amherst made 72 visits to the Los Angeles and 23 visits to the San Francisco metropolitan areas but made no visits to schools in the Central Valley or to any noncoastal area of California. Other metro areas that were also frequently visited across cases include Houston, Boston, San Francisco, Denver, Atlanta, Philadelphia, Orlando, Baltimore, and Miami. Figure 4 presents out-of-state and in-state recruiting visits by rural versus urban location and suggests that the majority of out-of-state visits across all cases were less likely to be located in rural areas in comparison with in-state visits. While visits to public high schools made up the largest proportion of both out-of-state and in-state recruiting efforts, out-of-state recruiting included a large number of private high school visits for most universities. Do out-of-state visits include a disproportionate number of visits to private high schools? To answer this question, we compared the actual number of out-of-state private and public high schools visited with the hypothetical number of private and public high schools that would have been visited if each school had an equal probability of receiving a visit. For example, if public and private high schools had an equal probability of receiving a visit, 353 of Georgia's 441 total out-of-state school visits would have been to public high schools versus 88 visits would have been to private schools. Yet Georgia's 192 private school visits were more than double this number. Applying this calculation to all universities, Figure 5 shows the actual number of out-of-state visits to public and private schools compared with our calculated, proportional number of visits under equal probability of receiving a visit. For 12 of the 15 universities, the actual number of out-of-state private school visits exceeded the estimated number of visits under equal probability. Rather than focusing solely on the number of in-state recruiting visits, we analyze universities'"coverage" of public high schools in their respective states. Public universities hold unique responsibilities in providing educational opportunities to state residents. Some universities in populous states and those that are part of centralized higher education systems distribute this responsibility geographically, but we begin by defining coverage as the proportion of visited public high schools to the total number of schools within the state. For example, there are a total of 400 public high schools in New Jersey, 63% of which received at least one recruiting visit by Rutgers. Figure 6 shows coverage of in-state public high schools across all universities in the study. Not surprisingly, universities in less populous states had better coverage of public high schools (e.g., South Carolina, Nebraska, Kansas) than universities in very populous states (e.g., UC Irvine, UC Berkeley, Stony Brook). Nebraska was the only university in the study with extensive in-state coverage. The university visited nearly nine of every 10 public high schools in the state, suggesting that making substantial out-of-state recruiting visits and having good in-state coverage of public high schools may not be mutually exclusive. For cases in large, populous states, we also consider coverage of public high schools in their local metropolitan area. Restricting coverage to local in-state public high schools in the San Francisco and Los Angeles metropolitan areas, we find that UC Berkeley and UC Irvine had 45% and 26% coverage, respectively. Stony Brook visited 35%, Pittsburgh visited 50%, and NC State visited 31% of in-state schools in their local metropolitan areas. These figures suggest that some universities in larger states may be focusing in-state recruiting efforts within their local metropolitan areas rather than across their respective states. Because visits to public high schools comprise the vast majority of recruiting efforts across all universities, we focus on analyzing the economic and racial characteristics of public high schools that receive a visit versus those that do not. Figure 7 presents the average median income of public high schools that received and did not receive visits by in-state versus out-of-state location for each university in the study. For nearly all universities, visited schools tended to be located in more affluent communities than nonvisited schools. However, the magnitude of this disparity tended to be much greater for out-of-state visits than in-state visits. Out-of-state recruiting visits to public high schools were concentrated in highly affluent areas, a finding that is true across all cases in the study. For example, UMass Amherst visited out-of-state public high schools in ZIP codes where the average median household income was $115,000, whereas schools that did not receive a visit were located in areas with an average median income of $64,000. This income disparity between visited and nonvisited out-of-state schools was highest for NC State, which visited schools in ZIP codes where the average median income was $121,000 in comparison with $67,000 for schools that did not receive a visit. This relatively large disparity may be a function of NC State making the fewest out-of-state visits across all universities in the study. By contrast, the income disparity for Alabama's extensive out-of-state recruiting efforts was relatively smaller ($90,000 for visited schools compared with $60,000 for nonvisited schools). Recruiting by Nebraska exhibited the lowest income disparity between out-of-state public high schools that received a visit ($85,000) and those that did not ($61,000), likely the result of focusing visits to states in the Midwest region as opposed to visiting the wealthiest metropolitan areas across the country like most of the other universities in the study. We test whether the relationship between income and receiving an out-of-state recruiting visit persists after controlling for other factors that could affect whether a school receives a visit. Appendix B reports linear probability models predicting the likelihood of receiving a visit for out-of-state public high schools. These models include measures of public high schools' income, the racial composition of the student body, academic achievement, size, school type, locale, and distance from the university (although coefficients are only shown for independent variables of interest for space considerations)., Across all universities, public high schools in wealthier communities were significantly more likely to receive an out-of-state recruiting visit than schools in low-income communities even after controlling for factors that may influence the likelihood of receiving a visit. For example, looking at the column of results for Rutgers in Appendix B, out-of-state public high schools located in communities with average median incomes greater than $200,000 were nearly 40% (p < .001) more likely to receive a recruiting visit by Rutgers than schools located in communities with average median incomes less than $50,000 (reference category). Generally, the magnitude of the relationship between income and the probability of receiving a visit across all universities in the study was larger for higher income bands than lower income bands. This pattern indicates that public high schools in the wealthiest communities had the greatest probability of receiving a visit. Results from the CBSA-level robustness check exhibited a similar income disparity between visited and nonvisited schools when only out-of-state schools in CBSAs that received at least one visit were included. However, the income disparity between visited and nonvisited schools was not quite as large as the state-level analysis. For example, the CBSA-level analysis for NC State shows a $44,000 average median income disparity between visited and nonvisited schools ($121,000 average median income for visited schools compared with $77,000 average median income for nonvisited schools), whereas the state-level analysis resulted in a $54,000 disparity (Figure 7). This pattern was consistent across all universities in the study. Regression results also exhibited this general pattern where the CBSA-level robustness check produced coefficients slightly lower in magnitude than the state-level analyses. Figure 7 suggests that recruiting visits to in-state public high schools also demonstrated similar patterns of income bias for most universities in the study. With the exception of NC State and UC Irvine, the average median income of visited in-state public high schools was greater than that of nonvisited schools, although the income disparity was relatively smaller in comparison with out-of-state recruiting visits. For example, CU Boulder visited in-state public high schools in ZIP codes where the average median income was $75,000, whereas schools that did not receive a visit were located in areas with an average median household income of $61,000. In-state recruiting efforts by NC State and UC Irvine included visits to high schools that were on average slightly less affluent than schools that did not receive a visit. As seen in Appendix C, the relationship between income and receiving an in-state visit persisted for most universities in the study even after controlling for other factors, although regression coefficients are smaller in magnitude than for out-of-state visits (Appendix B). For example, the column for South Carolina in Appendix C shows that in-state public high schools located in communities with average median incomes between $100,000 and $149,000 were about 27% (p < .01) more likely to receive a recruiting visit than schools located in communities with average median incomes less than $50,000. However, we find that in-state public high schools in wealthier communities were not significantly more likely than lower income schools to receive a visit by Alabama, Rutgers, Arkansas, and Nebraska after controlling for racial composition, achievement, size, locale, and distance from the university. In addition, consistent with descriptive results (Figure 7), schools in more affluent communities were significantly less likely to receive a visit by NC State and UC Irvine. Across all cases, out-of-state recruiting visits were concentrated in public high schools with lower percentages of Black, Latinx, and Native American students than nonvisited schools. Figure 8 shows the racial composition of the average visited and nonvisited out-of-state public high school. Visited high schools had a substantially lower percentage of Black, Latinx, and Native American students than nonvisited high schools. For example, Georgia visited out-of-state public high schools where Black, Latinx, and Native American students made up, on average, 31% of total student enrollments, whereas Black, Latinx, and Native American students made up, on average, 45% of total enrollments at schools that did not receive a visit. This is particularly troubling as the state of Georgia has experienced steady increases in high school graduates since the early 2000s, partly a factor of the rapid growth in the number of Latinx high school graduates in the state (see Supplemental Appendix G). These findings coincide with research that finds that public research universities become less racially diverse as they increase nonresident enrollment, as nonresident students are less likely to identify as Black or Latinx compared with resident students (Jaquette et al., 2016). This racial disparity between visited and nonvisited out-of-state public high schools was the highest for Rutgers, which visited public high schools where Black, Latinx, and Native American students made up only 27% of total enrollments in comparison with 51% of total enrollments at nonvisited schools. However, this difference was relatively modest for some universities (e.g., NC State, UC Irvine, Nebraska). We find a significant and negative relationship between enrollment from Black, Latinx, and Native American students and the probability of receiving an out-of-state recruiting visit for most universities in the study even after controlling for other factors (Appendix B). This negative relationship tended to be larger for schools with higher percentages of Black, Latinx, and Native American students. For example, Appendix B shows that out-of-state public high schools with 40% to 59% enrollment from Black, Latinx, and Native American students were 3% (p < .001) less likely to receive a visit from UMass Amherst than schools with less than 20% enrollment from these students. This probability increased to 4% (p < .001) less likely for schools with 60% to 79% enrollment, to 6% (p < .001) less likely for schools with more than 80% enrollment from Black, Latinx, and Native American students. However, the magnitude of coefficients on racial composition were generally smaller than the magnitude of coefficients on income. Probability changes on the likelihood of out-of-state schools receiving a visit ranged from 1% to 9% for race coefficients, whereas income coefficients ranged from 1% to 77%. Nebraska and Cincinnati were the only cases in the study for which the relationship between Black, Latinx, and Native American student enrollment and the probability of receiving an out-of-state recruiting visit was not significant after controlling for other factors. However, the CBSA-level robustness check—where only out-of-state schools in CBSAs that received at least one visit were included in the analysis— found larger racial disparities between schools that received a visit and those that did not across all universities in the study. For example, Georgia visited out-of-state public high schools where Black, Latinx, and Native American students, on average, still made up 31% of total student enrollments. However, the average percentage of Black, Latinx, and Native American students at nonvisited schools increased to 50% (compared with 45% at the state level, as seen in Figure 8). CBSA-level regression models also produced coefficients higher in magnitude than the state-level robustness checks. For example, out-of-state high schools with more than 80% enrollment from Black, Latinx, and Native American students were 18% (p < .001) less likely to receive a visit from UMass Amherst than schools with less than 20% enrollment from these students (in contrast to 6% less likely in the state-level analysis shown in Appendix B). Additionally, the relationship between Black, Latinx, and Native American student enrollment and the probability of receiving an out-of-state recruiting visit remained statistically significant (p < .05) after controlling for other factors for both Nebraska and Cincinnati in the CBSA-level analysis. Figure 8 also suggests that racial characteristics of in-state recruiting visits differ across universities. Nine of the 15 universities visited in-state public high schools with, on average, smaller proportions of Black, Latinx, and Native American students than nonvisited schools. However, in comparison with out-of-state visits, the difference in racial composition of visited schools versus nonvisited schools was relatively modest. For example, Pittsburgh exhibited the highest racial disparity for in-state recruiting by visiting public high schools where Black, Latinx, and Native American students made up only 16% of total enrollments in comparison with 29% of total enrollments at nonvisited schools. However, for the other universities, the difference in Black, Latinx, and Native American student enrollments between visited and nonvisited public high schools was less than 3 percentage points (South Carolina, CU Boulder, Cincinnati, Stony Brook, Georgia, Berkeley). Four cases in the study visited in-state public high schools that were, on average, more racially diverse than schools not visited (Rutgers, UC Irvine, Kansas, Nebraska). Additionally, we find that the negative relationship between the racial composition of high schools and the probability of receiving an in-state visit persisted for four universities (Alabama, Pittsburgh, Stony Brook, NC State) after controlling for other factors (see Appendix C). Twelve of the 15 universities in our sample made more out-of-state than in-state visits. While universities differ from one another in terms of which regional markets they visit, all universities focused out-of-state visits on affluent, predominantly White public high schools and private schools in populous metropolitan areas. The relationship between income and the probability of an out-of-state high school receiving a visit persisted across all universities even after controlling for other factors that could affect whether a school receives a visit. The relationship between the percentage of Black, Latinx, and Native American student enrollment and the probability of receiving an out-of-state visit also persisted for all but two universities in the study (Nebraska and Cincinnati). In-state recruiting visits also tended to privilege affluent communities, although income differences between visited and nonvisited schools were relatively modest in comparison with the income disparity found in out-of-state visits. More than half of the universities in the study also visited in-state high schools with smaller proportions of Black, Latinx, and Native American students than schools that did not receive a visit. While the relationship between income and the probability of an in-state high school receiving a visit persisted across most universities in the study after controlling for other factors, the relationship between the percentage of Black, Latinx, and Native American students and the probability of receiving an in-state visit diminished for all but four cases. Our analyses revealed substantial variations across universities' recruiting efforts, some of which appears to be related to external factors. For example, having a nonresident enrollment cap was associated with making fewer out-of-state visits (UC Berkeley, UC Irvine, NC State). For UC Irvine and NC State, more generous state funding appears to be associated with fewer out-of-state visits and greater socioeconomic and racial equity in in-state visits. Results also suggest that strong out-of-state recruiting efforts are not necessarily associated with weak in-state recruiting. For example, Nebraska and South Carolina made a large number of out-of-state recruiting visits and also visited the majority of public high schools in their respective states. However, a great deal of variation across universities is not clearly connected to external factors. For example, UC Irvine and UC Berkeley face similar external environments, but in-state visits by UC Irvine demonstrate greater socioeconomic and racial equity. Although they are similarly prestigious, Kansas made a substantial number of visits to California, while Nebraska did not. Several universities in our sample face the same dual challenge of declining state support and challenging demographics, but Alabama stands alone in the number of out-of-state recruiting visits, the number of regions visited across the country (e.g., Northeast, Midwest, South, Southwest), and the number of metropolitan statistical areas visited within each region. The study's findings extend the economics of education literature. Beyond just leveraging financial aid to buy the best students (Clotfelter, 1996, 2017; Winston, 1999; Zimmerman, 2003), our study suggests postsecondary institutions also craft their incoming classes by focusing recruiting efforts in visiting affluent, predominantly White prospective students across the country. Findings from this study also support previous research that draws on the integration of the national college market to explain the sorting of students among colleges. Hoxby (1997, 2009) argues that declines in costs of travel and information have shifted students' college choices to be driven less by distance or rising selectivity and more by a college's resources and student body. Our study makes a supply-side contribution to this research by investigating how public research universities, institutions that do not have the most resources and are not the most selective, incite student demand by taking advantage of the integration of student markets. The focus on recruiting affluent, out-of-state students across the majority of university cases in the study also suggests public research universities may be recruiting prospective students for resources rather than prestige. These findings also forward the discussion of how strategic enrollment management practices shape student markets. Slaughter and Rhoades (2004) argue that "colleges and universities are not simply subject to the power of consumer choice; they engage in aggressive marketing to strategically shape those consumer choices" (p. 292). Findings exemplify what Slaughter and Rhoades (2004) characterize as a college market that is segmented by which prospective students are preferred and which are overlooked. Universities in this case study devote a greater share of enrollment management energies on recruiting out-of-state, affluent, White students and a lesser share on recruiting rural, in-state, low-income, Black, Latinx, and Native American students. Through an academic capitalist lens, results suggest that universities in the study prefer to recruit prospective students in more privileged segments of the student market and overlook segments made up of historically undeserved student populations. These findings provide new empirical detail on how universities' recruiting efforts segment and sharpen class and racial divisions in prospective student markets. Our findings also contribute to the modest literature that analyzes recruiting behavior. The schools and communities visited by public research universities were dissimilar to those visited by for-profits (Cottom, 2017) and a private nonprofit adult education college (Posecznick, 2017). However, out-of-state visits by public research universities were broadly similar to visits by a selective private liberal arts college (Stevens, 2007) in that they focused on affluent, predominantly White, and disproportionately private high schools. Though not analyzed for this article, we also collected off-campus recruiting visit data for a sample of selective private colleges and universities (https://map.emraresearch.org/). This interactive map of recruiting visits shows broad similarities between high schools visited by selective private institutions and out-of-state high schools visited by public research universities. By contrast, in-state recruiting visits by public research universities—although they skew somewhat toward affluent communities—are much more racially and socioeconomically representative than visits by selective private institutions. The findings and limitations of this study motivate future research. As the first study in a larger project examining off-campus recruiting by colleges and universities, we sought to investigate broad similarities and differences in recruiting patterns across a large number of public research universities. However, the purposeful sample of 15 universities analyzed in this study is, on one hand, inappropriate for making claims beyond our analysis sample and is, on the other hand, too large for deep analyses of particular universities. Future research could utilize public records requests to collect data from a representative sample in order to make claims about a broader population of public universities. The variation in findings across universities in the study, such as the dramatic differences in recruiting by Alabama compared with any other university in the study, calls for a more in-depth case study analysis with fewer universities. Future research, using purposeful or representative samples, should also examine how recruiting patterns vary across types of colleges and universities (e.g., regional universities and public research universities). Additional research is also needed to understand the spatial and racial dynamics of recruiting visits. Recent scholarship on the geography of college opportunity applies a spatial lens to a well-established college choice literature (Dache-Gerbino et al., 2018; Hillman, 2016), finding that the location of colleges and universities varies widely along the racial and socioeconomic characteristics of neighborhoods. Given universities are not immobile in their efforts to provide access, a spatial analysis of recruiting visits would make a significant contribution to exploring whether and to what extent the enrollment management behaviors of colleges and universities reinforce the geography of unequal college opportunity. Future research should also ground the enrollment management practices of universities within more critical frameworks that do not neutralize or deracialize the location of recruiting visits (e.g., assuming visiting predominantly White schools and communities as the result of the differential distribution of wealth among racial and ethnic groups). Given that results from the out-of-state CBSA-level robustness check indicated larger racial disparities between visited and nonvisited schools compared with the state-level analysis, future research would benefit from exploring recruiting visits in metropolitan areas and ground analyses within the historical disenfranchisement of people of color through neighborhood segregation and stratification. Perhaps the most important contribution of our study is demonstrating the feasibility of new approaches to collecting data on recruiting behavior. Access to data has been a barrier to empirical research on recruiting, particularly quantitative analyses. Our study is the first to use Web-scraping and public records requests to collect quantitative data on recruiting visits. Recent scholarship also utilizes experimental audit designs to analyze how admissions officers respond to inquiry emails from fictitious prospects (Hanson, 2017; Thornhill, 2018). Other feasible data collection strategies include streaming data from social media (e.g., Twitter) and purchasing data on digital advertising. Scholarship on enrollment management can flourish by embracing these, and other, innovations in data collection. Given the abundance of data and the dearth of empirical research, we close by calling for the development of a systematic literature on marketing (which includes scholarship on recruiting). Scholarship on marketing is valuable for two broad reasons. First, marketing decisions by colleges affect access for students, as demonstrated by prior research (e.g., Cottom, 2017; Dynarski et al., 2018; Stevens, 2007). Over the past 20 years, advances in marketing and market research have permeated all consumer industries, including higher education. Because most higher education programs are open access or close to it (even prestigious universities offer relatively open access programs, in addition to their selective programs), one could argue that marketing affects student access more than admissions. Second, marketing behavior reveals insights about organizational priorities. Scholarship on organizational behavior in economics and sociology argues that organizations expend resources on goals they care about. Within economics, Bowen (1980) argues that research universities care most about the pursuit of prestige and Winston (1999) argues that universities pursue prestige, in part, by enrolling students that contribute to the academic profile. Universities expend considerable resources wooing students with desired characteristics (Hossler & Bean, 1990), for example, offering generous financial aid packages (Epple et al., 2003). Within sociology, theories of organizational behavior argue that organizations expend substantial resources on the goals they care most about (Meyer & Rowan, 1977; Thompson, 1967; Weber et al., 2009). On the other hand, organizations "symbolically adopt" goals demanded by external stakeholders by engaging in highly visible actions (e.g., speeches, formal policy adoption) without substantively affecting how resources are allocated (Meyer & Rowan, 1977). Postsecondary institutions expend substantial resources on marketing. Theories of organizational behavior suggest that knowing how universities allocate marketing resources among prospective students indicates which student characteristics they are pursuing. By systematically investigating the diverse set of marketing interventions used by different postsecondary institutions, we can make assertions about organizational priorities. Scholarship on marketing will be more impactful if it is informed by practice within the enrollment management industry. The enrollment funnel (depicted in Figure 1) identifies the stages of leads/prospects, inquiries, applicants, admits, and enrolled students, in order to inform targeted interventions. One topic for future research is the "student list" business. Universities identify prospects (and their contact information) by purchasing "student lists" from College Board, ACT, and other vendors. Once identified, prospects are targeted with direct mail, email, text messages, social media, and other marketing and recruiting interventions. Future research should investigate which student characteristics postsecondary institutions prioritize when purchasing these student lists and which combinations of recruiting interventions are received by prospects. Scholarship within economics privileges institutional financial aid, an intervention that primarily targets the enrollment decisions of admitted applicants. By contrast, the enrollment management industry expends substantial resources on interventions that target earlier stages in the funnel (Clinedinst et al., 2015). Economists should investigate these interventions, who is receiving them, and how they affect students. In a similar vein, scholarship within the sociology of education can build on qualitative case studies of marketing behavior (e.g., Kirp, 2003) by using data science and public records request data collection strategies to develop systematic accounts. Finally, the interdisciplinary "college choice" literature has focused mostly on how students choose colleges (e.g., Hossler & Gallagher, 1987; McDonough, 1997). By collecting data on the behavior of students and colleges, scholars can develop new insights about how students and colleges find and choose one another.
10.3102_00028312211003501	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/00028312211003501	Can Free Community College Close Racial Disparities in Postsecondary Attainment? How Tulsa Achieves Affects Racially Minoritized Student Outcomes	 Promise programs, or place-based tuition-free college policies, have become increasingly popular among policymakers looking to expand postsecondary attainment. In this article, we examine Tulsa Achieves, a widespread, albeit understudied type of promise program that covers the balance of students' tuition and fees after other aid is exhausted at a single community college. Utilizing a difference-in-differences and event-study design, we investigate the role Tulsa Achieves eligibility plays in promoting or hindering vertical transfer and bachelor's degree attainment across racial/ethnic groups. We find that Tulsa Achieves eligibility is associated with increases in bachelor's degree attainment within 5 years among Native American and Hispanic students and an increased likelihood of transfer within 4 years for Hispanic students.	 Policymakers have long sought to increase postsecondary educational attainment in the United States with the primary aim of reaping the numerous benefits higher education confers to individuals and to society (McMahon, 2009). One public policy approach intended to increase postsecondary degree attainment that has proliferated in recent years is "free college" (Perna & Leigh, 2018). Commonly known as Promise programs, free-college programs take various forms and operate at different levels (e.g., school, county, state). Their shared characteristic is that they guarantee tuition coverage to students who graduate from a particular school or reside in a particular place (e.g., city, county, state) for a specified period of time (Miller-Adams, 2015). Some researchers have identified more than 400 Promise programs nationwide (Perna & Leigh, 2018), although definitions and counts of Promise programs differ (see, e.g., Miller-Adams et al., 2017). While they all generally espouse the goals of expanding postsecondary degree attainment, Promise programs vary significantly in their design (Perna & Leigh, 2018). Our study focuses on Tulsa Achieves, which represents the most common type of Promise program: It covers the balance of students' tuition and fees after other aid is applied (i.e., a last-dollar program) at a single community college (Perna & Leigh, 2018). Recently established programs overwhelmingly take this form (i.e., last-dollar program restricted to 2-year colleges; Billings, 2018b), highlighting the importance of evaluating these particular types of Promise programs. Notwithstanding their prominence, free community college programs have not been the subject of much research (Swanson et al., 2020; W. E. Upjohn Institute, 2018). Community colleges occupy a unique space in the higher education landscape (Dowd, 2007). They provide postsecondary educational opportunity to students who may not otherwise be able to access higher education (Cohen et al., 2014; Goldrick-Rab, 2010). However, these institutions have been subject to scrutiny given their outcomes on traditional measures of student success (Karabel, 1972). For instance, 3 years after enrolling, 18% of associate's degree-seeking students had obtained their degree; 40% were still enrolled (Burns & Bentz, 2020). It is important to consider these figures in the context of the varied purposes community colleges serve, the multiplicity of goals for students who enroll, and the relatively low levels of per-student funding for these institutions (Baber et al., 2019; The Century Foundation, 2019; Cohen et al., 2014). For instance, students attend community colleges for numerous reasons beyond pursuing a postsecondary degree, including vocational, career and technical education, continuing education, and "personal interest" (Cohen et al., 2014; Dowd, 2007). According to the Voluntary Framework of Accountability (2019), which uses comprehensive measures of student outcomes at community colleges, half of colleges participating in 2018 had at least 53% of students earn a certificate or degree, transfer to another institution, or remain enrolled 6 years after initial enrollment. Still, as Promise programs that induce students to attend community colleges proliferate, it is critical to understand how and for whom these programs affect postsecondary attainment, particularly since a prominent goal of Promise programs is to promote degree attainment (Perna & Leigh, 2018). In this study, we estimate the effect of eligibility for Tulsa Achieves, a last-dollar, free community college program in Oklahoma, on postsecondary outputs (i.e., transfer and degree attainment) employing a difference-in-differences and an event-study design. To this end, we leverage a unique student-level data set from Tulsa Community College's (TCC) administrative data system that includes 2005–2012 cohorts. In particular, we focus on the relationship between Tulsa Achieves eligibility and postsecondary outputs for racially minoritized students (i.e., those classified as Black, Native American, and Hispanic). Racially minoritized individuals are those who have been historically oppressed based on their racial categorization and continue to have the lowest educational attainment rates in what is now the United States. Tulsa Achieves, the free community college program we examine in this study, has the potential to address some of the factors sustaining racial disparities in educational attainment. For instance, by making college attendance an expectation for all students, Tulsa Achieves could reduce racial disparities in educational expectations among teachers and counselors (Howard, 2003; Stanton-Salazar, 1997). This Promise program could also reduce the financial burden associated with pursuing college, which can be more crippling for racially minoritized individuals (Heller, 1997). As another example, Tulsa Achieves could enhance cooperation across schools and postsecondary institutions, and serve as a catalyst to reform structures (e.g., remedial education, unclear transfer pathways) that depress educational attainment for racially minoritized students (Duchini, 2017; P. Gándara et al., 2012; Schak et al., 2017). Emergent research on free community college programs similar to Tulsa Achieves finds that these programs increase college enrollment for racially minoritized students at eligible (2-year) institutions (e.g., D. Gándara & Li, 2020). What remains underexamined is the role free community college programs play in promoting or hindering transfer and degree attainment across racial/ethnic groups. To address that gap in our understanding of these programs, this study addresses the following research questions: Research Question 1: How does Tulsa Achieves eligibility affect the likelihood of vertically transferring (from a 2-year to a 4-year institution) among racially minoritized students? Research Question 2: How does Tulsa Achieves eligibility affect degree attainment among racially minoritized students? Results indicate that eligibility for Tulsa Achieves increased bachelor's degree attainment within 5 years among Native American and Hispanic students, with the largest increase among Native American students. Tulsa Achieves eligibility also significantly increased the likelihood that Hispanic students transfer vertically within 4 years. Among White students, Tulsa Achieves eligibility increased the likelihood of obtaining 63 credits and graduating TCC within 3 years. These effects vary across the high school GPA distribution for Black, Hispanic, and Asian American students, indicating the importance of prior academic achievement in moderating the effects of Tulsa Achieves on transfer and degree attainment. To provide evidence on the robustness of these results, we empirically explore whether the results may be biased by changes in student composition, and present results from a variety of model specifications. Promise programs have seen mounting popularity in recent years, but the degree to which they alleviate or exacerbate inequality in postsecondary attainment across racial/ethnic groups remains largely unexamined. This study contributes to extant literature on free-college programs in three major ways. First, we examine the most common type of free-college program (limited to a 2-year college) and its relationship with degree attainment. Attention to this type of program is important, since last-dollar, free community college programs are diffusing rapidly and have the potential to affect longer term outcomes like bachelor's degree attainment by inducing students to begin college at a community college rather than a 4-year institution (Billings, 2018b; Carruthers & Fox, 2016; Gurantz, 2020). Second, the setting for this study, Oklahoma, allows us to attend to one racial/ethnic group that is generally neglected in quantitative research, Native American students (Lopez & Marley, 2018). Last, by focusing on racial/ethnic disparities, this study illuminates the extent to which free community college programs can reduce inequality in bachelor's degree attainment for racially minoritized students. To understand how eligibility for Tulsa Achieves affected postsecondary outcomes for students from different racial/ethnic groups, we first garner insights from literature on racial disparities in postsecondary educational attainment. This study is also anchored in scholarship exploring community colleges' dual roles in expanding educational opportunity (democratizing role) and diverting students from 4-year institutions (diversionary role). Since Tulsa Achieves induces students to attend a community college, this literature contextualizes our findings on program eligibility and bachelor's degree attainment. These literatures on educational disparities across racialized groups and on the democratizing and diversionary roles of community colleges form the foundation of our conceptual framework. To contextualize our study on the effect of Tulsa Achieves eligibility on postsecondary outcomes for different racial/ethnic groups, we review key factors associated with racial/ethnic disparities in postsecondary attainment. First, racially minoritized students have historically been denied access to quality schooling, which limits their postsecondary educational possibilities (Aleman & Luna, 2013). Although the "separate but equal" legal doctrine was deemed unconstitutional in 1954, schools remain highly segregated by race and ethnicity (Frankenberg et al., 2019). Students are also segregated across racial/ethnic categories within schools due to practices such as curriculum tracking (Noguera, 2008; Oakes, 2005) and disparate punishment in schools (Noguera, 2009; Skiba et al., 2011). Since racially minoritized students have more limited educational opportunities in K–12, they tend to have lower levels of academic preparation, limiting their likelihood of college completion (Association of American Colleges and Universities, 2015). When they enroll in college, racially minoritized students are more likely to attend lower resourced institutions, including community colleges and less selective 4-year institutions (Carnevale & Strohl, 2013; The Century Foundation, 2019; Nichols, 2020). This is important, because recent evidence shows a causal link between institutions' levels of spending and degree attainment rates (Deming & Walters, 2018). Racially minoritized students also have considerably lower levels of income and wealth, which conditions their ability to forgo earnings and pay for college (Dettling et al., 2017; Nelson, 2015; U.S. Census Bureau, 2018). Attainment gaps in postsecondary education across racial/ethnic groups also relate to institutional policies and practices that produce barriers that are especially harmful to racially minoritized students, including remedial and developmental education (P. Gándara et al., 2012; Jimenez et al., 2016) and inadequate transfer and articulation agreements between 2- and 4-year institutions (Bensimon, 2020; P. Gándara et al., 2012). Finally, campus racial climate and students' sense of belonging and community are also critical factors shaping college outcomes for racially minoritized students (Brayboy, 2004; Mosholder et al., 2016; Museus et al., 2008; Museus et al., 2018; Youngbull, 2018). A program like Tulsa Achieves could disrupt some of these mechanisms contributing to racial disparities in postsecondary attainment (e.g., by universalizing college-going expectations, streamlining transitions between 2-year and 4-year institutions, and creating a positive sense of belonging). We return to some of these mechanisms in our description of the Tulsa Achieves program and its design features. Researchers have long recognized the dual roles community colleges play in democratizing higher education on one hand and potentially diverting students away from higher postsecondary pursuits on the other (e.g., Brint & Karabel, 1989; Clark, 1960a, 1960b; Dougherty, 1994; Rouse, 1995). Community colleges fulfil their democratizing function by providing access to higher education for students who would not otherwise have it, since community colleges tend to have open-admission policies, are generally more affordable than other postsecondary options, and are geographically widespread (Dougherty, 1994). In fact, community colleges enroll higher shares of racially minoritized, lower income, and older adult students, who have historically been underserved by higher education institutions, than their 4-year counterparts (National Center for Education Statistics [NCES], 2019, 2020). Although community colleges expand access to higher education, critics have argued that rather than fulfilling their promise of reducing inequality, they reproduce existing social inequities (e.g., Labaree, 2013; Richardson & Bender, 1987). Brint and Karabel (1989) posited that community colleges reproduce inequality by diverting students away from 4-year tracks, ultimately reducing postsecondary attainment levels for disadvantaged students. Some scholars have argued that in fact a core function of community colleges is to "cool out," or lower students' baccalaureate aspirations by reorienting them toward "terminal" (nontransfer) paths (Clark, 1960a, 1960b). Recent research paints a more complex picture of the extent to which community colleges embody a "cooling out" function. In particular, most students' aspirations do not change when they attend a community college (Alexander et al., 2008), and studies have found evidence of both cooling out and "warming up," or raising students' aspirations within community colleges (Adelman, 2005; Alexander et al., 2008; Deil-Amen, 2006). Regardless of whether community colleges "cool out" or "warm up" student aspirations, only a small fraction (14%) of students attending community college graduate with a bachelor's degree within 6 years, despite roughly 80% aspiring to do so (Community College Research Center, n.d.). For the roughly 80% of students who enter a community college with the goal of obtaining a bachelor's degree, the evidence is mixed on whether starting at a community college suppresses bachelor's degree attainment (Alfonso, 2006; Long & Kurlaender, 2009; Melguizo, 2009; Melguizo & Dowd, 2009; Monaghan & Attewell, 2015; Reynolds, 2012; Sandy et al., 2006; Wang, 2015). In a meta-analysis of research examining the effects of starting in a community college on bachelor's degree attainment, Schudde and Brown (2019) concluded from the "best evidence" that starting at a community college (vs. starting at a 4-year university) decreases the likelihood of bachelor's degree attainment by 23 percentage points. Relevant to our study, students of color are less likely to transfer vertically than White students (Dougherty & Kienzl, 2006). Therefore, despite the democratization function of community colleges, there are persistent disparities in transfer and bachelor's degree attainment that raise important questions regarding how to promote the success of racially minoritized students in community colleges. Ultimately, some of the primary reasons for limited baccalaureate success for community college entrants are structural. For instance, community colleges tend to have relatively low levels of funding (Dougherty et al., 2017; Kahlenberg, 2015; Richardson & Bender, 1987). Others have identified structural problems beyond funding, including a lack of articulation between 2- and 4-year institutions (Dougherty et al., 2017) and "wasted credits," those that are not counted toward a student's degree plan at the receiving institution (Monaghan & Attewell, 2015; Schudde & Brown, 2019). These studies suggest that policies that increase funding and improve course alignment across institutions could ameliorate the negative impact of community college entrance in pursuit of a bachelor's degree that prior research has detected. With this important backdrop, we build on the literature exploring the democratizing and diversionary roles community colleges play in this analysis of the effects of a program that induces students to attend a community college. Consistent with these perspectives, Tulsa Achieves could affect student enrollment by (a) enticing students who would not have attended college to enroll in the eligible college (TCC) or (b) diverting students away from other institutions—either 2-year or 4-year institutions—and into TCC. Either effect could potentially increase bachelor's degree attainment (e.g., if students aspire to attain a bachelor's degree and proper supports are in place to facilitate these pursuits or if students "warm up," increasing their aspirations while at TCC). On the other hand, if students are "cooled out" or face barriers to vertical transfer, baccalaureate attainment could be suppressed. Extant research indicates Promise programs generally increase college enrollments, particularly at institutions eligible for the program (Bartik et al., 2021; Billings, 2018a; Carruthers & Fox, 2016; Page et al., 2018; Page & Iriti, 2016; Pluhta & Penny, 2013). However, prior research also suggests that program design matters. Some programs with burdensome requirements or highly targeted eligibility criteria do not yield larger enrollments (Daugherty & Gonzalez, 2016; Harris et al., 2018; H. Nguyen, 2019). Programs that only allow Promise funds to be used at 2-year colleges may divert students from 4-year colleges (Carruthers & Fox, 2016; Gurantz, 2020; H. Nguyen, 2020). For instance, Knox Achieves, a free community college program, diverted some students from the state's 4-year institutions, although the program primarily attracted students who would not have attended college at all (Carruthers & Fox, 2016). The diversionary effect, even if modest, could negatively affect bachelor's degree attainment, especially since vertical-transfer rates remain relatively low (Shapiro et al., 2017). It is possible, however, that Promise programs could increase transfer rates, such as by propelling cooperation across institutions or improvements in academic advising. To our knowledge, our study is among the first to examine these key questions in the context of a last-dollar, single-college Promise program like Tulsa Achieves. Only a handful of studies have focused on free community college programs (e.g., Carruthers & Fox, 2016; Gurantz, 2020; Pluhta & Penny, 2013). One recent study examined 32 Promise programs across the United States that apply to a single community college and found that they generated considerable enrollment increases at eligible institutions, on average (Li & Gándara, 2020). A follow-up study finds that after Promise program implementation, enrollment increases at these institutions were largest among Black and Hispanic students (D. Gándara & Li, 2020). Fewer studies have focused on the effect of free-college programs on postsecondary outcomes, in part because student-level data are not readily available. Research that does examine outcomes focuses on programs that allow students to use their Promise scholarships at either 2- or 4-year institutions. For instance, two studies examining the Kalamazoo Promise, a broad and generous Promise program in Michigan, find that the program increased bachelor's degree attainment among eligible students (Bartik et al., 2021; Billings, 2018a). Importantly, Billings (2018a) found that Black and Hispanic students saw the smallest increases in degree attainment, compared with students classified as White and Asian. In a different setting, the El Dorado Promise, which allowed students in rural Arkansas to attend any accredited institution in the United States, also improved bachelor's degree completion rates, particularly among students with higher high school GPAs (Swanson & Ritter, 2020). Last, a study of the Pittsburgh Promise found that the program increased enrollment, particularly at 4-year universities, and persistence (Page et al., 2018). All of the aforementioned studies on postsecondary outcomes, which find generally positive aggregate results, examine Promise programs that apply to both 2-year and 4-year institutions. Only one study we are aware of has focused on postsecondary outcomes for a single-community college program similar to Tulsa Achieves. In that descriptive study, Pluhta and Penny (2013) found that the program was associated with higher enrollment and retention rates, but high student placements in remedial/developmental courses. Beyond examining outcomes for a Promise program that applies to a single, 2-year college, our analysis extends prior literature in a number of ways. First, we examine differentiated outcomes based on students' racial/ethnic classification. Second, we examine the intersection of racial/ethnic identity and students' prior academic achievement. Last, we analyze the effect of the program not only on associate's degree attainment but also on transfer and bachelor's degree attainment. Given racial/ethnic disparities in postsecondary attainment (NCES, 2017), a focus on how free community college programs affect postsecondary outcomes across racial/ethnic groups is momentous for higher education research. Beyond the scholarly community, it is also important for policymakers, funders, students, and other stakeholders to understand how programs that promote community college enrollment affect degree attainment and how these effects differ by students' racial/ethnic classification. Tulsa Achieves is a pioneering Promise program that began providing tuition-free community college for students who graduate from high school in Tulsa County in 2007. Unlike Promise programs in the majority of previous studies, students enrolled in the Tulsa Achieves program are only able to utilize the aid to attend a single 2-year college, TCC. The program is last-dollar, meaning the amount of aid a student receives from the program depends on the other elements of the student's financial aid package. Specifically, Tulsa Achieves covers any remaining balance on tuition and fees after all other state and federal financial aid is exhausted. Additionally, the aid is available for up to 3 years or for 63 credits (whichever comes first) and scales based on the number of years the student resided in Tulsa County while in high school. Students who were Tulsa County residents for all 4 years get 100% of the remaining tuition and fee expenses covered by Tulsa Achieves, and each year the student was not a Tulsa County resident reduces the percentage covered by 25%. To be eligible, students must reside in Tulsa County on high school graduation and make at least a 2.0 GPA in high school. To keep the scholarship, students must maintain residency in the county, fill out the Free Application for Federal Student Aid (FAFSA), and meet satisfactory academic progress standards. Students must also take a student success course the first year and complete at least 40 hours of community service each year to remain eligible. The "treatment" includes various components, most of which are common across Promise programs. The first part of the treatment is the financial aid students receive from the program. Despite being active participants in the program, some students do not receive aid from Tulsa Achieves because the program is last-dollar and, for some students, the cost of tuition and fees is already covered by the Pell Grant or state financial aid. Students who received aid from Tulsa Achieves are disproportionately White and higher income, suggesting that this element of the treatment may be less important in shaping attainment for racially minoritized students. However, consistent with prior research (Carruthers & Fox, 2016), we expect the program to affect even those who do not receive much aid from the program through other mechanisms. Beyond awarding aid to students and reducing financial uncertainty, Tulsa Achieves could enhance perceptions that college is affordable due to its"free college"message (Harnisch & Lebioda, 2016; Perna, 2016). This mechanism could be particularly salient for racially minoritized students, whose studies have shown to be more sensitive to college prices (Heller, 1997); less confident in their knowledge of college prices (Grodsky & Jones, 2007); and more likely to perceive college as unaffordable (Freeman, 1997; McDonough & Calderone, 2006; Perna, 2000; Tachine, 2015; Tierney et al., 2007; Zarate & Pachon, 2006). Racially minoritized individuals also have considerably lower levels of income and wealth, affecting their ability to pay for college and forgo earnings while attending college (Dettling et al., 2017; U.S. Census Bureau, 2018). The "free college" message, regardless of the amount of funding received from the program, could encourage students to attend college. Related to the "free-college" message, the college-for-all ethos that accompanies free-college programs could be particularly impactful for racially minoritized students. College-going expectations, including among teachers and counselors, tend to be lower for racially minoritized students (Diamond et al., 2004; Freeman, 1997; P. Gándara & Contreras, 2009; Howard, 2003; Stanton-Salazar, 1997). Relatedly, racially minoritized students and their families might be less likely to receive encouragement or technical support with college application processes (Archer-Banks & Behar-Horenstein, 2012; P. Gándara & Contreras, 2009; McDonough et al., 2015; Noguera, 2003). Tulsa Achieves could attenuate these barriers to college enrollment for racially minoritized students by universalizing college-going expectations. Another part of the treatment is the student-success course requirement, which could help students develop skills that will help them be successful. This course provides students with resources for developing study skills and provides an orientation to campus resources. Yet another part of the treatment is the community-service requirement, which requires students to complete 40 hours of community service. These two requirements could help racially minoritized students persist by keeping them engaged and perhaps fostering a sense of community. On the other hand, these requirements could have the unintended effect of imposing high time costs on students that can hinder their academic success. This could be particularly harmful for racially minoritized students, who might have more substantial time constraints. For instance, Black students are more likely to work full time while enrolled in college, while Black, Hispanic, and Native American students are significantly more likely to have dependents (Radford et al., 2015). Finally, Tulsa Achieves could improve the transition between TCC and a 4-year university. After Tulsa Achieves was implemented, nearby 4-year colleges made articulation agreements with TCC to minimize the loss of credit hours in the process of transferring and provided transfer scholarships to Tulsa Achieves students who complete the program (earn 63 credits or maintain eligibility throughout 3 consecutive years of enrollment at TCC) and vertically transfer. These benefits, especially course alignment, can enhance the likelihood of bachelor's degree attainment (Monaghan & Attewell, 2015; Shaat, 2020). Moreover, these parts of the treatment can be particularly important for racially minoritized students for whom transfer remains a key barrier to bachelor's degree attainment (P. Gándara et al., 2012). While we are unable to tease out these various components of the treatment due to data limitations, this conceptualization of the treatment guides our interpretation of the findings. In particular, deconstructing the treatment this way sheds light on potential mechanisms by which Tulsa Achieves could have differential impacts across racial/ethnic groups. In the next sections, we detail our unique student-level data set as well as our analytical approach to examining the effect of Tulsa Achieves eligibility on vertical transfer and degree attainment. We extracted student-level administrative data for all first-time entering TCC students, except those concurrently enrolled in high school, from 2005 to 2012. Then, we matched these student records with data from the National Student Clearinghouse for all students who transferred to 4-year colleges. The data set, summarized in Table 1, includes information on student racial/ethnic identity, gender classification, age, county residency at the time of high school graduation, high school GPA (grade-point average), transfer to 4-year colleges, and degree attainment at both TCC and 4-year colleges. In Table 1, we summarize the analytical data set for eligible and ineligible students before and after the implementation of Tulsa Achieves. We highlight two notable takeaways from Table 1. First, only a fraction of students eligible for Tulsa Achieves receive more than a dollar of aid from the program. Despite 58% of eligible Tulsa Achieves students enrolling in the program in the postimplementation cohorts, only 20% of students ever received more than a dollar of financial aid from the program. This reveals that a large proportion of students in the Tulsa Achieves program receive other federal or state financial aid that fully covers tuition and fees before Tulsa Achieves provides any aid. The group of students that received more than a dollar of financial aid in the program were predominantly White (63%) and had a significantly higher estimated family contribution (EFC) than nonrecipients ($13,493 compared with $4,136). Unfortunately, we are unable to provide the average award amount since we do not have data on the amount of aid received through the Tulsa Achieves program; we only know whether the student participates in the program and whether the student receives more than $1 of Tulsa Achieves aid in each semester. Second, Table 1 shows that the student population is significantly different in postimplementation years. Specifically, the postimplementation cohorts (2007–2012) include a younger student body with more racially minoritized students and a higher EFC. For instance, 69% of students identified as White in the 2006 cohort, one year before Tulsa Achieves, but by the 2012 cohort only 53% of students identified as White. The EFC increased from $800 in the 2005 and 2006 cohorts to $9,549 in the post–Tulsa Achieves cohorts, revealing the stark increase in the proportion of students from middle- and upper-income families. At the same time, there was also an increase in Pell Grant recipients (from 29% before Tulsa Achieves to 38% after the program was implemented). Together, these data points suggest that there was a simultaneous influx of middle/upper-income families and low-income families after Tulsa Achieves was implemented. Another notable change is an increase in the size of cohorts: In the 2005 and 2006 cohorts, there were 2,500 first-time entering students, which rose to an average of 3,500 first-time entering students in post–Tulsa Achieves cohorts. Therefore, after Tulsa Achieves was implemented, the school saw an increase in cohort size, driven partially by a larger proportion of non-White students and students from upper- and middle-income families. We supplement the information in Table 1 with Table 2, which breaks down the overall characteristics of students in the sample by race/ethnicity. Table 2 shows that the student group with the highest GPAs prior to attending college was Asian students, followed by White students; student groups with the lowest high school GPAs are Black students followed by Hispanic and Native American students. The racial/ethnic group with the highest percentage of Pell Grant recipients is Black students, and the proportion of students receiving aid from any other scholarship or grant program is low across racial/ethnic groups (roughly 2%). Turning to outcome variables, we calculate four outcomes capturing whether a student (a) earned 63 credits within 3 years, (b) graduated TCC within 3 years, (c) transferred to a 4-year college within 4 years, or (d) obtained a bachelor's degree within 5 years. We utilize the programmatic rules to guide our choice of timing for each outcome. Importantly, our data do not capture certificate completion, either at TCC or at other 2-year colleges. Thus, we may be underestimating the effect of Tulsa Achieves, since certificates, particularly long-term certificates, can carry significant labor market value (Dadgar & Trimble, 2015). Table 2 reveals that there are significant racial gaps in vertical transfer and degree attainment at TCC. Across our outcome measures, Asian and White students are significantly more likely to vertically transfer and obtain credentials, compared with Hispanic, Native American, and Black students. Whether Tulsa Achieves improved vertical transfer and degree attainment among racially minoritized students, and was able to reduce these racial/ethnic gaps in vertical transfer and degree attainment, remains an open question that we address in this study. We leverage the discontinuous timing of program implementation in a difference-in-differences design to estimate the average treatment effect of Tulsa Achieves eligibility on vertical transfer and degree attainment. We identify the students eligible for Tulsa Achieves as the students who graduated from high school in Tulsa County and made over a 2.0 high school GPA, the two most important eligibility requirements of the program. While we would like to include FAFSA completion in the determination of eligibility, these data were not available. We implement the following model in a regression framework presented in the equation below:[MATH](1) where our outcomes of interest ([MATH])—vertical transfer and degree completion— are a function of an interaction between Tulsa Achieves eligibility [MATH]) and an indicator for whether the student was in a pre- or postimplementation cohort ([MATH]; a vector of covariates, including age, gender, Pell Grant receipt, and high school GPA ([MATH]); a cohort fixed effect ([MATH]); a high school fixed effect ([MATH]); a constant ([MATH]); and an idiosyncratic error term ([MATH]). By comparing eligible and ineligible students from the same high school in the same cohort, we aim to isolate the impact of Tulsa Achieves eligibility and do our best to account for the influence of the economic recession. Our parameter of interest ([MATH]) reveals the intent to treat estimate (ITT) of the shift in vertical transfer and degree attainment for students eligible for Tulsa Achieves, relative to the shifts in ineligible students residing outside of Tulsa County or Tulsa County students with less than a 2.0 high school GPA, after controlling for level differences between the groups. It is important to note that our empirical strategy provides an estimate of the effect of Tulsa Achieves eligibility (ITT) not Tulsa Achieves receipt. In accordance with best practices, we cluster our standard errors by cohort and high school to account for the correlation within these clusters (Abadie et al., 2017). To estimate the differential effect of Tulsa Achieves by students' race/ethnicity, we take three approaches to the estimation of Equation (1). First, we split the sample into two main groups—(a) Hispanic, African American, and Native American students and (b) White and Asian students—and estimate Equation (1) separately for each group. Next, we add additional nuance by estimating an additional series of equations in which we estimate Equation (1) separately for each racial/ethnic group. This combination of techniques allows us to compare the changes in vertical transfer and degree attainment for Tulsa Achieves eligible students across racial/ethnic groups, relative to their ineligible counterparts. Finally, in a third series of models we investigate the interaction between racial/ethnic identity and academic achievement by splitting the sample into students with above-average and below-average high school GPAs and estimating Equation (1) separately for (a) Hispanic, African American, and Native American students with above- and below-average high school GPAs and (b) White and Asian students with above- and below-average high school GPAs. Together, these techniques reveal the differential effect of Tulsa Achieves for different racial/ethnic groups and across students with varying levels of high school academic achievement. For a difference-in-differences design to reveal causal effects, two central identification assumptions must be met. First, we must assume that we have identified an appropriate counterfactual group and that in the absence of Tulsa Achieves, trends for eligible and ineligible students would have followed similar trajectories. Although there is no formal way to tell how students would have fared in a counterfactual world without Tulsa Achieves, we can examine whether there are differential trends across the treatment and control groups before Tulsa Achieves. If we find evidence of differential preexisting trends there is the potential for our estimates to be biased. We examine this assumption, commonly called parallel trends, by graphing the changes in the outcome measures across the treatment (eligible students) and control (ineligible students) groups before and after program implementation (Figure 1). This figure demonstrates that prior to the implementation of Tulsa Achieves, the trends are generally similar across eligible and ineligible students, but that there are some differences between groups from the 2005 cohort to the 2006 cohort. With only two pre–Tulsa Achieves cohorts, we are unable to determine whether these differences were part of a preexisting trend, but we do note some differences in the likelihood of obtaining 63 credits within 3 years, the likelihood of graduating from TCC, and the likelihood of obtaining a bachelor's degree within 5 years across the 2005 and 2006 cohorts. While it would be ideal to have more preimplementation cohorts, these data were not available prior to the 2005 cohort. Therefore, we leverage the data available to provide the best possible estimates in our main analysis below, which we supplement with event study specifications in the robustness checks. The event studies formally estimate whether there are differences across eligible and ineligible students in pretreatment cohorts when controlling for our set of covariates. Next, we assume that any changes in the outcomes of interest are the result of Tulsa Achieves and not another simultaneous reform. We have spoken with the program administrators at TCC and have confirmed that there were no other significant policy changes at the college in the year 2007, making this less of a concern. We address the potential for other simultaneously occurring events, such as the economic recession, in the robustness analysis. Finally, as with any causal identification, the major threat to validity comes from the potential for selection bias. In the context of Tulsa Achieves, this bias could come in multiple forms. First, it is possible that Tulsa Achieves had a diversionary effect—students who would have attended a more selective 4-year college instead enrolled in TCC due to Tulsa Achieves. As a result, the eligible post–Tulsa Achieves cohorts could be more academically prepared for college and have other unobservable characteristics potentially associated with our outcomes (transfer and degree attainment), which could create an upward bias in our causal estimates. However, it is also possible that Tulsa Achieves had a democratizing effect—students who would not have attended college in the absence of the program could be induced to enroll in TCC due to Tulsa Achieves, which would likely negatively bias the results. Based on prior research, the democratizing effect is likely stronger in magnitude than the diversionary effect, which would mean that we provide conservative estimates rather than inflated estimates of the effect of Tulsa Achieves eligibility (Carruthers & Fox, 2016; Gurantz, 2020; H. Nguyen, 2020). To explore this question empirically, we investigate the changes in first-time entering fall enrollment patterns across racial/ethnic groups at TCC and surrounding 4-year universities before and after Tulsa Achieves was implemented in Supplemental Appendix A (available in the online version of the journal). Supplemental Appendix A demonstrates that Native American, White, and Asian students enrollment does appear to be significantly affected by Tulsa Achieves, which could introduce selection that would influence the interpretation of our results on vertical transfer and degree attainment. If there was diversion among these students, this could mean that any impacts on vertical transfer and degree attainment would be partially due to systematic differences between ineligible students and eligible students who would have attended a 4-year college instead of TCC absent Tulsa Achieves. To address the threat to internal validity and account for the possibility of selection bias, we include fixed effects for cohort and high school and include a series of control variables capturing high school GPA, Pell Grant receipt, age, gender, and whether the student is the recipient of any other scholarships or grants. We also provide multiple robustness checks to test the sensitivity of our estimates. We are encouraged by the dearth of evidence that high school GPAs differ decidedly between pre– and post–Tulsa Achieves cohorts. Still, there could be unobservable characteristics among the students induced into TCC by Tulsa Achieves that could relate to our outcomes of interest. We heed this context in interpreting our main results. We present the first set of results in Table 3, which displays the average treatment effect estimates (ITT) for eligible Tulsa Achieves students compared with their ineligible counterparts, by the grouped racial/ethnic categories. In line with prior research, we begin by combining Black, Hispanic, and Native American students in one group and White and Asian students in another to increase power and sample size (Billings, 2018a). Table 3 reveals that for the group of Black, Hispanic, and Native American students, Tulsa Achieves eligibility resulted in a 3.6 percentage point increase in the likelihood of graduating TCC within 3 years and a 6.2 percentage point increase in the likelihood of graduating with a bachelor's degree within 5 years of initial enrollment. For the group of White and Asian students, we find that the likelihood of obtaining 63 credits within 3 years increased by 2.2 percentage points and the likelihood of graduating TCC within 3 years increased by 4.2 percentage points. To investigate which specific racial/ethnic groups are experiencing these effects, we build on the results in Table 3 by estimating Equation (1) separately for each racial/ethnic group. The results, which appear in Table 4, reveal that the positive effects of Tulsa Achieves on the probability of obtaining a bachelor's degree within 5 years were concentrated among Hispanic and Native American students. Among Native American students, Tulsa Achieves eligibility increased the likelihood of graduating with a bachelor's degree within 5 years by roughly 9.1 percentage points. For Hispanic students, Tulsa Achieves eligibility increased the likelihood of obtaining a bachelor's degree within 5 years by 4.2 percentage points and the likelihood of transferring to a 4-year college within 4 years by 12.7 percentage points. Finally, Table 4 reveals that Tulsa Achieves eligibility increased the likelihood of obtaining 63 credits and the likelihood of graduating TCC within 3 years for White students by 2 and 4.3 percentage points, respectively. In contrast to the significant effects among Native American, Hispanic, and White students, the estimated effects of Tulsa Achieves eligibility are null for Black and Asian American students for each outcome of interest. In Supplemental Appendix B, we conduct three separate robustness analyses to test the sensitivity of these results. First, we provide the estimates when limited to a set of cohorts affected less by the economic recession. Those results suggest the effect of Tulsa Achieves eligibility differed somewhat between pre- and postrecession cohorts for Black students and White students. Most notably, we find significant positive effects of Tulsa Achieves eligibility on Black student attainment (associate's degree at TCC, vertical transfer, and bachelor's degree attainment). Second, we present event-study estimates, which allow the effect to vary across post–Tulsa Achieves cohorts. Those results indicate some potential divergent trends on one outcome, bachelor's degree attainment, between eligible and ineligible students in the 2006 cohort for Black, Hispanic, and Native American students. To address that potential threat to internal validity, we implemented coarsened exact matching to restrict the comparison group to students that match eligible Tulsa Achieves students on observable characteristics. In the matched sample, we do not detect significant differences between eligible and ineligible Tulsa Achieves students in the 2005 and 2006 cohort in bachelor's degree attainment. Those analyses also yield positive point estimates for bachelor's degree attainment in post–Tulsa Achieves cohorts. These results provide greater support for the validity of our main findings on the differentiated effects of Tulsa Achieves eligibility on vertical transfer and degree attainment across racial/ethnic groups. Next, we investigate the potential for heterogeneity based on students' level of prior academic achievement. In Table 5, we present the results separately for students who have above-average and below-average high school GPAs for the group of White and Asian American students on one hand and the group of Black, Hispanic, and Native American students on the other. We provide these grouped estimates rather than separate results for each racial group due to our concern regarding sample size restrictions and statistical power. For the first group of students—Black, Hispanic, and Native American students with above-average GPAs—Tulsa Achieves eligibility increased the likelihood of obtaining a bachelor's degree within 5 years by 11.7 percentage points, compared with ineligible students with above-average GPAs. Turning now to the group of Black, Hispanic, and Native American students with below-average GPAs, we find that Tulsa Achieves eligibility also increased the likelihood of bachelor's degree attainment within 5 years, albeit more modestly (by 5.3 percentage points). Based on our main results, these significant positive effects on bachelor's degree attainment seem to be driven by Native American and Hispanic students, and not by Black students. We also note that while an increase of 5.3 percentage points in bachelor's degree attainment is substantively meaningful (especially given that the sample mean is 8%), the magnitude of the effect for Native American and Hispanic students with above-average GPAs is more than twice the size of the increase in bachelor's degree attainment among the group of students with below-average GPAs. We see similar dynamics across the GPA distribution for the group of White and Asian students. For the group of White and Asian students with above-average GPAs, Tulsa Achieves eligibility increased the likelihood of graduating TCC within 3 years by 8.7 percentage points. However, the effect of Tulsa Achieves eligibility is null for all outcomes for the group of White and Asian students with below-average GPAs. In light of our main findings above, we conclude that the positive effects within this group are driven by White students with above-average GPAs. Together, these results reveal that the effects of Tulsa Achieves eligibility are significantly different across the academic achievement distribution, with the positive effects concentrated among students with above-average high school GPAs. Still, the significant, positive effect of Tulsa Achieves eligibility on bachelor's degree attainment within 5 years for racially minoritized students with below-average high school GPAs is noteworthy. As studies begin to grapple with the potential for Promise programs to have differential effects, a key population to consider for the advancement of equity and social justice in higher education are racially minoritized students, those who have been systematically oppressed in the education system based on their racial/ethnic classification. Racially minoritized students generally have access to fewer educational resources before college (EdBuild, 2019), have more limited opportunities to learn (James et al., 2020; Oakes, 2005), and tend to be subject to lower expectations from teachers, counselors, and other school adults (Chin et al., 2020; Ferguson, 2003; P. Gándara & Contreras, 2009; Stanton-Salazar, 1997). Tulsa Achieves—a last-dollar, tuition-free college program that can be used at a single community college—represents a comprehensive effort to improve postsecondary attainment that could combat inequities across racialized groups. The results from this study provide some promising signs for the potential of programs similar to Tulsa Achieves to reduce racial/ethnic disparities in postsecondary attainment. First, we detect positive effects of Tulsa Achieves eligibility on bachelor's degree completion, which are concentrated among Native American and Hispanic students. We also find that Tulsa Achieves eligibility increased bachelor's degree attainment for Black students in prerecession cohorts. The positive effect for Native American, Hispanic, and Black students (in prerecession cohorts) reveals the potential for Tulsa Achieves to reduce inequality by improving educational outcomes among racially minoritized groups. Indeed, our findings suggest that this free-tuition model was especially beneficial for racially minoritized students, which historically have been underserved in higher education. This is despite the program disproportionately allocating financial aid to White and higher-income students, suggesting financial aid may not be the primary mechanism driving improvements in bachelor's degree attainment among racially minoritized students. While we are unable to precisely deconstruct the various aspects of the intervention that were beneficial to racially minoritized students, one potential explanation is that perceptions of the price of college are a strong deterrent for these students to pursue higher education (Freeman, 1997; McDonough & Calderone, 2006; Perna, 2000; Tierney et al., 2007; Zarate & Pachon, 2006), in part because racially minoritized people have less ability to pay (lower incomes and wealth) (Dettling et al., 2017; U.S. Census Bureau, 2018). Tulsa Achieves might be eliminating that barrier by spreading the "free college" message. In addition, Tulsa Achieves might provide these students additional security in knowing that their community college education (up to 63 credit hours) will be covered, regardless of the source of the aid. This is an important finding, because racially minoritized students receive the least money from Tulsa Achieves. Yet positive effects on transfer and degree attainment were greater for this population than for White and Asian students. Moreover, there are numerous administrative burdens associated with enrolling in and persisting in college (e.g., meeting deadlines, filling out the FAFSA, and understanding program requirements and financial aid letters). These burdens can be especially harmful to minoritized students, who are less likely to receive administrative support and for whom college-going expectations might be lower (Archer-Banks & Behar-Horenstein, 2012; P. Gándara & Contreras, 2009; McDonough et al., 2015; Noguera, 2003). By universalizing college-going expectations, Tulsa Achieves could alleviate a burden that is especially heavy for racially minoritized students, given racialized schooling contexts. Similarly, the transfer supports embedded in Tulsa Achieves (articulation agreements and transfer scholarships) could be especially helpful to minoritized students, for whom vertical transfer is a steadfast obstacle to bachelor's degree attainment (Bensimon & Dowd, 2009; Crisp & Nuñez, 2014; P. Gándara et al., 2012; Shaat, 2020). Considering the components of the "treatment" that comprise Tulsa Achieves, it is also possible that the mandatory student success course is especially helpful to minoritized students by creating a sense of community and personalized support. On the other hand, this course, as well as the 40-hour community-service requirement, could impose high-time costs on students, particularly those who have other obligations. For instance, racially minoritized students are more likely to have dependents, and Black students are most likely to work full time while enrolled in college (NCES, 2019). These requirements could partly explain why Tulsa Achieves eligibility was less impactful on Black student outcomes. Would outcomes be more positive for racially minoritized students absent these requirements? Future research, including qualitative inquiry, could explore the effects of the requirements that impose time costs on students and how those differ across racialized groups. Our positive and significant findings for Native American students are particularly noteworthy and could be due to considerable support for Native American students from tribal nations, and the partnership between tribal authorities and higher education institutions in Tulsa. The Tulsa community is home to the Cherokee Nation and Muskogee (Creek) Nation, both of which support students through scholarships and partner with the nearby 4-year institutions, including Oklahoma State University (OSU) and Northeastern State University. These 4-year colleges were where the majority of Native American students chose to transfer after TCC. OSU offers a minor in American Indian studies and has a research center dedicated to the advancement of health care for Native American communities, and Northeastern State University has a student support center dedicated to recruiting and supporting Native American students. This center also provides scholarships to students in one of the five tribes: Cherokee Nation, Muscogee (Creek) Nation, Chickasaw Nation, Seminole Nation, and Choctaw Nation. Targeted support for Native American students as well as community building are critical for their success (Brayboy, 2004; Mosholder et al., 2016; Youngbull, 2018). Our findings on Native American students are particularly critical given the importance of degree attainment for Native Nation Building and self-determination (Brayboy et al., 2012; Nelson & Tachine, 2018). While Tulsa, Oklahoma, provides a unique context to examine the impact of promise programs on Native American student attainment, it is also possible that the cooperation of local higher education institutions with tribal authorities could be a tool for promoting college access and success for Native American students more broadly. In future research, scholars should examine whether the cooperation of tribal authorities and higher education institutions can promote college access and success, outside of the context of Oklahoma. At the same time, the finding that improvements in postsecondary attainment for Black students were short lived warrants further attention among researchers and Tulsa Achieves officials. The results could be a function of greater labor-market engagement among Black individuals during the economic recovery; in other words, Black students could have pursued work rather than college enrollment and persistence as the economy began to strengthen. It is also possible, however, that Tulsa Achieves was less helpful for Black students, who face deeply embedded structures of oppression, which inhibit their educational opportunity (Kelley, 2018). Given that Tulsa has a deep history of antagonism against Black people epitomized in the Tulsa Massacre of 1921 and persistent racial segregation in K–12 schools, it is imperative to examine how programs like Tulsa Achieves could alleviate or perpetuate structures of oppression that drive racial disparities in educational attainment (Hirsch, 2003). The interventions embodied in Tulsa Achieves may need to be more comprehensive to combat the structures of oppression that affect Black people in the Tulsa community. Compared with Native American students, we found less evidence of comprehensive targeted supports for Black and Hispanic students. While we found one scholarship for Hispanic students administered by the Tulsa Community Foundation, we did not find evidence of any scholarship designated for Black students. Likewise, evidence of nonfinancial supports for Black and Hispanic students was limited. TCC houses some affinity groups for Black students (i.e., an African student association and an African American Male Success team) but none we could find for Hispanic students. Providing both financial and nonfinancial supports akin to those available for Native American students could yield even greater outcomes for Black and Hispanic students than the ones detected in this study. Our study also revealed important findings regarding the differential effects of Tulsa Achieves eligibility across the distribution of academic achievement. Overall, the largest positive effects on attainment are concentrated among students with above-average high school GPAs. The significantly lower magnitude of the effect for students with lower high-school GPAs relative to those with higher GPAs suggests the program could benefit from greater academic support systems for students (Bettinger & Baker, 2014; Carrell & Sacerdote, 2017; Clotfelter et al., 2018; Page et al., 2017; Scrivener & Weiss, 2013). To improve degree attainment for students less academically prepared for college, Tulsa Achieves should include greater investment in supports for students with lower levels of academic achievement. For example, tuition-free programs like Tulsa Achieves could follow the lead of programs like the Detroit Promise Path and the Carolina Covenant, where scholars highlight the importance of including student supports such as college coaching in increasing degree attainment (Ratledge, 2017; Ratledge et al., 2019). In fact, in the Carolina Covenant program, which covers the full cost of attendance for students, large positive effects on bachelor's degree attainment were only observed after the program incorporated additional counseling and student support systems (Clotfelter et al., 2018). Therefore, while the students who are already academically prepared for college do well in the Tulsa Achieves program, additional supports are likely needed before the program significantly increases postsecondary outcomes among students who enter college with less academic preparation. Our ITT estimates of the effect of Tulsa Achieves eligibility should be interpreted with some important caveats. First, many students did not take up the program; only 58% of eligible students entered a Tulsa Achieves cohort and only 20% of eligible students ever received financial support from the Tulsa Achieves program while enrolled at TCC. However, we conceptualize the potential effects of Tulsa Achieves as broader than just financial as noted previously. Other potentially impactful elements of the program include the free college message, financial security, a student success course, volunteer hours, and satisfactory academic progress standards, as well as incentives to transfer to nearby 4-year colleges through scholarship programs designated for Tulsa Achieves students. This helps explain why so many students are actively completing volunteer hours and requirements for a program they do not currently benefit from financially. Another important caveat for our findings is the possibility that Tulsa Achieves diverted students from 4-year colleges to TCC. Given that we examine longer-term outcomes including vertical transfer and degree attainment, our results are conditional on these enrollment shifts. After examining potential shifts in enrollment, we did find some evidence to suggest that Native American, White, and Asian students who would otherwise have attended two nearby 4-year universities instead decided to enroll in TCC due to Tulsa Achieves. This finding is consistent with prior evidence of some diversion away from 4-year institutions because of free community college programs (Carruthers & Fox, 2016; Gurantz, 2020). This presents both a challenge and an opportunity for our study. First, it presents a potential challenge to validity if these changes in enrollment introduce selection bias. While it is impossible to estimate precisely the potential for selection bias to drive our results, we explore this possibility empirically and test the sensitivity of our results in the robustness checks section. We also note that this diversion of enrollment presents a unique opportunity to examine whether students who attend a 2-year college instead of starting at a 4-year college end up transferring and completing a bachelor's degree. This evidence is consequential for contemporary debates on the design of free-college programs and provides some support for programs with a similar design. It is important to emphasize, however, the comprehensive nature of this free community college program. The findings from this study may not be generalizable to programs that only offer free tuition to attend community colleges without the added supports. Prior literature suggests that the transfer-related supports, including scholarships and articulation agreements likely played an important role in the positive results we observed (P. Gándara et al., 2012; Monaghan & Attewell, 2015). Another important design characteristic is the last-dollar structure of the aid, which created an environment in which the beneficiaries of the program are disproportionally White and higher income. In addition to the distributional implications of allocating the most financial support to students who have less financial need, it is possible that this element of the program design makes it less efficient. As previous studies have shown, when financial aid is allocated to students who need the aid the most, rather than on the basis of academic merit, it is more effective at increasing degree attainment levels (Dynarski, 2000; Dynarski et al., 2018; Dynarski & Scott-Clayton, 2013; Heller & Marin, 2002). Consistent with the literature on academic support programs like the Detroit Promise and the Carolina Covenant, is possible that the aid awarded to more advantaged students could be better spent on providing additional support (e.g., for books, housing, food, transportation) to students with the greatest need. Moving forward, scholars should attend to the role that program design, like last-dollar structures and the integration of student support systems, plays in promoting longer-term outcomes such as degree attainment, vertical transfer, and labor market success for different groups of students. Finally, our study makes an important contribution to the decades-long discussion concerning the democratizing and diversionary roles of community colleges (Schudde & Brown, 2019). In particular, our study reveals that a program that incentivizes community college attendance improves longer term outcomes, including bachelor's degree attainment. In summary, we find that Tulsa Achieves had an especially large, positive effect on postsecondary attainment for racially minoritized students, which is a step in the right direction toward closing gaps in racial/ethnic attainment levels. Elizabeth Bell https://orcid.org/0000-0003-1021-9287 Denisa Gándara https://orcid.org/0000-0001-5714-5583
10.3102_00028312211047854	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/00028312211047854	How Finance Reform May Alter Teacher and School Quality: California's $23 Billion Initiative	 Gains in school spending helped to lift achievement over the past half century. But California's ambitious effort—progressively distributing $23 billion in yearly funding to poorer districts—has yet to reduce disparities in learning. We theorize how administrators in districts and schools, given organizational habits and labor constraints, may fail to move quality resources to disadvantaged students. We identify the exogenous portion of California's post-2013 reform, finding that schools receiving progressively targeted funding tended to hire inexperienced teachers and disproportionately assign novices to courses serving English learners. New funding expanded the array of courses in high schools, as access to college-preparatory classes by English learners declined. These unfair mechanisms operated most strongly in high-needs schools serving larger concentrations of poor students.	 New infusions of school funding often work to raise average levels of achievement, including learning gains enjoyed by disadvantaged pupils (Candelaria & Shores, 2019; Hyman, 2017; Jackson et al., 2016; Lafortune et al., 2018; Rothstein & Whitmore, 2021). But spending increases have not consistently narrowed achievement gaps as defined along familiar lines of children's race or social class (e.g., Hanushek et al., 2020). Such is the case with California's ambitious effort to progressively fund school districts that serve large shares of poor students. Lower income districts in California have been unable to make progress in reducing disparities in reading or math scores (Warren & Lafortune, 2019) despite a significant boost in new funding for each high-need student, gaps that grew wider during the pandemic. This article examines how the distribution of teacher and classroom inputs, along with organizational practices, help explain why even large gains in state funding may fail to reduce achievement inequities. Pressed by then-Governor Jerry Brown, the California legislature approved in 2013 a weighted-student formula (Local Control Funding [LCF]) that soon boosted annual spending by $23 billion, sending the lion's share of new funding to districts serving large shares of English learners (ELs) and pupils from poor or foster care families (Lafortune, 2019). We know that pupil achievement rose, on average, in the initial years following the reform (at least in high schools; Johnson & Tanner, 2018). Their study showed that poor students displayed slightly higher gains in 11th-grade math scores (0.19 standard deviation [SD]), compared with those of nonpoor peers (0.07 SD), effects attributable to the state's new allocation formula. But no significant progress was observed across racial or social class groups in reading or high school graduation rates. The theory-of-action tied to California's weighted-pupil finance formula (WPF) assumes that new dollars or teacher resources will reach and elevate the learning of disadvantaged students. More experienced or effective teachers, for instance, will presumably be allocated by district or school-level managers to low-achieving children, undoing historical misallocation of instructional resources (Goldhaber et al., 2015). Or progressively targeted funding may aim to offer tutoring support or widen access to college-preparatory courses, enriching the opportunity to learn (Darling-Hammond, 2001; Oakes, 1990). But our conceptual framework—drawing on input allocation and organizational sociology perspectives—emphasizes how resources distributed among and within schools by (constrained) administrators may impede the progressive distribution of new resources to intended students. We closely follow recent methodological advances (Jackson et al., 2016; Johnson & Tanner, 2018) to first identify the exogenous portion of change in district-level spending on instruction during the initial 4 years of LCF implementation. Then, we estimate the effects of exogenous gains experienced by California's local districts on their resource allocations between schools and among student groups within schools. We conceptualize this multilevel framework, guiding observation of how teacher and instructional resources are distributed among districts, between schools within districts, and among student groups inside schools—helping to answer whether finance reforms animate inputs and organizational practices proximal to student learning. We also emphasize the importance of distinguishing heterogeneous effects of finance reforms among schools within districts that serve varying types of students (utilizing quantile regression), which Johnson and Tanner (2018) earlier urged but did not tackle. We also focus on the allocation of teachers (of varying qualities) and the allocation of college-prep curricula to exemplify how resources are variably allotted among differing student groups within schools. We find that the LCF-induced increases in district spending led to a rising propensity to hire inexperienced or probationary teachers, along with long-term substitutes. The infusion of new state dollars also spurred greater hiring of White teachers, mainly in schools serving the poorest students. LCF-induced funding increases led to a modest shrinkage of average class size and increased the total number of distinct courses offered in schools. But the latter effect reduced the share of courses qualifying as college-preparatory, and occurred in schools previously most college-prep oriented. In these ways, we discover heterogeneous effects of reform observed among differing schools within districts. We also found that ELs—already displaying less access to experienced teachers and college-prep courses—experienced worsening access during the initial 4 years of LCF implementation (2013–2014 to 2016–2017). Consistent with these descriptive results, our estimation findings confirm that California's finance reform worsened ELs' access to experienced math teachers and college-prep courses in high-needs schools. Overall, when progressive financing helps close teacher quality gaps between schools, organizational practices inside schools may regressively allocate less experienced teachers or fail to enrich course offerings for disadvantaged students. This article details how such allocation patterns (or social-mediational processes), set in motion by California's massive finance reform, can work against the narrowing of achievement gaps. We first review prior work that associates finance gains with achievement change, along with recent studies on patterns of between-school allocation of school inputs. Second, we summarize what's known about decentralized and variably targeted finance reforms, especially WPF. Third, we describe California's ambitious reform, highlighting potential threats to equitable resource distributions at three levels of the system (among districts, between schools within districts, and among student groups within schools), putting forward a conceptual framework that prompts our specific empirical questions. Scholars continue to find achievement effects stemming from state-initiated gains in spending, typically aimed at lifting average student performance or narrowing disparities in learning. Recent work, for instance, details how per-pupil spending grew by over half in the poorest quintile of school districts nationwide between 1990 and 2012, versus just under one third in the most affluent fifth of all districts (Lafortune et al., 2018). Encouragingly, students in the poorest districts displayed modest gains in achievement (about 0.10 SD over a decade, depending on grade level and subject) following discrete jumps in state spending. Candelaria and Shores (2019) also provide evidence on the positive benefits of court-ordered finance reforms, 1989–2010, on high school graduation rates in high-poverty districts. Similarly, Jackson et al. (2016) worked from randomly timed finance litigation among states, from the 1970s forward, investigating whether pupil achievement climbed in postreform periods. Conducting an event study with instrumental variables (IVs), they estimated that a 10% boost in spending per pupil over 12 grades resulted in learning gains corresponding to one third of a year in additional schooling, along with a 7% bump in graduates' wages downstream. These benefits were discernibly greater for students from poor families. Jackson et al. (2021) also found that exposure to $1,000 per-pupil spending cuts during the Great Recession resulted in about 0.04 SD lower test scores and declining college-going rates. Discernible benefits of modest magnitude have been estimated for California's LCF reform, first underway in the 2013–2014 school year. Johnson and Tanner (2018) found that the exogenous portion of dollars tied to the reform predicted significant gains in graduation rates and 11th-grade test scores in math and English language arts (ELA) over the initial 3 years of implementation. Students from poor families made slightly stronger gains in math than middle-class peers, but not in reading. A positive relationship between per-pupil spending and mean student achievement has been observed in other single-state studies, including cases from Michigan (Hyman, 2017; Papke, 2008; Roy, 2011), New York (Gigliotti & Sorensen, 2018; Lee & Polachek, 2018), Massachusetts (Guryan, 2001), and Ohio (Kogan et al., 2017). In sum, discrete gains in per-pupil spending appear to raise average levels of student achievement among disadvantaged children—money does matter in this regard. But often, achievement for middle-class students climbs as well, yielding little progress in narrowing disparities in learning. Nor do we understand how rising spending might work to narrow inequities in student learning among schools within districts or among pupils within schools. Thus, we push on the issue of how new infusions of dollars may work to alter the attributes of teachers deployed inside schools, class size, or the more equitable structuring of curricular offerings—specific input allocations and organizational practices that mediate the arrival of new funding and operate proximal to student learning. Researchers have long tried to identify discrete inputs that display the greatest predictive validity (efficiency) vis-à-vis achievement (e.g., Fuller & Clarke, 1994; Hanushek & Woessmann, 2017; Hedges et al., 2016). But attention to mediators is rarely tied back to state-level finance reforms; nor is it observed in terms of how district or school-level managers variably allocate inputs among and within schools. A handful of scholars have begun to focus on mediating mechanisms and organizational practices inside districts or schools ("technical efficiency" in economics parlance), which finance gains may help to animate. For instance, Jackson et al. (2016) explored the mechanisms through which reform-induced spending gains improved long-run adult outcomes such as educational attainment and earnings. This team found that exogenous spending increases were associated with sizeable enhancements in school inputs, including reductions in pupil-teacher ratios, increases in teacher salaries, and longer instructional time, which may in turn have driven positive downstream outcomes seen among graduates. Lafortune and Schönholzer (2018) found significant effects of school construction on achievement, as mostly disadvantaged children moved into new facilities in Los Angeles. Shifts in class size and teacher composition, which differed between aging and new campuses, did not explain pupil-level effects. But reduction in overcrowding did significantly mediate learning gains, resulting from a $19.5 billion boost in capital spending by the L.A. school district. Klopfer (2017) exploited the random incidence of state finance reforms since the 1970s to estimate change in indicators of school quality and instructional time, possible mediators that drive achievement gains. He found that finance infusions did not lead districts or schools to hire more or better qualified teachers. Yet more funding did lengthen the count of instructional days, an organizational shift that yielded real consequences for students. Klopfer reasoned that district managers seek to minimize labor costs: Lengthening instructional time was half as costly as hiring additional teachers. District and school administrators, of course, are constrained by labor supplies or union agreements as they attempt to distribute new inputs among and within schools. Brunner et al. (2020), for example, investigated whether the local strength of teacher unions affected allocation of differing school inputs stemming from finance reform. They documented that districts with stronger unions increased education expenditures nearly one-for-one with reform-induced increases in overall state aid and then drove larger allotments to teacher compensation. On the other hand, local school boards facing weak unions applied new state aid mainly to property tax relief and spent remaining funds on hiring new teachers. States may display less concern for efficiency under some reforms. In California, the governor and legislature placed rhetorical priority on narrowing achievement gaps, dramatically shifting new dollars to districts that serve greater concentrations of poor students. In turn, Johnson and Tanner (2018) found significant gains in teacher salaries and instructional spending, higher pension contributions by districts, and modest reductions in pupil-teacher ratios in the wake of the 2013 reform. Still, we know little about the attributes of newly hired teachers when new funding arrives to local districts, whether they are deployed to lift low-achieving students, and whether organizational practices or curricular structuring inside schools shift to benefit poor students. The California reform has moved billions of dollars to districts serving these children, a distributional policy reinforced by Governor Gavin Newsom in the wake of COVID-19. But has this ongoing boost in state funding discernibly altered the allocation of school resources to varying teaching staff, classroom organization, or curricular practices within districts and within schools to narrow achievement gaps? Policy activists and scholars have embraced WPF over the past quarter century, aiming to adequately fund schools, while narrowing achievement gaps. WPF funding allocates dollars to districts or schools based on pupil characteristics, distributing additional resources to districts that serve high-needs students. This strategy recognizes that certain pupils require greater resources to meet state proﬁciency standards (Roza et al., 2020). "Because not all students come to school with the same individual, family, or neighborhood advantages, some need more resources than others to meet a given achievement standard," as put forward by architects of California's finance reform (Bersin et al., 2008, p. 5). This marks a shift from equal to greater funding for poor pupils, as policy leaders struggle to narrow learning disparities. WPF strategies often replace centrally regulated funding for particular programs or students, previously designed and regulated by state government, while returning budget authority to local school boards (Augenblick et al., 1997; Odden & Picus, 2014). This decentralization-with-progressivity appeals to district administrators who gain discretion and unions leaders who welcome new (unregulated) monies placed on the bargaining table, at least in districts serving large shares of disadvantaged students. On the other hand, some equity advocates worry about fiscal decentralization, fearing diminished accountability over funds once focused on poor children and regulated by the state. The original argument for categorical aid, going back to the Civil Rights era, was that dollars would drift to schools in politically stronger neighborhoods if left unregulated. Evidence remains mixed on the extent to which WPF strategies shift the allocation of inputs among schools within districts or to intended students within schools. Examining 19 districts employing WPF strategies, Roza et al. (2020) report that study districts afforded to schools variable flexibility on the number and type of staff to hire, while 16 of the 19 districts exercised compensation-related flexibility to use their WPF funds to award stipends to staff who take on additional work. Hawaii's WPF model built from a 1989 governance change that granted principals wide discretion over how state funding can be allocated within schools. Three years later, the legislature approved "lump-sum" budgeting, collapsing many categorical programs into a simple block grant to each school. In 2004, state allocations to schools were then weighted by counts of disadvantaged students. This did effectively move new dollars to schools serving larger shares of the most challenging pupils, although no medium-term effects on achievement could be detected (Levin et al., 2013). In contrast to Hawaii's phase-in of WFP-related flexibility at the school level, California did not prepare local principals, teacher leaders, or site councils for their renewed budget authority and implementation challenges. Another WPF experiment, attempted in Prince George's County, triggered only a slight redistribution of resources to poor pupils. This district set aside about one fourth of its operating budget to be allocated via a pupil-weighted formula, compared with nearly two thirds in Baltimore and Houston (Malen et al., 2017). Prince George's "unlocked" for principal allocation only certain teaching posts, while most remained centrally allocated. And the district had a "soft-landing provision" that largely held harmless schools with higher per-pupil spending (driven by their enjoying greater shares of experienced and higher paid teachers). Slight equity effects could be detected in terms of per-pupil allocations among schools, based on student attributes. Miles and Roza (2006) found that the devil lies in policy details, after studying WPF schemes in Cincinnati and Houston. The share of district budgets to which pupil weights are applied, niceties of the school-allocation formula, and highly institutionalized ways of distributing teaching posts worked to undercut redistributive effects. Two related studies reveal how the L.A. Unified School District progressively allocated more new dollars to high schools serving greater concentrations of poor students in the wake of LCF funding (Lee & Fuller, 2020; Partnership for Los Angeles Schools, 2018). But per-pupil spending climbed equally among elementary schools, whether situated in middle-class or poor neighborhoods. Governor Brown in California clearly targeted new dollars on districts serving greater shares of poor students. But his legislation placed no statutory requirement on how districts could spend their new dollars or whether schools serving higher concentrations of disadvantaged students should receive greater funding. The governor resisted efforts by the American Civil Liberties Union and equity advocates to require that local boards report on which schools received new revenues. This led some to ask whether this $23 billion reform would actually narrow achievement gaps, along with the extent to which district or school-level managers were focusing inputs, teaching staff, and practices on poorly achieving students. We are bringing government closer to the people, to the classroom where real decisions are made, and directing the money where the need and the challenge is greatest. (Brown, 2013) The Golden State's finance reform swept aside scores of categorical aid programs in the summer of 2013, along with local revenue limits set earlier by Proposition 13. The state's WPF continues to include three funding tiers. The base grant originally provided equal dollars per pupil set at $6,900 per K–6 student, $7,200 per middle school student, and $8,300 for each high school pupil, adjusted for inflation each year (Legislative Analyst's Office, 2013). Supplemental grants are sent to districts, equaling 20% of the base grant for each student from a poor family, designated EL, or child in foster care. Concentration grants further increase per-pupil distributions for districts by an amount equal to 50% of the base grant for each additional disadvantaged student after their representation surpasses 55% of district enrollment. These per-pupil grant levels have increased substantially since 2014, due to inflation allowances or legislative adjustments to base, supplemental, or concentration grant levels. Regulations set statewide and pressed by pro-equity advocates, require that districts expand or improve services for students generating supplemental and concentration grants in proportion to their enrollment share, hoping to preclude substitution of earlier district investments. California's resurging economy after the post-2009 recession, along with a constitutional set-aside for K–12 spending, spurred a rapid climb in education appropriations. This helped ease political resistance to giving larger shares of new dollars to urban districts and those serving large shares of disadvantaged students. Spending per pupil had declined by one fifth through the Great Recession. K–12 spending then climbed by $23 billion yearly, rising to $61 billion in 2018–2019. Per-pupil spending reached an all-time high ($11,645) prior to the short-lived COVID-spurred recession. (California still falls well below states like New York and Massachusetts in per-pupil spending.) In 2021, enjoying another bounce back of the economy, along with federal stimulus dollars, the legislature increased the share of dollars distributed to districts through concentration grants, another attempt to narrow learning disparities, made worse by the pandemic. The bulk of state education funding (88%) now flows through the WPF mechanism, with remaining dollars moving through categorical aid. The progressivity of LCF allocations means that districts serving larger shares of disadvantaged students have received most of the new state dollars. Districts with enrollments of fewer than 25% disadvantaged students enrolled received a 5% bump in state revenues per-pupil between 2012 and 2019, compared with a one-third gain for districts with at least 80% of enrollments made up of disadvantaged children (Fensterwald, 2016). California's LCF reform intended to narrow stubborn disparities in children's learning, defined along familiar lines of race and social class. This bold initiative aimed to raise both absolute levels of funding across districts and the progressivity of spending with relative revenue increases for high-need districts. This prompts the parallel—empirically testable—distinction between how this reform, progressively distributing new funding to districts serving disadvantaged students, may have (1) improved the absolute mean levels of student proficiency and (2) resulted in relative achievement gains for targeted students, shrinking achievement disparities among subgroups. If relative levels of achievement did change over time, what mediators can be identified to explain how greater equity was achieved? Gains in absolute levels of student proficiency have been observed for high school math and reading (Johnson & Tanner, 2018), and for third-grade reading test scores (Warren & Lafortune, 2019) during the initial 4 years of LCF implementation. This is consistent with the prior literature: New infusions of state funding can raise average achievement levels. But did this ambitious restructuring of school finance narrow gaps in achievement among student subgroups? Panel A in Figure 1 reports the percentage of high school pupils, split between ELs and English-proficient students, who met or exceeded the proficiency standard in math. This percentage for ELs equaled 13.5% in low-need schools and 1.7% in high-need schools in 2014 (first year of Smarter Balanced testing), compared with 44.0% and 15.7% of English-proficient peers. These gaps did not move significantly through 2017. The pattern is similar when splitting students by race or economic status; disparities actually widened in high-needs schools. Warren and Lafortune (2019) report similar trends for students in Grades 3 to 7. Their district-level analysis found that gaps in math scores grew as students progressed from third to sixth grade. This is consistent with the earlier findings from Johnson and Tanner (2018)—despite short-term gains in 11th-grade achievement on average, they detected little evidence that learning disparities are closing in California, as intended by policymakers. Thus, what motivates our study is the question of why—or through what allocational routes—the flow of new spending to districts has failed to narrow disparities in achievement within schools. We take up Johnson and Tanner's (2018) call to delve into LCF's "effects on intra-district school resource allocation decisions and resultant effects on student achievement gaps" (p. 27). Next, we offer a conceptual framework that locates where inputs are variably distributed, along with organizational practices (that may shape technical efficiency), from within three levels of the education sector, acting to remedy or reinforce learning inequities. The framework leads to our discrete research questions. Figure 2 puts forward a simple conceptual frame for locating the institutional levels and mediators through which resources may, or may not, be distributed to benefit disadvantaged students. This model gives shape to a pair of working hypotheses. First, district and school administrators face varying constraints in (or threats to) their capacity to enter teacher labor markets and hire experienced, high-quality teachers. Then, as they allocate teachers and instructional resources within schools, and adjust curricular structures, disadvantaged students may not benefit from these inputs or organizational practices relative to stronger pupils. In short, we ask how threats to teacher acquisition and deployment, along with organizing actions by principals, may attenuate the benefits of finance reforms. First, local school boards and district managers are variably constrained in their ability to recruit experienced and high-quality teachers, then to distribute this resource fairly among schools. Even as districts with large shares of poor students enjoyed new funding, they may have drawn less expensive teachers from the labor market or allocated them in nonprogressive ways, constrained by staffing formula or labor contracts, rather than surgically focusing on high-needs schools. We know, for example, that some districts defined "equity" as evenly spreading new resources across constituent schools, while others progressively focused new teachers and dollars on campuses with high concentrations of weighted students (Allbright et al., 2019). Second, on the demand side, district leaders may prefer less costly teachers, as they enter the labor market to acquire new instructors. Larger urban districts, for instance, struggled to get back to class sizes and staffing levels present before the post–2009 recession. This placed stiff cost pressures on districts among competing priorities, as costs related to special education, pension plans, and health insurance continued to climb (California State Auditor, 2019). Thus, selecting less expensive teachers may have seemed rational to cost-conscious administrators. Third, on the supply side, experienced teachers may prefer to exit high-needs schools, migration fostered by seniority rules. Higher rates of teacher turnover in high-poverty schools lead to ample job openings, often filled by younger, novice teachers—a perennial problem in urban districts like Los Angeles (Fuller et al., 2016). Across California districts, Lafortune (2019) found a widening gap in teacher experience levels between high- and low-poverty schools in the wake of LCF implementation. Teacher experience is not the only factor that may narrow achievement gaps, but it plays a significant role (e.g., Ladd & Sorensen, 2017). What remains unknown is whether schools benefiting more from funding gains came to rely on inexperienced or nontenured teachers, stemming from these demand and supply constraints. The distribution of collateral instructional resources—say, the capacity of administrators to lower class size or reduce daily teaching loads—may also be constrained by institutional habits or labor rules. So even when new state dollars arrive at districts, the progressive distribution to high-poverty schools or less-advantaged pupils may be sharply constrained. It's the play of these inputs and organizational practices that may mediate the efficacy with which new funding operates to narrow achievement disparities. Our conceptual frame also highlights distributional decisions or practices inside schools that may result in nonprogressive distributions. Once new teaching posts or instructional dollars arrive at a school, principals play an influential role in their assignment to student groups (Grissom et al., 2017). We know that substantial variation within schools operates in terms of teacher experience or (value-added) effectiveness, at times corresponding to student attributes (Clotfelter et al., 2006). Principals may prefer to retain experienced teachers, yet also face constraints set by seniority rights, along with limited supply of teacher candidates available to replace sage teachers who migrate to suburban schools. To retain experienced teachers, principals may reward them by assigning courses that host higher-achieving pupils (Kalogrides et al., 2013). In turn, principals may assign novice teachers to low-performing students, based on seniority or institutional customs. We focus on teacher allocation and curricular structuring by examining possible changes in who teaches ELs, along with proportional access to college-prep courses by these students. We know, for example, that ELs (and other disadvantaged students) tend to be tracked into a variety of elective courses, rather than more rigorous classes (Gándara et al., 2003). On the supply side, principals in high-poverty schools may be unable to find additional teachers prepared to offer Advanced Placement (AP) or college-prep courses (Oakes, 2005). Principals may face strong incentives and fewer supply constraints to simply expand elective courses, rather than creating college-prep offerings for disadvantaged students. We emphasize that such threats to the progressive distribution of new resources may operate within district offices, as teacher posts and classroom resources are allocated among schools; and within schools, as principals assign resources to differing student groups and curricular structures. Overall, we test whether between-school or within-school distributional patterns served disadvantaged students during the initial 4 years of LCF implementation. Or perhaps pro-equity mediational inputs and practices were not sufficiently animated to narrow disparities in achievement. We empirically examine these specific questions: Research Question 1: Do LCF-induced increases in district funding (per pupil) help explain changes in the level and distribution of school resources among districts, including attributes of teachers, their working conditions, and curricular structures (between-district resource allocations)? Research Question 2: To what extent do the effects of LCF-induced funding gains on school-level resources vary among schools within the same district (between-school resource allocations)? Research Question 3: Do LCF-induced increases in district funding help explain school-level changes in the access of ELs to experienced teachers or college-preparatory courses within schools (within-school resource allocations)? Core information comes from data compiled by the California Department of Education (CDE), yielding measures of district-level revenues and spending, control variables, and outcomes measures related to teacher characteristics, organizational practices, and curricular structure within schools. To exploit variation between schools, nested in all California school districts, we built a school-by-year-level panel data set, 2003–2004 to 2016–2017, for 6,867 elementary and high schools (excluding charter schools) situated in 941 districts. LCF "snapshot data" and district-level revenues and expenditures drawn from the standardized account code structure files were used to estimate the funding-formula-induced exogenous increases in district expenditures (CDE, 2018a). Data on district enrollment (unduplicated counts), which drive supplemental and concentration grant allocations to districts, come from the LCF funding snapshot data. Monthly statements of general fund cash receipts and disbursements from the state's controller, including general fund spending, are used to construct counterfactual trends in district per-pupil revenues, exploiting the exogeneity of the onset of the LCF reform. To break down standardized account code structure financial data into meaningful categories, we borrowed definitions of expenditures utilized by Loeb et al. (2006), distinguishing instructional spending from other categories. This bin for student spending excludes district expenditures distant from classroom instruction, teacher salaries, or student support services (e.g., debt service, capital, or facilities). The California Longitudinal Pupil Achievement Data System (CALPADS) provides yearly data for teacher and staff demographics, pupils nested in courses (or elementary homerooms), and staff assignments by course, allowing us to construct a variety of outcome measures related to teacher characteristics and organizational features at the school level (CDE, 2018b). We linked CALPADS staff data files for each school year: (1) teacher demographics, experience, and credentials; (2) counts of FTE (full-time equivalent) teachers; (3) teacher assignments to course and students; and (4) course enrollment data. Using CALPADS data, we generated school-level outcome measures for school-by-year panel data (2003–2016), including 79,688 school-by-year observations of 5,764 elementary schools and 14,972 observations of 1,103 high schools. Table 1 reports descriptive statistics for our three sets of outcome variables—teacher characteristics and working conditions, and curricular structure—for elementary and high schools statewide. We include two control variables, enrollment counts and percentage of students eligible for free or reduced-price meals (FRPM). Data are split between the highest-poverty quintile (Q5) of schools and the lowest-poverty (economically best-off) schools (Q1), based on 14-year averages. These data, including percentage of pupils FRPM eligible, were drawn each year from the CALPADS data. For elementary schools, we observe higher mean enrollments in high-poverty schools (627), than in low-poverty schools (558). The reverse is true for high schools. These attributes were aggregated to the school, calculated from CALPADS data. This affords information on teacher counts by school and year, and by whether teachers were new to the district in a given year, ethnicity, novice status (two or fewer years of experience), probationary or tenured status, and attainment of a master's or higher degree. Profiles of teachers differed between schools serving low- or higher income families. Only 39.2% of teachers in Q5 high schools held a master's degree or above, compared with 48.6% in Q1 high schools. The ethnic composition of teachers differed sharply: Under half the teachers in Q5 elementary schools were White (47.6%), compared with 84.5% of peers in Q1 elementaries. High schools displayed similar differences. Discrete organizational facets were calculated from CALPADS course-level data, including mean class size (aggregated to the school level) and mean number of class periods taught by each teacher, split for math and ELA classes, as measures of working conditions. We see in Table 1 that mean class size is slightly smaller in high-poverty (Q5) high schools in ELA and math classes, than in low-poverty (Q1) schools. However, no significant differences in the mean number of class periods assigned to ELA or math teachers appear between Q1 and Q5 schools. The distribution of certain inputs, say, the allocation of teaching posts among schools, is tied to district-level policies. Goldhaber et al. (2019), for instance, found that two thirds of collective bargaining agreements in California set allowable ranges of class size, one third specified daily teaching loads. In addition, principals and school-level practices play a role, including which new teachers are hired with varying characteristics and for which subject matter areas, along with how teachers are allocated within schools to varying courses, AP and college-prep programs, or between differing types of students, including those with varying English proficiency, issues to which we return below. CALPADS course-level data were used to generate the total number of courses and shares of classes designated as AP, and courses approved by the University of California for possible admissions. The latter fall into so-called "A to G" course categories: core academic subjects, such as English; math; and lab sciences (AP and A-G courses, college-preparatory). The curricular structure of schools differs between Q1 and Q5 campuses. Those hosting higher income families offered over 78 differing courses over the period, on average, compared with 61.1 in low-income schools. Low-poverty high schools offer more college-prep courses than high-poverty schools. The average percentage of math classes that qualify for the A-G designation is much lower in Q5 high schools (49.8%), than in Q1 high schools (66.5%). To capture how students are differentially assigned to instructional resources within schools at the third analytic stage, we linked CALPADS course enrollment data to teacher attributes via each teacher's unique ID, available for 2012–2017. We then constructed measures that capture ELs' access to instructional resources within schools. We calculated a simple index that equals the mean percentage of enrolled students who are ELs in classes taught by novice teachers (2 or fewer years of experience), minus the mean percentage taught by experienced teachers (more than 2 years) within each school. The same measure is constructed for classes taught by nontenured and tenured teachers. Table 2 reports descriptive statistics for these proportional differences. ELs' access to experienced or tenured teachers differs little in elementary homerooms. But novice or nontenured teachers are more often assigned to classes with greater shares of EL students in high-poverty high schools, compared with low-poverty schools. For example, math classes in high-poverty high schools taught by novice teachers enrolled 6.5% more ELs on average compared with those taught by experienced teachers in the same school. The mean difference measure equaled 2.6% for low-poverty schools. We generated a similar measure to summarize ELs' access to A-G classes: the mean percentage of enrolled students who are ELs in classes approved as A-G, minus the mean percentage in classes not approved as A-G by school. The math classes approved as A-G in high-poverty schools had 18.4% fewer EL students on average compared with non-A-G math classes in the same school. This gap ranges lower in low-poverty schools. These gap measures capture the disparate allocation of instructional resources, including change in curricular structuring, within schools—events likely manipulated by principals under organizational or labor constraints. We replicate school-level teacher quality indicators for experience and educational attainment, used by Johnson and Tanner (2018), adding detailed aspects of teacher employment status. We also examine the differential effects of LCF policy on different schools within districts (Johnson & Tanner, 2018, assumed commonly shared effects). The key challenge in estimating effects that stem from progressively targeted finance, including WPF-style initiatives, is that school spending is an endogenous treatment. That is, school spending tends to be associated with unobserved time-varying or time-invariant school-level factors, either attributes of students or parental selection, forces that likely drive both school-level outcomes and funding gains. Thus, we must attempt causal identification by isolating the exogenous spending changes induced solely by the finance reform. Two sources of exogeneity have been exploited to identify reform-induced changes in per-pupil spending: the timing of reform events and the state's funding formula. The novelty of Johnson and Tanner's (2018) methodology lies in leveraging both sources of exogeneity by conducting an event study with a simulated IV approach. The IVs for estimating the effects of LCF on student outcomes are (1) the number of school-age years a student was exposed to the LCF policy (exposure) and (2) the reform-intended fully funded amount of district per-pupil spending from the state (dosage or simulated IV). While the former relies on the timing of the reform event being random or arbitrary, the latter exploits the availability of the formula for the intended allocation of funding, along with the variables the formula was based on. Let us review how Johnson and Tanner (2018) exploit these two sources of exogeneity to tease out only the LCF reform–induced funding increases and use the exogenous variation to estimate potential effects of spending increases on average student outcomes at the school level. Their design consists of three estimation steps: (1) prediction of the counterfactual district per-pupil revenue in the absence of LCF, (2) estimation of the LCF-induced exogenous increases in district per-pupil expenditure, using dosage (or simulated IV), exposure, and the predicted revenues from the first step, and (3) estimation of the effect of the LCF-induced exogenous increases in district per-pupil expenditure on averaged school-level outcomes. Steps 2 and 3 correspond to the first and second stage of two-stage least squares IV estimation (2SLS-IV). This involves carving out a part of variation that is exogenous in the first stage, then using only that part to estimate causal impacts on the outcome in the second stage. Step 1 is necessary for the causal identification when combining the 2SLS-IV with an event-study framework. The unobserved time-invariant district or school-level confounders and statewide time trends in outcomes might be addressed by incorporating a variety of fixed effects and by instrumenting the district per-pupil spending with dosage and exposure. Still, unobserved time-varying confounders remain, namely, the dynamic effect of economic conditions on district revenue and expenditures, which might confound the relationship between the LCF policy treatment and changes in school-level outcomes over time. To account for these time-varying confounders, Johnson and Tanner (2018) modeled the predicted counterfactual evolution of K–12 revenues in the absence of LCF. The rapid acceleration of state K–12 spending may stem both from the legislated framework under LCF and California's Proposition 98 funding guarantee, requiring that 38% of the state budget go for schools and community colleges. As California's economy continued to expand in the postrecession period, district per-pupil revenues would have grown without LCF, due to Proposition 98 and the growing state treasury. Johnson and Tanner (2018) directly predict this counterfactual trend based on prior funding and statewide California spending on non-K–12 expenditures. Figure 3 shows gains in observed per-pupil revenues during the prerecession years (2003–2006), along with dramatic reductions during the recession (2007–2012). Predicted values in the post-LCF period (2013–2017) suggest that average per-pupil revenues would have increased without LCF, as expenditures recovered from the post-2009 recession. The gap between observed revenue and predicted levels in post-LCF years can be seen as an exogenous increase in per-pupil revenue due to the reform. Also, recall how the LCF reform dramatically shifted the distribution of new funding to districts serving greater shares of disadvantaged pupils. Our analytic approach replicates Johnson and Tanner (2018)'s three-step research design, acknowledging that their methodology is a generalized strategy for tracing the effects of finance reforms. We replicated Steps 1 and 2, with a single exception: We separate district total spending into student and nonstudent spending and use only student spending for the predictor of focal interest. Student spending includes teacher salaries, instructional materials and supplies, special education, and pupil services. We estimate the effect of LCF-induced per-pupil student spending increases, which are more integral to pupil experience inside schools. Our estimation method diverges from Johnson and Tanner (2018) at Step 3: We do not estimate the effect of LCF-induced spending increases on birth-cohort-specific pupil outcomes. We estimate the effect on changes in teacher inputs and organizational practices applied to the school, given our focus on input levels and organizational practices linked to resource allocations. Thus, the analytic unit exposed to the LCF treatment is defined as a school, not a student. This requires a different definition of the exposure variable. Johnson and Tanner (2018) compared change in (school-level average) pupil outcomes between exposed and unexposed "birth cohorts" by district; we compare the change in school characteristics between exposed and unexposed academic years for each district. Both approaches share the identification assumption that the timing of the LCF reform is exogenous to changes in outcomes across different time points. Since Johnson and Tanner (2018) used school-level averages of achievement, the structure of the school-by-academic-year panel data constructed from our design is identical to the school-by-cohort panel data they used, differing only in the time scale. One original contribution appears in Step 3, where we apply a multilevel IV quantile regression approach (Chetverikov et al., 2016) to estimate the heterogeneous effects of district-level LCF policy on school-level outcome distributions within districts. The common approach of estimating the location shift of group-level averages of outcomes may mask important effects on the outcome distribution. In finance studies, district-level increases in per-pupil spending may exert little effect on the district-level average of school-level teacher quality but may still move the lower or higher quantiles of teacher quality distributions within a district. We allow each district to experience a differing treatment effect and estimate these effects on district-level quantiles instead of the district-level mean. Overall, these methodological contributions are motivated by our conceptual framework, emphasizing the multilevel nature of resource allocation. First, we capture the between-school effect heterogeneity by examining whether the LCF-induced district-level spending increases have differential effects at different points (quantiles) of the school-level outcome distribution. Essentially, our difference-in-difference (DiD) estimation model at Step 3 compares the pre-to-post-LCF change in the distinct quantiles of school-level outcome distributions between high- and low-dosage districts. A positive value of the DiD estimate for the 0.2 quantile, for example, would indicate that LCF-induced spending increases boosted the lower tails of the within-district distribution of the school-level outcomes after the reform. Additional methods details appear in the Technical Appendix. Next, we examine effects on within-school resource allocations by constructing unique school-level measures that capture within-school disparities in distributing resources among ELs and English-proficient students. As mentioned above, we use the class-level enrollment data to generate a teacher-quality-gap measure within each school, for example, calculating the difference between the mean percentage of enrolled students who are ELs in classes taught by novice teachers and the mean percentage taught by experienced teachers. A positive value of this measure means that the classes taught by novice teachers are more likely to have higher shares of ELs compared with classes taught by experienced teachers. This class-level gap measure gives a snapshot of the extent to which ELs experience unequal access to experienced teachers within a school. District-wide changes in the share of novice and experienced teachers do not affect its magnitude because this measure is based on within-school comparisons across classes. Furthermore, since this measure is a school-level summary, we can further examine the between-school effect heterogeneity on the distribution of the within-school disparity index. We first examine descriptive trends for three sets of outcomes—teacher characteristics, working conditions, and curricular structure at the school level—prior to LCF and during the initial four years of implementation, beginning in 2013–2014. We first display time trends for teacher characteristics, aggregated to elementary schools statewide, then split between low- and high-poverty campuses (Figure 4). Overall, we observe that the hiring of novice teachers or those new to the district picked up as new state funding came to districts and schools. These staffing profiles, in general, return to prerecession patterns. The post-2013 surge in hiring also led to a modest spike in the hiring of long-term substitutes (not shown); yet this trend line settled back down, as districts and schools hired inexperienced yet credentialed teachers. High-poverty schools tended to rely more on novice teachers in the postreform period, compared with low-poverty schools. One exception appears in Panel D, where high-poverty schools came to employ a higher share of tenured teachers, relative to heavier reliance on probationary (nontenured) staff prior to the post-2009 recession. This may be the result of leveling student enrollment statewide and stabilizing teacher staffs. These time trends are similar for high schools (available from authors). We observe more notable shifts, postreform, relative to the prerecession period, when it comes to teacher working conditions and curricular structure (organizational features of schools). To illustrate the shrinkage of class size, we display this outcome for elementary homerooms, where students spend most of their time (Figure 5). Mean class size declined after the LCF reform, from 25.6 to 24.0 in low-poverty homerooms, and from 24.4 to 23.3 in high-poverty schools, a modest improvement (Panel A). These class sizes remained higher than in the prerecession period, 2003–2008. Turning to high schools, the mean count of instructional periods assigned to teachers (workload) moved up slightly in high-poverty high schools (Panel C) from a low of 3.9 courses each day during the recession to a high of 4.2, postreform. No change in this count is observed for teachers in low-poverty high schools, and their mean count is considerably lower postreform, compared with prerecession workload levels. We see in Panel B (Figure 5) that the mean number of courses offered steadily increased, although counts remain lower in high-poverty schools. The curricular structure was becoming more differentiated in low-poverty schools as well, even relative to the prerecession period. This may be a reaction to the earlier curricular narrowing under No Child Left Behind, including state testing emphasis on math and ELA, along with observed growth in tutorial and special instructional periods during the post-LCF period. Panel D suggests that high schools moved away from college-prep curriculum since the post-2013 infusion of new funding. As the listing of courses grew among high schools statewide, a diminishing percentage were approved as A-G college-prep in character. And this structuring differed between low- and high-poverty high schools. The average share of courses deemed A-G declined from 70.2% to 63.9% during LCF implementation in low-poverty high schools. This diminishing trend was also observed in high-poverty high schools, with the share declining from 54.3% to 49.4%. Additional research is required to discover whether this lowers student odds of entering college-prep courses in high-poverty schools. But the diversification of the curriculum clearly worked against the proportional emphasis on college-prep courses inside schools statewide. The ability of EL students to attend courses taught by experienced teachers and A-G courses also slipped in the postreform period (Figure 6). Panel A shows that the proportional representation of EL pupils in math classes taught by nontenured (probationary) versus tenured teachers varied little in low-poverty high schools. But EL representation in math classes taught by nontenured staff versus tenured staff increased during the postreform period in high-poverty schools. Ideally, this disparity would narrow under finance reform, assuming that more experienced teachers would help reduce disparities in achievement. Panel B displays a similar pattern, with lower representation of EL students attending A-G courses versus other courses in the year prior to the LCF reform, a gap that grows modestly post-2013. The practical magnitudes of these differences are not great. But they are consistently moving in the opposite direction of organizational or curricular shifts that would likely narrow learning gaps, at least for ELs. Next we report DiD estimates of LCF treatment effects for each set of school-level outcomes: teacher characteristics, school organization and working conditions, curricular structure, and ELs' access to instructional resources within schools. Table 3 provides a summary of DiD estimates, [MATH], from Equation 4 in the Technical Appendix. Estimates shown in Table 3 indicate the effect of 1% LCF-induced increase in district per-pupil spending on change in the district-specific conditional quantile of school-level outcomes following the event. We observe that LCF-induced increases in per-pupil spending tended to significantly increase the percentage of teachers newly hired to their district, as well as the share of teachers composed of novice teachers. These findings are consistent with the significant reduction in average years of service in the district. An LCF-induced 10% increase in district per-pupil expenditures during the post-LCF years resulted in a 0.98 percentage point gain in the share of newly hired teachers in elementary schools that previously hosted lower shares of new teachers in the pre-LCF period ([MATH] = 0.2). The effect size diminishes as the quantile level increases to [MATH] = 0.8 in elementary and high schools, which implies that schools with higher shares of new teachers in pre-LCF years hired fewer new teachers post-LCF. Yet we observe that funding increases significantly lifted overall levels of their distributions within districts. The event-study estimates presented in Figure 7 (along with 95% confidence intervals) show the varying effects of LCF-induced funding increases across the 4-year post-LCF period. We only present results from the 0.5 quantile level (median), as patterns were quite similar in the other quantiles. None of the effect estimates in pre-LCF years are significant at the 5% level. This means that the estimated LCF-induced increases in funding were not relevant to changes in outcomes before the reform, lending support to our analytic strategy's ability to isolate the effects of the LCF-induced exogenous funding increases. Effects of the LCF reform on the share of teachers newly hired were felt immediately in the first year of implementation, then peaked in the fourth year (Panel A). This pattern is mirrored by the reduction in average years of service in the district (Panel B). An LCF-induced 10% increase in funding leads to the percentage point increase in the share of teaching staff who were novices after 4 years of exposure to LCF (Panel C). Newly hired teachers, stemming from LCF-induced funding increases, often included nontenured staff, such as novices. Districts relied on hiring long-term substitutes in the first and second years of LCF and then hired more probationary teachers in subsequent years. The magnitude of effects from LCF-induced funding was the largest for the share of teachers with probationary status: A 10% increase in funding leads to a 1.75 percentage point increase in the median share of probationary teachers after 4 years of LCF exposure (Panel E). To put these magnitudes in context, per-pupil spending grew by up to 40% during the post-LCF period in urban districts with high shares of disadvantaged students (California Legislative Analyst, 2018). So these effects hold practical significance in the staffing and organizational structure of many schools. Figure 8 displays heterogeneous effects of LCF for within-district distributions of school-level teacher composition. Panel A shows that an LCF-induced 10% increase in per-pupil spending results in about a 1.1 percentage point and 1.0 point gain in the share of school staff made-up of White teachers at the lower tail of its distribution ([MATH]) in the third and fourth years of LCF, respectively, while no significant effect was found for the higher quantile ([MATH]). A similar pattern was observed for the share of teachers holding a master's degree or above (Panel A). This means that exogenous funding increases lifted the percentage of school-level teaching staff that are White or holding a master's degree, especially for schools that previously employed small percentages of such teachers in the reference year. Since schools with lower shares of White or master's holders tended to serve high-poverty students, as seen in Table 1, we can infer that the infusion of new LCF dollars moved high-poverty schools to attract White teachers and those with master's degrees. We also found that LCF-induced funding gains lowered average class size at elementary and high school levels, consistent with descriptive results (Panels A, B, and C in Figure 9). Elementary schools showed immediate declines in class size after 1 year of LCF exposure, while high schools felt the effect incrementally as the exposure accumulated. Though statistically significant, effect sizes were modest: An LCF-induced 10% increase in funding reduced the elementary homeroom class size by 0.59 in the first year, and high school ELA and math class sizes by 0.27 and 0.25, respectively, after 4 years of exposure. We found little evidence that spending gains altered the count of instructional periods assigned to teachers each day. Panels D, E, and F display how the curricular structure changed among high schools in the wake of LCF-induced changes in spending. An LCF-induced 10% increase in per-pupil spending significantly lowered the percentage of all high school ELA classes that qualified for the college-prep A-G designation by about 0.52 percentage point in the third year of reform. Shares of ELA and math AP classes declined following exogenous spending increases. DiD estimates for each quantile presented in Table 3 suggest that the proportional shrinkage of rigorous college-prep courses (AP and A-G) occurred most severely at the higher quantile ([MATH] = 0.8), and in schools that began with higher shares of college-prep courses in the reference year (low-poverty schools). In schools that previously offered low shares of college-prep ELA and math courses (high-need schools), the infusion of funding resulted in the proportional reduction of the college-prep A-G courses in ELA. This practice may constrain college-going and academic outcomes in the high-need schools. We found little evidence that LCF-induced funding reduced disparities in access to experienced teachers or A-G courses by EL students. We found that ELs' access to experienced teachers worsened for math classes in high-need schools, which showed greater inequities in the reference year ([MATH] = 0.8). We discovered how ELs' access to college-prep ELA courses shrunk proportionally in similar schools. Inequities in the curricular structure and teacher assignments persisted from the pre-LCF period, then widened in high-poverty schools after the reform. This raises concern that within-school sorting allocates experienced teachers away from the students who could benefit most, even when between-school teacher sorting may be mitigated. California's effort to progressively fund districts that serve larger shares of poor children, while decentralizing fiscal control, offers a remarkable policy experiment. Our findings demonstrate how resulting spending gains in targeted districts far exceeded levels predicted in the absence of the LCF reform. The state's distribution of funding among districts changed dramatically, favoring local schools that host greater counts of disadvantaged pupils, relative to districts situated in middle-class and advantaged communities. Still, this sizeable infusion of funding unfolded under a hazy theory-of-action in terms of how teacher quality, related inputs, and school-level practices might change—mediators that operate proximal to student learning. Nor did policymakers anticipate constraints that limit the ability of district or school-level managers to allocate new resources to disadvantaged schools and students, constraints stemming from labor market conditions, institutional habits, or union contracts (Brunner et al., 2020; García & Weiss, 2019; Roza et al., 2020). Four years into California's ambitious funding reform—boosting K–12 yearly spending by $23 billion—little discernible progress in narrowing achievement gaps could be observed. Thus, our analytic strategy aimed to understand how LCF may have failed to alter the mix of teacher inputs and school-level practices in ways that benefit low-achieving pupils. Returning to our research questions, we found that the exogenous portion of the LCF reform did, overall, affect levels of teacher hiring, flows of instructional resources, and the curricular structure of local schools observed statewide. Earlier work in Los Angeles revealed that this was not the case for elementary schools, as the new funding was spread evenly among all campuses, regardless of which children they served (Lee & Fuller, 2020). So it's reassuring that among California's districts statewide, schools hosting larger shares of disadvantaged pupils did experience gains in teacher hiring and instructional resources. Yet this in-flow of additional funding moving through district offices, along with school-level practices, did not necessarily result in distributions to the students generating all the new funding for districts: children from poor or foster care homes, and ELs. The finance reform effectively lowered average class size, especially at the elementary level, for low- and high-poverty schools (Research Questions 1 and 2). But principals in the latter set tended to enter the labor market and hire long-term substitutes and novice teachers, often new to the host district. LCF did deliver new funding to bring on new teachers, but they proved less experienced when hired into high-poverty schools, compared with staffing patterns observed prior to the reform. Similarly, the LCF initiative allowed schools to expand the count of distinct courses, bringing more electives back into the curricular structure, perhaps as No Child mandates dwindled, earlier centered on math and ELA. But this acted to lower the percentage of courses that were college-preparatory in character, either AP classes or those meeting A-G qualifications for university admissions. Some educators may applaud the robust return of electives. But the shift away from college-prep may fail to boost college going or narrow achievement disparities among high school graduates. More research is required to determine whether such changes in curricular structure have lowered the odds that disadvantaged students attend college-prep classes. At the same time, the infusion of LCF funding into high-need schools reduced the representation of ELs in courses taught by experienced teachers, resulting from school-level assignment of novices to these students (Research Question 3). The proportional representation of EL students in A-G courses also declined in high-poverty schools, relative to English-proficient peers, in the wake of new funding. It's difficult to see how these within-school allocations would work to narrow disparities in achievement. These findings also highlight the utility of examining resource distributions among and within schools among student subgroups. Future work should examine whether these trends persist, or whether district and school-level managers shift resource allocations in progressive directions, whereby resources move to intended students. Overall, we discovered how inequities in teacher assignment and curricular structure persisted from the pre-LCF period, then widened in high-poverty schools after the reform. This illuminates how within-school sorting tends to allocate experienced teachers away from the students who could most benefit from them, even when between-school teacher sorting may be mitigated. Such inconsistencies with input allocations and organizational practices inside schools may help explain why California's ambitious reform has yet to narrow achievement gaps. Whether defined by race, home language, or economic status, the disparities in learning apparent prior to LCF have persisted, largely unaltered across 4 years of implementation (Figure 1). Even in high-need schools, White and nonpoor kids experienced modest gains in their test scores, relative to their non-White peers. Our study thus complements results from Johnson and Tanner (2018) and Lafortune et al. (2018), which found that finance reforms do not necessarily yield discernible effects on resource distributions or achievement gaps between high- and low-need students. While their studies documented the effectiveness of finance reforms on reducing between-district disparities, our results suggest that sharper policy tools aimed at changes in the school-level environs of disadvantaged students will be required to narrow disparities. Large infusions of new funding will accomplish equity goals, perennially articulated by policymakers, only if educators progressively allocate inputs, quality teachers, and curricular structuring in ways that equalize the opportunity to learn. The present study does hold limitations. The focus on teacher attributes and organizational features of schools is limited by available data for the 15-year time series. These mediators make intuitive sense and exemplify how finance infusions might alter historical trends in teacher inputs, key facets of school organizations, curricular patterns, and teaching assignments. Still, we have much to learn about the predictive validity of these mediators in shaping student learning over time. Ideally, other mediators that operate between finance infusions and student achievement will become available. We might also examine how new school funding shapes the migration of effective teachers. California's education department maintains course-level data, which we exploited to tease apart how novice teachers are assigned to courses dominated by EL students. Yet much work remains in understanding the racial or class-based tracking of students within schools. The arrival of fresh inputs and new dollars could be deployed to reduce such tracking, again dependent on progressive organizational decisions. Our results suggest that reliance on less experienced teachers, who are then assigned to classes with higher shares of EL pupils, fails to address inequities in their opportunity to learn. It's a ripe example of how gains in dollars or raw inputs are variably mediated by the allocation decisions of school-level actors. Governor Jerry Brown displayed little interest in learning about the effectiveness of his massive finance reform. This is beginning to change as LCF comes under greater scrutiny by Governor Newsom and legislative leaders keen on addressing school disparities. The immediate drive to mitigate worsening achievement gaps, after a year of shuttered schools and remote instruction, complicates the policy conversation. Yet the underlying question remains the same: How can large infusions of new education funding affect the local allocation of resources and organizational practices to progressively lift low-achieving students? Our study illustrates how these key mediators are manipulated by district and school-level managers, not state policymakers. But state policymakers should not be satisfied with pumping additional funding into an institution too often beset by regressive habits. Despite calls for research not simply about whether money matters, but also when or through what mediators, it remains a question that's undertheorized and rarely examined in the wake of finance reform. This limits our understanding of when—through what organizational practices and for which schools—more money will likely matter. Knowledge continues to grow regarding how the organization of schooling affects social cohesion and motivates students and teachers (e.g., Bryk et al., 2010; Fuller, 2022). But little has been learned about how such mediators—be they mixes of teacher inputs or textured organizational practices—are touched by sizeable injections of new funding. When the political stars do align to progressively fund schools, we must rigorously dig into whether these initiatives truly advance fairness and, if so, through what changes in staffing and the social organization of schools. We provide below details for each step in the estimation process, as summarized in the Method section. The first step constructs the counterfactual trends in district per-pupil revenues. Following Johnson and Tanner (2018), we construct two variables that predict counterfactual district per-pupil revenue: (1) expenditures for total state operations, excluding education-related categories such as spending on state universities and colleges ([MATH]), and (2) the total local assistance expenditures outside spending on K–12 schools, community colleges, and the state teacher retirement system ([MATH]). These two variables were deflated by the consumer price index to real 2016 dollars, divided by the state K–12 enrollment each year, and then converted to the natural log scale. The Step 1 prediction model is given by[MATH](1) where [MATH] is the natural log of district per-pupil revenue from the state for district [MATH] for year [MATH]. [MATH] and [MATH] are the natural logs of [MATH] and [MATH], and [MATH] is generated by subtracting the very first year in the dataset, 2003, from year [MATH]. The [MATH], the LCF formula weight parameter for district [MATH], is calculated based on the formula [MATH] where [MATH] is the unduplicated percentage of disadvantaged students: those eligible for free or reduced-price lunch, with limited English proficiency, or in foster care. [MATH] is an error term for district [MATH] and year [MATH]. [MATH] is a district-specific intercept, and [MATH] and [MATH] are district-specific coefficients of the interaction terms between the state-level variables, [MATH] and [MATH], and district indicators. Hence, [MATH] and [MATH] represents the expected percentage change in the district per-pupil revenue for district [MATH] when [MATH] and [MATH] increase by 1%, respectively. These coefficients encapsulate the district-specific sensitivity of revenues to changes in statewide expenditures. The impact of [MATH] on [MATH] is a function of the district-specific formula weight ([MATH]), that is, [MATH], since the formula weight by time trend interaction is included. The parameters of this model were estimated using the pre-LCF data (2003–2012), and then predictions were made for the post-LCF (2013–2017) years. Thus, the predicted log per-pupil revenue [MATH] in the post-LCF years can be viewed as an estimate of the counterfactual per-pupil revenue if LCF had not occurred (Johnson & Tanner, 2018). This reflects the dynamic effect of time-varying economic conditions on district revenues that might confound the relationship between LCF policy treatment and changes in school-level characteristics over time. To isolate exogenous changes in district per-pupil student spending that are unrelated to unobserved determinants of school-organizational features, we first construct two key variables containing the sources of exogeneity at this step: [MATH] and [MATH].[MATH], the simulated IV or dosage for district [MATH], is the LCF-intended amount of the supplemental and concentration grants in 2013, generated from the state funding formula. The CDE publishes the LCF Funding Snapshot every year to show key data elements and to summarize individual LCF target entitlement calculations for all school districts. LCF target entitlement refers to the target levels of LCF–fully funded amount based on the following funding formula for district [MATH]:[MATH](2) [MATH] is the base grant that depends on enrollment and varies only by grade level, and [MATH] is the unduplicated percentage of disadvantaged students as defined above. The supplemental grant is 20% of [MATH] multiplied by the [MATH]. The concentration grant is an additional grant equal to 50% of [MATH] for each district with [MATH] in excess of 55% multiplied by the district's [MATH] points above 55%. LCF funding snapshot data provides [MATH] and [MATH], which enables us to obtain the LCF-intended amount of funding, [MATH]. [MATH] is defined as [MATH]. This is simply [MATH] according to Equation 2 because it takes only the supplemental and concentration grants portion, which is directly relevant to the overall level of district-level disadvantage. We use only the dosage for the first year of the reform (2013–2014), [MATH], to rule out any effects caused by district's incentive to classify more students as disadvantaged to obtain more funding. Next, [MATH] represents the number of school years after the initial year of LCF reform for academic year [MATH]. [MATH] varies from 0 (pre-LCF years from 2003, before 2013–2014) to 4 (post-LCF year 2016–2017) and reflects the exogenous timing of reform event. To estimate a flexible version of the first-stage equation of 2SLS-IV rather than assuming linear slopes of [MATH] and [MATH], we convert each original variable to a series of indicator variables, [MATH] and [MATH], respectively. [MATH] equals 1 if [MATH] equals [MATH], which varies from 0 to 4, and 0 otherwise. [MATH] is an indicator variable that takes a value of 1 if the decile of [MATH] ([MATH]) equals [MATH] and 0 otherwise. Once the two key variables are defined, the first stage model of 2SLS-IV is estimated through the following event-study model (following Johnson & Tanner, 2018; Jackson et al., 2016):[MATH](3) The endogenous treatment variable of interest, [MATH], is the natural log of per-pupil student spending for district [MATH] for year [MATH]. District fixed effects [MATH] and year fixed effects [MATH] are included to account for general underlying differences across districts and years and to exploit only variation within district-by-year cells. [MATH] is the predicted natural log of the counterfactual per-pupil revenue for district [MATH] for year [MATH] estimated from Step 1 to capture time-varying confounders. The coefficients for the two-way interactions of [MATH] and [MATH], [MATH], summarize the LCF-reform-induced exogenous increases in per-pupil student spending in districts with dosage decile [MATH] after [MATH] years from the reform. [MATH] is then the natural log of per-pupil student spending for district [MATH] for year [MATH] instrumented by the two sources of exogeneity, the timing of reform event and funding formula. Once we carve out the part of per-pupil spending variation that is exogenous in the first stage of 2SLS-IV at Step 2, we specify the second-stage outcome model at Step 3 to estimate the effect of the LCF-induced exogenous increases on school outcomes. The structure of the outcome model follows a DiD estimation approach. Instead of having treatment and control groups, the LCF-induced exogenous spending increases serve as "the amount of treatment" or "dosage." We can obtain the DiD estimate by comparing the pre-to-post intervention change in school-level outcomes between high-dosage districts and low-dosage districts. In such DiD estimation, a conventional linear model can be fit to estimate the treatment effect on the district-level mean of the school-level outcome variable. This common approach of estimating the location shift of district-level averages of outcomes, however, may miss heterogeneity of treatment effects among different types of schools within districts. The multilevel IV quantile regression approach (Chetverikov et al., 2016) allows us to estimate such within-district heterogeneity. The goal of this model is to examine whether the LCF-induced district-level spending increases have differential effects at different points (quantiles) of the school-level outcome distribution within the same district, while controlling for unobservable district-level confounders. Within the DiD framework, the conditional quantile, [MATH] at quantile level [MATH] (e.g., [MATH]) of the outcome variable, [MATH] for school [MATH] within district [MATH] in year [MATH] is modeled as[MATH](3)[MATH](4) Here, the cluster is defined as a district-by-year cell. Hence, Equation 3 is referred to as the Level 1 or within-cluster model, and Equation 4 is referred to as the Level 2 or between-cluster model. For a fixed quantile level [MATH], the key term of this model is the varying intercept [MATH], interpretable as the district-by-year-specific conditional quantile of the school-level outcome after adjusting for differences between clusters in the level of the two school-level confounders: total enrollment ([MATH]) and percentage of FRPM students ([MATH]). Each district-by-year cell (cluster) has one value of [MATH]. [MATH] is treated as the outcome variable in the Level 2 between-cluster model to estimate the effect of the key treatment variable [MATH] where [MATH] represents unobserved factors at the district-year level, which can affect the [MATH]th quantile of [MATH]. We are primarily interested in estimating the DiD parameter [MATH]. [MATH] equals 1 if [MATH] is larger than 0, which indicates that the academic year is after the LCF. [MATH] represents the predicted or instrumented natural log of district per-pupil student spending for district [MATH] in year [MATH]. Thus, the coefficient of their interaction term, [MATH], measures how much the change in [MATH] between exposed and unexposed academic years from the same district tends to be larger for those districts that experienced more LCF-induced increases in per-pupil spending across exposed and unexposed years, after controlling for the effect of time-varying confounders [MATH]. A positive value of [MATH], for example, would indicate that the LCF-induced increase in per-pupil spending boosted the lower tails of the within-district distribution of the school-level outcomes after the reform. We then extend this DiD model at Level 2 to the event-study model to account for the LCF's multiyear phase-in timeline to incrementally close the gap between new target level of funding and actual funding over years (following Johnson & Tanner, 2018):[MATH](5) where [MATH]. Equation 5 includes the term [MATH] and the constraint [MATH] so that the parameter [MATH] represents the difference in the effect of [MATH] on [MATH] between reference year 2012–2013 ([MATH], the year prior to June 2013 enactment of LCF or the prereform base year) and [MATH] years after (or before) the reference year after controlling for the effect of the time-varying confounder [MATH]. We present the event-study estimates [MATH] using the event-study plots in Figures 7 to 9. The event-study plots show the varying effects of the LCF-induced spending increases across the post-LCF phase-in period while the [MATH] presents only an average effect over the same period. The event-study plots also allow us to visually assess the credibility of our research design by checking prereform estimates. Null effect estimates of LCF-induced spending increases in prereform years imply that the estimated LCF-induced increases in spending are not relevant to the changes of outcomes before the reform.
10.3102_0002831221989650	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831221989650	The Effects of Peer Parental Education on Student Achievement in Urban China: The Disparities Between Migrants and Locals	 Despite scholarly consensus on the positive influence of peers' parental education on students' academic achievement, less is known about whether marginalized students reap similar benefits as their nonmarginalized counterparts. Using data from the China Educational Panel Survey and a quasi-experimental design, we show that the impact of classmates' parental education on test scores is significantly stronger for local students than for migrant students in urban schools. These differential effects are largely driven by rural-to-urban migrants and not by urban-to-urban migrants. Additionally, we find that rural migrant students benefit less from the positive effects of peer parental education than their local counterparts, especially when their local peers hold higher levels of discriminative attitudes toward rural migrant students in their classes.	 Numerous studies show that attending schools with a high proportion of peers from an advantaged social background is positively associated with academic achievement and educational aspirations, independent of a student's own socioeconomic status (SES; Coleman et al., 1966; Lauen & Gaddis, 2013). As a policy response to the Coleman Report (1966), national efforts in the United States have attempted to reduce school segregation by enrolling students from different socioeconomic backgrounds in the same school. The idea undergirding these efforts is that students from disadvantaged families reap benefits from the socioeconomic composition of the student body (Kahlenberg, 2001; Moody, 2001b; Spees & Lauen, 2019). However, relatively little is known about whether and how the influence of peer SES on a student's academic outcomes differs on the basis of whether the student is socially marginalized in school. There are strong reasons to believe that peer SES composition does not benefit all students equally and that simply being exposed to a school context with a high SES student composition does not always result in academic benefits. In other words, some social conditions or peer cultures in schools may be more conducive to peer SES effects than others. Peer SES composition influences students' academic and behavioral outcomes through peer social interaction and support because sustained social relationships are channels that facilitate information/resource sharing between students (Bourdieu, 1986; Coleman, 1988; Jencks & Mayer, 1990; Putnam, 2000). However, some sociodemographic characteristics might make students more or less sensitive to peer SES effects. For instance, when disadvantaged students experience social exclusion and discrimination, social barriers might dampen the quality of peer relationships and be an obstacle for positive peer interactions. This may cause diminishing returns to exposure to a high concentration of higher-SES peers. In this regard, it is necessary to consider group-level heterogeneity in school or peer contextual effects (Coleman, 1961; Sacerdote, 2011). Children of immigrants or migrants are one of the most socially marginalized groups in many host societies in the world (e.g., Beiser et al., 2002; Tillman et al., 2006). In the international migration literature, previous studies have shown that it often takes time (and, in some cases generations) for the dominant group in the host society to accept the children of immigrants. In order to become integrated into the host society, children of immigrants often need to cross linguistic barriers, learn local social norms, and become familiar with the host society's culture (King & Skeldon, 2010). School peer contexts are crucial for immigrant children's integration in both Western (Crosnoe & Turley, 2011) and East Asian countries (Kang, 2010). When school peers and/or teachers discriminate against or socially reject students due to their status as newcomers and minorities (Kwong, 2011; Raabe, 2018), immigrant children may feel isolated and alienated. Under these circumstances, immigrant students likely have fewer positive social interactions with members of the dominant group compared with nonimmigrant students. Migrant children who migrate internally may encounter experiences of discrimination or exclusion akin to those experienced by international migrants. Despite the fact that internal migrant students encounter fewer cultural, linguistic, and school curriculum changes when they migrate, local people may stigmatize and marginalize migrants in the host societies. Studies in India, China, and Europe show that migrant children are marginalized due to sharp socioeconomic disparities and cultural differences between sending and receiving areas within their own country (King & Skeldon, 2010; Lu & Wang, 2013; Ramesh, 2016). It is possible that social barriers prevent migrant students from reaping the full benefits of exposure to peers with a high-SES composition in the host cities. China offers an interesting case for testing potential heterogeneity in the effects of peer SES on student educational outcomes by migrant status for several reasons. First, China has a large number of internal migrants (National Bureau of Statistics of China, 2018); in 2017, this number was about 291 million. Chinese cities are hot spots for migrants, but the household registration system (hukou) creates structural barriers for migrants. These obstacles prevent many migrant children from enjoying the same educational rights and benefits as their local counterparts. School teachers and peers label migrant children as outsiders. This boundary making creates a form of social segregation between local and migrant students in the public school system. Second, the number of rural-to-urban migrants is much larger than the number of urban-to-urban migrants. Although all migrants without a local hukou suffer from institutional exclusion, discrimination among host members against rural migrants is more prevalent due to long-lasting rural-urban hukou segregation (B. Wang et al., 2010). Therefore, when urban and rural migrant children attend the same urban public school as local peers, rural migrants tend to experience more severe social barriers than their urban counterparts. Therefore, it is reasonable to expect heterogeneity in peer SES effects between local students, rural migrants, and urban migrants. In this study, we examine whether classmates' parental education, an important dimension of peer SES composition, affects students' academic achievement in urban China. We evaluate our argument by applying a quasi-experimental research design to data from the China Education Panel Survey (CEPS), a school-based, nationally representative sample of Chinese middle school students. Key challenges in estimating peer effects include addressing endogenous school choice and nonrandom student sorting across classes. Both introduce systematic bias in the composition of classrooms. To circumvent these issues, we use school-by-grade fixed effects models that exploit a unique feature of the data set with detailed information on whether students are randomly assigned to classes within each grade of a school. Leveraging idiosyncratic variation in student composition across classes within the same school cohort, we assess the effects of class-level peer parental education on student test scores. It should be noted that our school-by-grade fixed effects framework only captures the effects of peers' parental education that operate through mechanisms that vary across classes within school cohorts. Although classmates may be more influential than schoolmates due to more frequent interactions (van Ewijk & Sleegers, 2010), schoolwide parental education may also affect student academic achievement through other channels (Armor et al., 2018). Although it is important to understand whether and how peer effects at different levels operate in parallel, our approach is unable to estimate potentially important school-level peer composition effects because they are absorbed by the fixed effects. In this regard, our analytic strategy intends to obtain credible estimates of the effect of peer parental education at the class level. Drawing on the existing literature on peer effects and internal/international migration, we further investigate whether the effect of peer parental education on test scores differs on the basis of whether a student is a migrant. We distinguish between urban-to-urban and rural-to-urban migrant students to examine whether a differential impact of peer parental education on academic achievement can be better understood alongside social divisions defined by hukou origin and migration status. Finally, we explore the role of peer atmosphere with regard to hukou-based discrimination—that is, perceptions about potential classmate discriminatory behaviors by local students against rural migrant students—in explaining the differential impact of peer parental education on academic achievement for migrant and local students. The socioeconomic composition of the student body is one of the most crucial school-level determinants of educational performance (Coleman et al., 1966). Among various measures of peer SES, peers' parental education is particularly salient for students' academic performance. Parental education has a stronger impact on own child's academic achievement than other SES measures such as income and occupation. Students with highly educated parents display higher levels of motivation, aspirations, and academic ability (Davis-Kean, 2005; Erola et al., 2016). More important, parental education better captures cultural resources that families pass on to children and that can be transmitted between peers (Bourdieu, 1973; Lareau, 2000). For example, the education of peers' parents, rather than the income of peers' parents, spills over and influences the academic achievement of children (Cherng et al., 2013; Fruehwirth & Gagete-Miranda, 2019). Given that friends' cultural resources matter more for students' educational attainment than friends' material resources, peers' parental education may be a proxy of peer cultural environment, which might affect student achievement. Previous work suggests two main channels by which peer parental education may influence student achievement (Coleman, 1988; Frank et al., 2008). First, a concentration of peers with highly educated parents create a proacademic environment. Highly educated parents of peers are often role models that reinforce proschool attitudes and motivate their children to academically invest in themselves (Coleman, 1988). In turn, students with highly educated parents foster learning-oriented school environments that may shape proschool attitudes and encourage academic engagement among other students, thereby leading to higher educational motivation and achievement of all students (Legewie & DiPrete, 2012; Portes & Rumbaut, 2001; Ream & Rumberger, 2008). Second, high peer parental education may influence student academic achievement through the exchange of information and the sharing of resources among students (Cherng et al., 2013; Stepick & Stepick, 2009). Peers with parents who attended college may have rich information on learning opportunities (e.g., after-school or summer programs, learning tools and materials, and/or tutoring and academic training information; Choi et al., 2008; Kim, 2020a). This know-how can academically benefit students who otherwise would not have access to this information (Stanton-Salazar, 2001). Therefore, peers with higher parental education not only possess more educational resources but also facilitate the spread of those resources through individual and/or group interactions (Cherng et al., 2013; Sampson et al., 1997). Most prior scholarship on peer SES effects assumes that the aforementioned two channels (proacademic environment and information/resource sharing) work universally for all students. In other words, all students benefit similarly from exposure to high levels of peer parental education. Yet social interactions between students are likely a prerequisite for the sharing of resources/information among peers and engagement in a proacademic peer culture (Harding, 2009; Kreager, 2004; Sampson et al., 2002). It is thus unrealistic to think that educational benefits reaped from a context with higher peer parental education are identical between students who are socially marginalized and those who are not (Berry et al., 2006; Sacerdote, 2011). Therefore, we argue that the extent to which students benefit from high peer parental education depends on their social embeddedness or the degree to which dominant groups accept them (Coleman et al., 1966; Jencks & Mayer, 1990; Palardy, 2013). In the United States and in European countries, previous studies provide several insights about the possibility that the effects of peer parental education are different between immigrant and nonimmigrant children (Goza & Ryabov, 2009; Heath et al., 2008). Moody (2001a) found that immigrant children tend to be more embedded in their own "ethnic enclave," which creates homophily in friendship networks, leading to a lower level of familiarity with the host society's culture and norms. A lower level of connectedness with mainstream peers may prevent immigrant children from gaining access to resources and information about education from highly educated families, a group usually dominated by nonimmigrants (Brown & Chu, 2012). In addition, parents of immigrant children tend to have a lower density of social networks in schools compared with parents of nonimmigrant children (Raabe, 2018). This may limit immigrant parents' opportunities to access information and educational resources to help their children (e.g., Dufur et al., 2013; McNeal, 1999). When students and parents are socially distant to the dominant group due to their marginalized social origin, parents likely face social obstacles that prevent them from engaging in positive interactions with highly educated parents. For instance, Hastings et al. (2007) show that parents tend to communicate and share information with other parents of a similar social background. Lack of social networks and connections within a school also makes immigrant parents less familiar with the school curriculum and the details of the educational system in the host society. Beyond immigrant children and their parents' own constraints in accessing peer-based resources, the extent to which immigrant children benefit from high peer parental education is also related to the peer culture of the dominant group. A peer culture in which social exclusion and prejudice are rampant may impede the transfer of proacademic norms and/or educational resources/information from peers with higher parental education to immigrant students. In many parts of the world, although public efforts have been made to reduce overt discrimination toward minorities, the host context often has its own subcultures (King & Skeldon, 2010). Variation in subcultures toward immigrants may lead to greater tolerance of diversity in some schools than in others. When the peer climate among local students is more "inclusive," immigrant students might feel more accepted and socially supported. Such atmosphere would facilitate immigrant students' interactions and friendship formation with local peers. With more integrated peer relationships, both immigrants and nonimmigrants would be able to enjoy similar educational resources and information available from peers with highly educated parents. In contrast, a culture of discriminating against minority students among dominant peers would compromise the potential for social relationships and learning experiences across social groups (Spencer, 1999; Steele, 1997). Compared with international immigrants, internal migrant students are expected to face fewer barriers to incorporation in the host society (Skeldon, 2006). For example, they often do not have to overcome racial/ethnic, cultural, and linguistic differences (Tong et al., 2015), which may ease their school integration and social interactions with local peers. School curricula are generally similar within the same country, possibly enabling a smooth transition to a new school. However, despite some degree of familiarity, there still can exist social segregation between migrant and nonmigrant children because the fundamental causes of internal migration and international migration are similar (Kwong, 2011). People often move from a poorer place to a more developed area regardless of whether it is across countries or within a country (Adepoju, 1998). Thus, both internal and international migrant students are often more economically vulnerable than local students (Lindstrom & Lauster, 2001). Moreover, internal migrants and their children may also face discrimination based on their ethnic background or social origin (King & Skeldon, 2010). Due to their disadvantaged social status, internal migrant children could experience a social exclusion akin to the exclusion experienced by international migrant children. For instance, in India, many migrant children are excluded from schools due to the vulnerable economic status of their parents and/or their lower social class in the caste system (Roy et al., 2015). Even when migrant children are allowed to attend a school in their host cities, they tend to be less familiar with the dominant peer culture. In Thailand, although the government provides access to public schools to migrant children, migrant children's academic achievement often lags far behind that of nonmigrants (Arphattananon, 2012). In China, due to social exclusion and discrimination based on residence status, migrant children often have trouble getting along with local peers, and they create migrant enclaves by befriending other migrant peers (H. Wang et al., 2018). Moreover, since migrant parents have fewer connections with local parents and are less familiar with the educational system in the host cities (Liang & Chen, 2007), their children likely benefit less from existing parental networks (Raabe, 2018). Parents may also be less involved in their child's school-based activities and have fewer opportunities to get to know other parents if they are low-skilled workers who tend to have less flexible work schedules and work longer hours (Keung Wong et al., 2007). Therefore, despite differences in social circumstances and integration processes between international and internal migrant children, the latter group also faces substantial barriers that likely limit social interactions and the exchange of resources/information with peers who have highly educated parents. The challenges faced by internal migrant children are particularly notable and serious in China, a context in which there is a powerful institutional barrier—that is, the hukou system. Through a combination of rapid economic development and rural-to-urban migration, China has experienced drastic levels of urbanization in the past few decades. The urban population has increased from about 20% in early 1980s to more than 50% in 2010. Therefore, about 15% of the national population who live in cities do not have an urban hukou (Y. Chen & Feng, 2013). Moreover, as the labor market became increasingly mobile, a growing number of people began to migrate from urban to urban areas (Cheng et al., 2014). As a result of this large-scale migration, as of 2015, there were 34.26 million migrant children in China, and about 14 million of them were in primary or junior secondary schools (National Bureau of Statistics of China et al., 2017). A distinct feature of China's internal migration is that migrants face some of the same constraints as international migrants due to the household registration system (hukou system). These constraints include, but are not limited to, job opportunities, health care access, subsidized housing, and children's schooling (Fang et al., 2016), which curtails migrants' full integration in their destination cities (Yew, 2012). Due to the hukou barriers, migrant students are unable to enjoy the same rights as local students in the public school system in urban China, as the funding for the compulsory education in each city is allocated based on the number of students with a local hukou. As a result, migrant children are often at risk of being excluded from urban public education. A large number of migrant children have to attend low-quality "migrant schools," an informal school system founded by nongovernmental organizations or migrants themselves (Y. Chen & Feng, 2013). These schools are typically characterized by higher teacher-to-pupil ratios, lower access to learning materials, higher teacher turnover, and less-qualified teachers. During the past decade, due to rapid fertility decline, urban public schools have increased their enrollment capacity for migrant students. Based on a national survey of the migrant population in 2012, nearly 80% of migrant students managed to enroll in public schools (Liang et al., 2020). Because local authorities lack incentives and financial resources to accommodate migrant students (Y. Chen & Feng, 2013), migrants who manage to attend public schools are often positively selected in terms of parents' SES, family residential location, and parents' educational expectations in comparison to those who attend migrant schools in the same city. Although migrant students in public schools enjoy better school facilities and educational resources than their peers in migrant schools, they are often socially marginalized in urban public schools due to rampant social segregation caused by hukou discrimination. The lack of social connectedness and limited positive interactions with local peers may undermine the potential benefits of attending public schools for these migrant children (Mao & Zhao, 2012; Zhou & Cheung, 2017). For example, without a local hukou, migrant children are often denied the equal opportunity to continue their high school education on finishing junior secondary school in the host cities. It is because local governments hold tight control over stepping-stone exams such as high school entrance exam, resulting in fierce educational competition (Tong et al., 2020). Nonlocal hukou holders would be excluded to ease the level of competition. For similar reasons, most of city-level education policies do not allow a nonlocal hukou student to take university admission exams in the host cities. Therefore, due to the diverged education tracks, teachers often divert resources toward urban children (Yiu, 2020). This leads to a higher level of segregation in the social circles of students with and without a local hukou in junior secondary schools (Wu, 2013; Wu & Zhang, 2015). Possessing a nonlocal hukou marks both rural and urban migrant children as "outsiders" to local peers. However, rural migrants are likely in a more marginalized situation than urban migrants (Cheng et al., 2014). For decades, the rural-urban divide has compounded the vulnerability of rural-hukou holders in Chinese society (B. Wang et al., 2010). In urban schools, rural migrant students are often stereotyped as wild, rustic, poor, and incompetent individuals by teachers and peers (Afridi et al., 2015). Hence, rural migrant students tend to experience more severe discrimination and isolation from local classmates as well as teachers than urban migrant students. Previous empirical studies show that compared with urban migrant children, rural migrant children are more likely to become targets of discrimination (X. Chen et al., 2009; Zhan et al., 2005). In an unfriendly classroom atmosphere toward rural-hukou children, rural migrant students may be more socially isolated, have fewer chances for positive interactions with peers, and experience more difficulties than their urban-origin counterparts in engaging in network-building activities and creating social support systems in urban schools (Jiang, 2016; Xiong & Yang, 2012). Classroom teachers may also contribute to a discriminating classroom atmosphere by treating rural migrant students differently: assigning them seats with obstructed views, providing fewer comments on their homework, reserving awards for urban students, and so on (Ming, 2003). Such classroom environments may restrict rural migrants' access to school resources and information from peers with highly educated parents that would otherwise be accessible (Fisher et al., 2000). Based on the above arguments, we formulate four hypotheses. Consistent with previous evidence that peer parental education has a positive effect on student academic outcomes in Western countries, we anticipate that peer parental education will have a positive impact on students' academic achievement in urban schools in China. We hypothesize that Hypothesis 1: Higher class-level peer parental education increases student academic achievement. Given the marginalized status of internal migrants in China, we examine whether the effects of peer parental education vary between migrant and local students. Since migrant students tend to be less connected to mainstream peers, we hypothesize that Hypothesis 2: Compared with local students, migrant students benefit significantly less from a high concentration of peers with higher parental education. Despite the fact that all migrant children may suffer from both institutional and social exclusion in their host cities, urban-to-urban migrant students may be relatively less disadvantaged compared with rural-to-urban migrant students. Based on this argument, we hypothesize that Hypothesis 3: The heterogeneous effects of peer parental education on academic achievement for local and migrant students in urban schools are largely driven by rural migrants, rather than by urban migrants. Furthermore, a discriminating peer climate toward rural migrant students would prevent them from benefiting from higher peer parental education. We therefore hypothesize that Hypothesis 4: The interaction effect between peer parental education and migrant status is more pronounced in peer environments with higher levels of hukou-based discrimination. The data we analyze come from the CEPS, a large-scale, nationally representative and longitudinal survey of Chinese adolescents. The survey adopted a stratified, multistage sampling design with probability proportional to size, randomly selecting approximately 20,000 seventh- and ninth-grade students from 438 classes within 112 schools in 28 counties in mainland China during the 2013–2014 academic year. For each selected school, two classes in the seventh and ninth grades were randomly chosen. All students in selected classes were surveyed. Survey questionnaires include information about, and are not limited to, students' educational outcomes, personal and school experiences/activities, and multiple factors related to family, school processes, and communities. Since our analytic strategy exploits variation generated from the random assignment of students to classes within schools and is only plausible for schools with strict random class assignment rules (and strong adherence to these rules), it is important to understand class assignment practices in China's middle schools. The 2006 Compulsory Education Law banned student assignment to classes based on academic performance or hukou status. Two allowed and common methods of class assignment in middle schools in China include (1) purely random assignment and (2) "average" assignment according to students' previous academic performance, such as the diagnostic exams arranged by the middle school when students first enroll (Hu, 2018). In the first case, students might be assigned to classes based on a lottery number or other purely random criteria. In the second case, the most important rule is not to group students with similar grades together. Theoretically, the distribution of students' academic ability should be similar across classes. One unique feature of the CEPS is that it contains information on class assignment rules. In the CEPS, both school administrators and homeroom teachers were asked about whether students were randomly assigned to classrooms or whether students were (re)assigned based on academic performance. Based on school staff self-reports, we limit the sample to public schools whose administrators reported they engage in random or "average" assignment in which schools assign newly enrolled seventh-grade students to classes and do not reassign eighth- or ninth-grade students to classes based on their previous academic performance. We further restrict the analytic sample to classes whose homeroom teachers reported no student tracking. These restrictions result in 6,070 students in 57 schools out of 11,972 students who attend urban schools (located in the center, outskirts, and rural-urban fringe zones of the city/town) and have no missing values on the dependent and control variables. Our outcome variable is the academic performance of students, measured as their standardized test scores. The CEPS collected student's midterm examination scores for the fall semesters in 2013 and 2014 on three compulsory subjects (i.e., Chinese, math, and English). These data are collected directly from school administrators. The scores are standardized in terms of school and grade, with a mean of 70 and a standard deviation of 10. Previous studies have shown that family SES influences math and language learning differently, as math is often seen as an innate ability (Cannon & Ginsburg, 2008). Therefore, we present the scores in two different ways: as an average midterm examination score of three compulsory subjects and as subject-specific test scores. The main independent variable of interest is peer parental education, measured by average years of highest parental education (years of schooling completed) in the same class excluding the parent of the focal student. This measure of peers' parental years of schooling completed is considered a valid measure of school or peer SES and is commonly used in prior research (Bifulco et al., 2011). In this study, we define migrant students as those whose school location differs from their hukou registration. Local students are those whose school location is the same as their hukou registration (Y. Chen & Feng, 2013; Jiang, 2016). The CEPS also provides information on whether students hold a rural or an urban hukou, as well as the specific locations of the hukou. This enables us to identify students' migration status. Based on this, we are able to differentiate between two types of migrant students: urban migrants (migrant students with an urban hukou) and rural migrants (migrant students with a rural hukou). We are interested in whether the extent to which students benefit from peer parental education differs on the basis of being a rural migrant, as opposed to being a local student or an urban migrant, in urban schools. We compare the following three groups of students: local students (78.6%), urban-to-urban migrant students (11.2%), and rural-to-urban migrant students (10.3%). We measure a classroom's discriminating culture by summarizing students' perceptions about local-hukou classmates' discriminatory behavior toward rural migrant students. Students' perceptions are measured by the following survey item: Do you think your classmates from the local county/district would do the following to students from other rural counties/districts? (a) make complaints or offensive remarks about them, (b) play together with them (reverse-coded), (c) discuss questions related to schoolwork (reverse-coded), and (d) make friends with them (reverse-coded). We first create a binary indicator of whether the student answered "yes" to at least one of the above four items and then compute a class-level proportion. A higher proportion indicates that classmates have stronger perceptions of local students' discrimination against rural migrants in the classroom. We split the classes in the sample into two categories. The first includes classes with strong perceptions of discrimination, defined as classes with a proportion of students in the class who report at least one discrimination above the median proportion. The second group includes classes with weak perceptions of discrimination, defined as having a proportion below the median proportion. As a robustness check, we estimate three-way interaction models in which peer parental education is interacted with migrant status and class-level discriminating peer culture. We also explore whether results change when using a different operationalization of the class-level discriminating culture variable—that is, at least one discriminatory behavior vs. the proportion of discriminatory behaviors. In all empirical models, we use the following set of individual-level control variables: gender, age, ethnic background (Han and others), parental education, household financial status, coresidence with parents (whether the student lives with his or her parents), number of siblings, and cognitive ability (measured by a cognitive test score). Parental education is measured by the highest years of schooling completed by either parent in the parental questionnaire. Household financial status is based on student responses to the question about their family's current financial conditions. Table 1 presents descriptive statistics of the main variables for local and migrant students who attend urban schools. All test scores except for Chinese are not statistically different between local and rural migrant students. Compared with their local and urban migrant counterparts, rural migrant students are slightly older and have more siblings. As expected, rural migrant students are overrepresented in families marked by socioeconomic disadvantage. They tend to have parents with lower levels of educational attainment and experience higher levels of financial hardship compared with other students. In order to identify the effect of peer parental education composition on academic outcomes while addressing endogenous school choice and nonrandom student sorting, key challenges in estimating peer effects, we exploit idiosyncratic variation in classmate composition within the same school and grade (Hoxby, 2000). In this quasi-experimental design, families are assumed to select schools based on the average school composition but not on the attributes of their child's classmates. This assumption is consistent with an environment in which parents can observe "more advantaged" and "less advantaged" schools, but attributes of specific classmates are not predictable as classroom assignment is random. Therefore, we argue that the distribution of classmates' parental educational attainment is quasi-random across classes within school and grade. To implement this strategy, we use school-by-grade fixed effects. Without controlling for school-by-grade fixed effects, it is possible and probable that students with highly educated parents and low educated parents attend different schools. This would invalidate our analytic strategy. School-by-grade fixed effects can account for "more advantaged" versus "less advantaged" school environments as well as other confounding factors at the school-cohort level. This research design allows us to rely on across-class (within the same school-cohort) differences in exposure to classmates' parental educational attainment to provide quasi-experimental variation for the analysis. The primary empirical specification of this study is as follows:[MATH](1) where [MATH] is an educational outcome of individual i in class c of grade g in school s. The key independent variable of interest is [MATH], the average years of schooling completed by classmates' parents. The coefficient of interest, [MATH], captures the effect of peer parental education on student's academic achievement. We also control for students' own parental educational attainment, [MATH]. Vector [MATH] contains a host of individual and family characteristics. [MATH] is a set of school-by-grade dummies that control for nonrandom school selection as well as unobserved characteristics shared by all individuals within the same school cohort. It should be noted that school-by-grade fixed effects absorb any school effects (including school SES composition). In other words, within-school-cohort estimates compare students in different classes within the same school-cohort who face a different set of classmates (i.e., different levels of classmates' parental education). Finally, [MATH] is the error term, and robust standard errors are clustered at the school level. This empirical strategy is persuasive only for schools that adhere to the strict random assignment policy and that do not engage in student tracking (i.e., allocating students to classes based on their academic performance). Relying on school administrator and homeroom teacher reports about the school's student assignment policy, our analytic sample only consists of schools that adopt a random assignment policy. However, it is reasonable to suspect that not all schools randomly assign students to classrooms in a strict sense (Ma, 2006). To assess the extent of potential deviation from randomness, similar to a few recent studies that exploited the feature of random classroom assignment in the CEPS (Hu, 2018; H. Wang et al., 2018), we conduct a number of tests to assess the randomness of classroom assignment. We offer strong and consistent evidence that students were randomly assigned to classes within each grade of a school in the CEPS. Due to space constraints, we present these results in Section A in Supplementary Data files in the online version of the journal. Along with a series of robustness tests, we directly test whether the variation we leverage is quasi-exogenous. We conduct "balancing tests" to examine whether the distribution of peer parental education is correlated with predetermined student attributes (e.g., a host of individual-level and family-level characteristics). The left panel of Figure 1 shows that peer parental education is significantly associated with most individual characteristics. This is strong evidence of endogenous school selection. For example, female students and Han Chinese students are more likely to attend classrooms with peers with highly educated parents, whereas students with a rural hukou are more likely to have peers with lower levels of parental education. More important, a variety of family characteristics are highly correlated with peer parental education: students from high-SES families (i.e., higher household financial status and parental educational attainment) are more likely to be enrolled in classrooms with higher peer parental education. Number of siblings is also negatively correlated with class-level peer parental education. Students living with both parents are more likely to attend a class with higher parental education. However, as shown in the right panel of Figure 1, when controlling for school-by-grade fixed effects, the correlations that we present in the left panel disappear. These results provide statistical evidence in support of our argument that students are not sorted purposefully into particular class-level characteristics within schools. Therefore, classmate parental education composition in a given school grade is plausibly exogenous. Based on these results, we have adopted school-by-grade fixed effects in our empirical models throughout the study. Table 2 presents estimates from the regressions of students' average test score on peer parental education and other covariates while controlling for school-by-grade fixed effects. Column 1 shows a bivariate relationship between peer parental education and average test score while controlling for school-by-grade fixed effects. In column 2, a set of individual- and family-level control variables is included. In column 3, to test whether other class-level characteristics spuriously drive the effect of peer parental education, we add an important set of class-level controls such as percent female, percent Han Chinese, percent rural hukou, percent migrant, mean number of siblings, and mean cognitive ability. Columns 4 to 6 present results for subject-specific test scores. In column 1, we find that peer parental education is positively related to average test score. This relationship is net of peer selection due to the inclusion of school fixed effects. When we adjust for a set of individual- and family-level covariates (column 2), the positive relationship between peer parental education and average test score reduces by 55% (2.762 vs. 1.251). However, the effect of peer parental education is robust to these controls. Column 3 shows that the effect of peer parental education on academic achievement is not confounded by other important class-level characteristics. The coefficients of other class-level measures, though large in magnitude, are difficult to interpret because they are imprecisely estimated with large standard errors. Therefore, we use column 2 as our preferred specification throughout the article. When using subject-specific outcome measures (columns 4–6), we find that peer parental education has a positive impact on English and Chinese test scores. It should be noted that the effect of peer parental education on math is as large as the effects on English and Chinese scores (1.251 vs. 1.299 and 1.206), though the nonsignificance of the effect on math may be due to imprecise estimation caused by large standard errors. However, it is also possible that there are potentially differential impacts of peer parental education across subjects. Math is often thought to reflect innate ability, so it may be influenced by peer context to a lesser extent than language subjects such as Chinese and English, which entail more cultural-based learning components (e.g., Huang & Zhu, 2020). To provide an assessment of the magnitude of the estimated effect of peer parental education on test score, shown in column 2 in Table 2, a one (unconditional) standard deviation increase in peer parental education (i.e., about a 2-year increase in classmates' parental educational attainment) is associated with about a 2.6-point increase (2 × 1.251) in a respondents' average test score. This is equivalent to 30% of one standard deviation. However, a 1-year increase in average years of schooling completed by classmates' parents may be a somewhat large increase, given that our empirical models rely on the within-school standard deviation in peer parental education (0.42) (Table A1 in Supplementary Data files in the online version of the journal). The effect of a one within-school standard deviation increase in peer parental education is equivalent to about a 0.5-point increase in average test score (equivalent to 6% of one standard deviation). These are sizable changes, given that a 1-year increase in respondent's own parental educational attainment is associated with about 7% of a standard deviation increase (0.245/3.759) in average test score (Table 2). Furthermore, since our estimates capture the effects of peer parental education that should operate across schools as well, the appropriate benchmark for assessing effect magnitudes may be the (unconditional) variation in peer composition across the sample. Thus far, we have explored the average impact of variation in peer parental education on academic achievement. Next, we examine whether the effect of peer parental education varies between local students and migrant students. In Table 3, we examine whether there are differences in the effect of peer parental education on academic achievement between local and migrant students. Column 1 presents the baseline model that shows a statistically significant effect of peer parental education on average test score (column 2 of Table 2). In column 2, we test whether peer parental education has differential impacts on academic achievement for local and migrant students by interacting migrant status with peer parental education. We find that the interaction term is negative and statistically significant. This suggests that migrant students benefit less from peer parental education compared with local students. In columns 3 and 4, we distinguish between type of migrant by separating out urban and rural migrants. We find that the interaction term of peer parental education and rural migrant status is negative (b = −0.505) and statistically significant (column 4). This result suggests that the interaction effect of peer parental education and migrant status (shown in column 2) is largely driven by rural migrant students. Compared with locals, peer parental education has statistically significantly weaker effects on the academic achievement of rural migrant students. We extend this investigation by addressing whether peer climate related to hukou-based discrimination moderates the interaction effects of peer parental education and migrant status. Compared with the models that estimate the main effect of classmates' parental education and the interaction effect between classmates' parental education and migrant status, the elaborated model is less rigorous from a causal point of view because the exogeneity of class-level discrimination is not clear-cut. Nonetheless, results from this model can identify potential mechanisms that explain how migrant status influences the way peer parental education impacts educational outcomes. In Table 4, we estimate the main (column 3 in Table 3) and interaction models (column 4 in Table 3) separately for classes with stronger and weaker perceptions of local students' discrimination against rural migrant students. As shown in columns 1 and 3 in Table 4, peer parental education has a stronger effect on the academic achievement of students who attend classes with lower levels of discrimination than on the academic achievement of students who attend classes with higher levels of discrimination (1.232 vs. 2.466). More important, in columns 2 and 4, we find that the estimated coefficient of the interaction term between peer parental education and rural migrant status is larger and statistically significant among students enrolled in classes with a higher level of discrimination. Using a statistical test (seemingly unrelated estimation), we find statistically significant differences in the interaction effects between column 2 and column 4 at the 5% level (−0.831 vs. 0.008). To ease interpretation, Figure 2 plots the results in columns 2 and 4 of Table 4. Overall, the slope coefficients are steeper for classes with a lower level of discrimination (left) than classes with a higher level of discrimination (right). This suggests that peer parental education has a stronger impact on academic achievement in less discriminating classes. While the slope coefficients of local and migrant students are nearly parallel in the left panel, the slope coefficient of rural migrant students in the right panel is obviously flatter than those of local and urban migrant students. These results reveal that rural migrant students do not benefit from a concentration of high-SES peers when they are in a peer context that discriminates against them. We conduct a series of robustness tests. First, we fit three-way interaction models to test the extent to which our findings vary across different model specifications (see Table A2 in Supplementary Data files in the online version of the journal). Results from three-way interaction models are qualitatively similar to those in our main models: For all our findings, point estimates are in the same direction as and of similar magnitude to those we report in our main tables. These results are plotted in Figure A1 in Supplementary Data files in the online version of the journal. Results look substantially similar to results plotted in Figure 2. Second, when creating the measure of discriminating peer climate, we use proportion of perceived discriminatory behaviors rather than a binary indicator of whether students report at least one discriminatory behavior. In Tables A2 and A3 in Supplementary Data files in the online version of the journal, we replicate the results of Table 4 (separate models) and Table A2 (three-way interaction). Results are substantially consistent, albeit statistical significance is somewhat weaker with the latter approach. For decades, researchers have examined whether school or peer SES context shapes students' academic achievement. An exploration of the returns to peer parental education is important because it shows how socially linked resources can be transformed into gains in student academic achievement above and beyond their own family settings. Although numerous studies suggest that school and/or peer parental education exerts a positive influence on academic outcomes (Legewie & DiPrete, 2012; Portes & Rumbaut, 2001), relatively less is known about heterogeneity in peer parental education effects. Peer parental education effects may depend on how socially connected and embedded students are in schools. In this article, we extend this line of research by investigating whether peer parental education affects academic achievement differently for local and migrant students in urban China. Consistent with research in Western countries, we find that students' exposure to high levels of peer parental education has a positive effect on test scores. It should be noted that the effect estimated in this study is economically meaningful, as it is equivalent to an increase in average test score associated with approximately a 1-year increase in the years of education completed by the respondent's own parents (own calculation based on Table 2). We also find that the effects of peer parental education on test scores are only statistically significant for Chinese and English, but not math. This may reflect the different nature of the subjects: Language achievement tends to be influenced by sociocultural contexts to a larger extent than achievement in math in China (J. Liu et al., 2020). Our main argument is that the effect of peer parental education on students' academic achievement varies across social groups and along important social boundaries. The marginalized status of migrant students in urban schools may shift the nature of peer interactions and create barriers to migrant students' integration into urban schools. Migrant students may have fewer positive interactions with and receive lower levels of support from school peers compared with nonmigrant peers. As hypothesized, our findings show that the effect of peer parental education on academic achievement is more pronounced among local students than among migrant students. We also find that the differential impact of peer parental education on academic achievement is driven largely by rural migrant students, not urban migrant students. Our findings are consonant with previous studies that report rural migrants, a marginalized social group, are often targets of social exclusion and discrimination in urban schools in China and elsewhere in the world (T. Liu & Laura, 2018; Zhang & Treiman, 2013; Zhou & Cheung, 2017). We provide empirical evidence that the extent to which rural migrant students experience social exclusion or discrimination shapes whether rural migrant students benefit from high levels of peer parental education. We find that a peer climate of social exclusion drives the differential impact of peer parental education on the academic outcomes of local and rural migrant students. When there is a high level of classroom-level discrimination toward rural-origin students, rural migrant students benefit less from being in a classroom with higher levels of peer parental education. We take this as evidence that rural migrant students, isolated from the urban mainstream culture due to the legacy of decades of the rural-urban divide in China, tend to experience ample exclusion and discrimination in urban schools. This reduces their chances of interacting with local peers, lowering their chance to benefit from important resources that might have otherwise been provided by classmates with highly educated parents. Therefore, despite potential benefits of migrating to urban areas (Xu & Xie, 2015), unfriendly peer cultures toward rural-hukou migrant children may set numerous barriers that hinder students from reaping the benefits of high-SES peers. Of course, lower returns to peer-SES among rural migrant students do not necessarily imply that rural migrant students perform the worst academically in urban schools. In our estimation model, migrant students, particularly those with a rural origin, have higher test scores than local students. This is consistent with the "positive selection" argument that rural migrant students are often positively selected on a number of characteristics such as parental expectations and investment, as well as learning ability and motivation (Crosnoe & Turley, 2011). That being said, we expect that rural migrants would have performed even better if they gained benefits from peer parental education commensurate to their local peers. Our findings make several important contributions to the existing literature. First, our study adds nuance to discussions about educational inequality in urban China. Although previous studies have discussed the differences in academic achievement between migrant and local students (e.g., Lu & Zhou, 2013), their focus was on the comparison of migrant students attending public and migrant schools. Our study builds on previous work by examining within-school inequality, and focusing on how migrant students may benefit from high peer parental education in public schools. Our findings about the lower returns to peer parental education among migrant students, especially rural migrant students, broaden our understanding about challenges marginalized students face in urban schools. These barriers appear to have real consequences that prevent rural migrant students from reaping the full benefits of having educationally rich school-based peer resources. In particular, we demonstrate that a class-level discriminating climate reduces the potential benefits migrant students reap from attending urban public schools. However, this does not mean that migrant students should be separated from local students to avoid discrimination among peers. Instead, efforts should be made to change the atmospheres in public schools to reduce the likelihood of any form of discrimination against nonlocal students. Our study suggests that socially marginalized children in China can indeed benefit from peer parental education as much as their local peers only when an accepting classroom atmosphere (free of discrimination) is guaranteed (e.g., Takenoshita et al., 2014). Second, several policy implications emerge from this study's findings. This study calls for formulating policy interventions in urban public schools that promote social incorporation and acceptance of nonlocal students (Buhin & Vera, 2009) and cultivate equity and equality citizenships among both local and nonlocal students (e.g., citizenship education, diversity, and inclusion campaigns; Geboers et al., 2013). This effort should be accompanied by building teachers' capacity to implement and manage such school-based programs (Buhin & Vera, 2009; Kim, 2020b). Policymakers may also consider creating incentives for schools that make strong commitments to practices that foster inclusive school cultures. For instance, school-based extracurricular activities that lead to positive social interactions between migrants and locals can be encouraged and supported. Relatedly, our study adds to the education policy debate on the incorporation of immigrant and migrant children in both Western and Eastern societies. In Western countries such as the United States, there are national efforts to reduce school segregation by bringing students from different socioeconomic backgrounds to the same school settings in order for all students to potentially benefit from high-SES peers (Kahlenberg, 2001; Moody, 2001b). However, in Western settings, these efforts are often related to desegregating schools whose demographic compositions are often drawn along racial/ethnic lines. Our findings, however, show that institutional and cultural barriers are another form of social segregation that can have effects on student outcomes in a subtler way. In the future, policies that aim to desegregate schools may be more effective if they also facilitate positive intergroup interactions beyond physical desegregation efforts. Although the current study focuses on internal migrants in China, results of this study are relevant to debates about education inequality among ethnic minority immigrant children in other East Asian societies. With growing numbers of ethnic minority immigrant students, East Asian societies such as South Korea, Taiwan, and Japan have begun to face the challenge of incorporating ethnic immigrant minority students into their public school systems. In these societies, discrimination toward non-Western ethnic minority students is prevalent, which dampens the academic achievement of minority students (Chin & Yu, 2008; Takenoshita et al., 2014). Debates in South Korea tend to focus on whether it is more beneficial to provide minority students special treatment programs or adopt a more "inclusive" approach in the national education system to avoid the "visibility" of their minority status (Kang, 2010). In contrast, lacking nationwide policy, Japan relies largely on local governments, nonprofit organizations, and schools to support immigrant children's integration in schools, and there is much controversy over the effectiveness of this approach (Takenoshita et al., 2014). Our study suggests that a local inclusive culture embracing group diversity and equality may be effective in reducing social boundaries between native and ethnic immigrant minority students. Third, we make a methodological contribution to the literature on the estimation of peer effects. Our study accounts for unobserved student sorting and school factors when estimating peer effects. Our reliance on random assignment of students and thus idiosyncratic variation in student composition across classes within the same school addresses school sorting issues, a major concern in previous studies. To provide empirical evidence that schools follow this policy in practice, we show that there is no correlation between classrooms assignment within a school and individual- or family-level characteristics. Furthermore, our results do not differ when we include additional controls. This is what would be expected if student assignment to classes is truly random. Our study has limitations. First, due to the lack of adequate data, we were not in a position to directly investigate detailed mechanisms through which rural migrant students benefit less from peers with high parental education due to limited positive interactions and/or social support in a classroom setting. Future research should examine how a lack of school integration or social support from high-SES peers hampers the transfer of proacademic cultural norms and resources available from socially advantaged classmates to hukou rural students. Second, our focus on migrant children in public schools is limited in scope. Since migrant children who attend private migrant schools in urban areas may have different social experiences in school, future researchers may wish to compare how school or peer environments influence academic achievement in varied schooling contexts. Third, our analytic approach cannot entirely remove the possibility that school resources such as teacher quality may vary across classes within a school, though we partially addressed this by empirically showing that teacher characteristics are not correlated with peer parental education, our key independent variable. Despite its limitations, this study contributes new insights about the role of peers' parental education in students' academic achievement in urban China. Results of this study hold implications for societies with deep social inequalities that cut across socioeconomic and/or demographic lines. In particular, this study may inform educational policies that aim to reduce achievement gaps via within-school changes such as creating programs that facilitate the sharing of resources and information between peers. Such efforts are particularly important in China, a country where the early experiences of social integration may not only shape migrant children's academic attainment and opportunities to go to high school and college but also have long-lasting effects on adult SES attainment, social mobility, and quality of life. Future studies should continue this line of research to enhance our theoretical understanding of the effect of peer parental education in non-Western contexts and suggest solutions for policymakers accordingly. Jinho Kim and Yuying Tong contributed equally to the research. Jinho Kim https://orcid.org/0000-0002-4675-4612 Skylar Biyang Sun https://orcid.org/0000-0002-4194-9875
10.3102_0002831221990359	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831221990359	Examining Clinical Teaching Observation Scores as a Measure of Preservice Teacher Quality	 We draw on rich longitudinal data from one of the largest teacher education programs in Texas to examine the properties of rubric-based observational evaluations of preservice teachers (PSTs) during clinical teaching. Using a variance decomposition approach, we find that little of the variation in observation scores is attributable to actual differences between PSTs. Instead, differences in scores largely reflect differences in the rating standards of field supervisors. Men and PSTs of color receive systematically lower scores, as do PSTs in lower-income and rural placement schools. Finally, higher-scoring PSTs are slightly more likely to become employed as K–12 public school teachers and substantially more likely to be hired at the same school as their clinical teaching placement.	 Roughly 80% of new teachers enter the profession through university-based teacher education programs, or TEPs (National Center for Education Statistics, 2018). However, there has been little attention to understanding the variation in preservice teacher (PST) quality, particularly within programs. TEPs need to accurately measure performance to provide PSTs with specific feedback on their practice and identify those who might need additional support prior to entering the classroom. This occurs most authentically during clinical teaching, a formative teacher preparation structure that affords PSTs opportunities to practice pedagogy under the guidance of a mentor teacher and field supervisor (Goldhaber et al., 2020b; Ronfeldt & Reininger, 2012). During this time, observational evaluations provide formative feedback to PSTs about instructional practices to support their development and prepare them for the profession. Much like in-service teacher observations can serve a dual purpose of development and evaluation (Kane et al., 2011; Steinberg & Donaldson, 2016), clinical teaching observations can also provide a signal of PST quality. Both of these purposes—development and signaling—rely on the ability to accurately measure PST performance. Despite the use of observational evaluations in the clinical teaching context (American Association of Colleges for Teacher Education, 2018), there is little systematic evidence about whether these observations accurately distinguish between high and low performers. To our knowledge, only a handful of studies have incorporated PST observation scores, in which these scores were one of a number of other measures that signaled an underlying construct of PST quality (Henry et al., 2013; Vagi et al., 2019a, 2019b). Instead, research on observation scores is almost exclusively confined to the in-service context, focusing on the reliability and validity of these scores as well as their connection to labor market outcomes (e.g., Bacher-Hicks et al., 2019; Cohen & Goldhaber, 2016; Doherty & Jacobs, 2013; Grissom & Bartanen, 2019; Kraft et al., 2020; Whitehurst et al., 2014). To date, there is mixed evidence as to whether these scores accurately measure teacher performance. While some studies find a positive relationship between observation scores and student achievement growth (Garrett & Steinberg, 2015; Grissom & Loeb, 2017; Kane et al., 2011), others have demonstrated that these scores in part reflect factors that teachers cannot control, such as their race, gender, classroom composition, and school context (Campbell & Ronfeldt, 2018; Cohen & Goldhaber, 2016; Steinberg & Garrett, 2016). Thus, "despite the ubiquity of teacher observations, we know relatively little about them" as a performance measure (Cohen & Goldhaber, 2016, p. 383). Our uncertainty is amplified in the preservice context, given the dearth of research examining observational evaluations in this setting. This article helps to fill an important gap by examining observational evaluations during clinical teaching as a measure of PST quality. Broadly speaking, we consider three properties of PST observation scores. First, we examine their reliability using a decomposition approach that separates variance attributable to PSTs from other facets of the observation process. We then turn to issues related to their validity, including correlations with other measures of PST qualifications, PST demographic characteristics, and the contextual characteristics of placement schools. Finally, we examine the potential for observational evaluations to influence the employment outcomes of PSTs. To be specific, we seek to answer the following research questions: Research Question 1: To what extent are observation scores of PSTs during clinical teaching explained by individual-level, placement school-level, and supervisor-level variation? Research Question 2: How are observation scores related to other measures of PST qualifications, PST demographic characteristics, and contextual characteristics of their placement schools? Research Question 3: What is the relationship between observation scores and employment as a K–12 public school teacher in Texas? To answer these questions, we access a unique data set from one of the largest TEPs in Texas, which contains detailed information on all undergraduates who enroll in the TEP. Texas requires that all PSTs in a clinical teaching placement receive observational evaluations conducted by a trained field supervisor. We analyze scores from over 8,200 observations—conducted by approximately 150 supervisors across 600 placement schools—of 2,100 PSTs to investigate the extent to which these scores are an accurate measure of PST quality. While ostensibly for developing PSTs' pedagogical skills, observation scores must also be provided to the principal of the clinical teaching placement school. Thus, schools or districts that serve as clinical teaching placements may draw on these scores as a measure of PST quality when making hiring decisions. PSTs may also consider their scores when deciding whether to enter the K–12 public school teacher workforce. We link these evaluation data to state administrative records, which allows us to observe whether a PST became employed in a Texas K–12 public school (and in which school) as a full-time teacher. Using a generalizability theory framework, our variance decomposition results demonstrate that little of the variation in PST observation ratings is attributable to differences between PSTs. Instead, the majority of the variation is explained by differences between supervisors. We demonstrate that supervisor-level variance likely reflects arbitrary differences in supervisors' rating standards rather than their contributions to PST development. In practical terms, this means that most of the information contained in PST observation scores is unrelated to the performance, readiness, or quality of PSTs. We further illustrate this point by documenting that only weak relationships exist between observation scores and other plausible measures of PST quality, such as undergraduate GPA, SAT scores, and scores on pedagogy and content certification exams. Additionally, we extend findings from prior work examining observation score biases among in-service teachers by documenting that race and gender gaps also exist among PSTs. Men score 0.27 SD below women, on average, while PSTs of color score 0.08 SD below their White peers. These gaps persist even after accounting for a rich set of individual and placement school characteristics. We also find that PSTs in high-poverty placement schools tend to receive lower scores, as do PSTs in rural schools. Finally, we show that PSTs receiving higher observation scores are slightly more likely to become employed as a K–12 public school teacher in Texas, and substantially more likely to be hired in the same school as their clinical teaching placement. By isolating the various components of the observation scores, we demonstrate that the positive relationship between employment and observation scores is likely explained by their use as a signal of PST quality, though we cannot definitively distinguish whether this behavior is driven by PSTs or hiring schools. Teacher education is vital in preparing new teachers for the profession (National Council for Accreditation of Teacher Education, 2010). Important to this function is identification and accurate measurement of teaching effectiveness within preparation programs. As stated by Guarino et al. (2006), The issue of teacher quality is integrally related to the interplay of supply and demand. Because not all teachers are alike, quality is an important variable that can be adjusted by policymakers in their efforts to bring supply in line with demand. (p. 176) We begin by reviewing studies that measure and identify PST quality, as well as those examining observational evaluations, more broadly. We then bridge this area with studies considering the factors that influence PST entry into the profession, which helps to frame the contribution of our study. PST quality has traditionally been measured in teacher education through certification exams (or professional licensure exams), which are required in nearly every U.S. state. These exams provide a baseline standard of knowledge required to enter the profession and can include separate tests for measuring basic skills, content knowledge, and pedagogical knowledge (Goldhaber, Cowan, & Theobald, 2017; National Center for Education Statistics, 2017; Youngs et al., 2003). Certification exams can be seen as a barrier into the profession for individuals who do not exhibit a baseline standard of professional knowledge to become a teacher and could be used to increase the quality of individuals entering the profession (D'Agostino & Powers, 2009; Memory et al., 2003; Shuls, 2018). However, there has been some evidence that these exams disproportionately restrict teachers based on ethnicity (Angrist & Guryan, 2008; Goldhaber, 2007), gender (Goldhaber & Hansen, 2010; Rucinski & Goodman, 2019), and content area (Gitomer, 2007), and any evidence that associates these exams with teacher effectiveness has been marginal at best (Goldhaber, 2007; Goldhaber & Anthony, 2007; Goldhaber, Cowan, & Theobald, 2017; Goldhaber et al., 2013; Goldhaber & Hansen, 2010; Harris & Sass, 2011; Jacob & Walsh, 2011; Wayne & Youngs, 2003). Thus, while our study incorporates certification exams as a proxy for PST quality, our focus is directed toward observation scores, which we discuss next. TEPs consistently use observational evaluations—primarily during clinical teaching—to develop instructional practice (American Association of Colleges for Teacher Education, 2018). Throughout this preparation structure, PSTs participate in professional activities under the guidance of a mentor (or cooperating) teacher for at least one full semester (Anderson & Stillman, 2013; Feiman-Nemser & Buchmann, 1987; National Council for Accreditation of Teacher Education, 2010). During this experience, PSTs are assessed on their pedagogical skills and their professional learning, often by university supervisors, who tend to be part-time faculty providing an external perspective on teaching quality (Clift & Brady, 2005; Ronfeldt et al., 2013; Slick, 1998). Teacher education policies have increasingly reduced the requirements to become a supervisor, even despite evidence that they can positively influence PST development (Darling-Hammond, 2014; Grossman et al., 2008; Wasburn-Moses & Noltemeyer, 2018). According to the National Council for Accreditation of Teacher Education (2010), increased use of clinical teaching observational data is one recommendation for effective teacher preparation to raise PST quality. Unfortunately, research exploring observation ratings and teacher education is extremely limited. In perhaps the most relevant study, Henry et al. (2013) used 5 years of data on elementary teacher education graduates from one large preparation program to examine how various measures collected by the program were correlated with in-service teacher value-added. The authors examined five different measures of PST quality, including clinical teaching ratings and basic skills certification exams, and found that these measures signaled only one underlying construct of PST quality, and were not associated with math and reading value-added scores. However, the study examines a particularly restricted sample of graduates teaching tested subjects in public schools, whereas we incorporate a full sample of PSTs in the TEP. Two additional studies have used in-service observation scores to draw conclusions about the quality of TEPs. Using statewide data from Tennessee, Ronfeldt and Campbell (2016) showed significant variation in the average observation scores of graduates from different teacher preparation programs. Bastian et al. (2018) reached a similar conclusion using data from North Carolina. Otherwise, research on teacher observation ratings is almost exclusively confined to the in-service context. Prior work suggests that teaching observations can provide evidence for teacher-student interactions, the quality of teaching practices across multiple dimensions, and a general marker for teacher effectiveness (Garrett & Steinberg, 2015; Grossman et al., 2013). Despite vast variation in protocols and instruments to conduct classroom observations, they have been shown to, at the very least, distinguish between weak and sufficient teaching (Grossman et al., 2014). Scholars have also argued that observations tend to be more predictive of quality instructional practices that are otherwise not encapsulated in—or correlated with—student achievement and value-added model scores (Bell et al., 2012; Grossman et al., 2013; Kane et al., 2011; Strunk et al., 2014). As a result, observations are one piece of multiple-measure teacher evaluation systems that tend to be well accepted by all stakeholders (Garrett & Steinberg, 2015). Nonetheless, recent work has demonstrated that classroom observations may suffer from race and gender biases (Campbell & Ronfeldt, 2018) as well as biases stemming from the student composition of a teacher's classroom (Garrett & Steinberg, 2015). Our study extends knowledge about teacher evaluation in the context of teacher education, where evidence is noticeably scant. Data are often difficult to systematically collect due to the multiple stakeholders involved (e.g., PSTs, TEPs, school districts), as well as the archaic nature of programs still relying on hand-written evaluations. One particularly important of aspect of PST quality is its relationship with entry into the K–12 teaching profession (Goldhaber et al., 2011; Goldhaber & Ronfeldt, 2020; Guarino et al., 2006). This mechanism affects not only the average quality of the workforce but also the distribution of teacher quality across schools, which in turn reduces or exacerbates student achievement gaps (Boyd et al., 2009). But teacher hiring is a complex process, incorporating separate teacher and district decisions. Research examining district hiring indicates that applicants' academic backgrounds are uncorrelated with hire (Jacob et al., 2018). However, these characteristics of PST quality—along with nontraditional measures of effectiveness such as personality traits, self-efficacy (Rockoff et al., 2011), or teacher perceptions of workplace fit identified through person-organization and person-job fit (Ellis et al., 2017; Player et al., 2017)—can be predictive of teaching success, mobility, and retention (Goldhaber, Grout, & Huntington-Klein, 2017). Yet evidence examining PST (as opposed to district) professional decisions remains limited. While increasingly accessible administrative data sets (particularly at the state level) combined with measures of teacher quality (e.g., value-added, observation scores) has spurred a large number of studies examining the relationship between teacher quality and retention (e.g., Feng & Sass, 2017), our understanding of recruitment or entry remains limited, absent some notable exceptions on which our study builds. Lankford et al. (2002) pioneered this work by constructing a quality measure based on certification exam failure (alongside teaching experience and college competitiveness). They found that high-quality teachers in New York tend to work in more advantaged schools, though they also leave these schools sooner than their peers. Similarly, Krieg et al. (2016) found that more qualified PSTs (based on their certification score and undergraduate GPA) tend to work in more advantaged clinical teaching schools and that clinical teaching school was a strong predictor of first teaching site. Relatedly, Goldhaber et al. (2014) identified that approximately one sixth of PST hires were in the same school as their clinical teaching (internship) placement. While certification exam score was not a significant predictor for entry, it was associated with being hired into the same school. The authors posit that clinical teaching was possibly used by districts or schools as a "screening time" to consider PSTs for future hire. However, each of these studies relies on certification score as the primary measure of PST quality, which mentioned earlier, can be problematic. Rather, Goldhaber, Cowan, and Theobald (2017) is likely the most comparable study, which investigated the predictive validity of edTPA (a high-stakes observational assessment of pedagogy) and found that scores were highly predictive of entry into Washington public schools. Nonetheless, validity issues cloud the use of edTPA, particularly as it relates to consequential decisions for both PSTs and teacher preparation programs (Gitomer et al., 2020), and there are sizeable structural differences between video recordings produced from edTPA and observations. In conclusion, we contribute to and connect literatures examining the measurement of PST quality—through observational evaluations from supervisors—and entry into the professional. In this way, our study addresses Goldhaber's (2019) call for additional research about how experiences in teacher preparation, namely, clinical teaching, connect to PSTs' professional outcomes. The study sample includes roughly 2,100 undergraduate students majoring in Interdisciplinary Studies in a college of education at a Texas public university—one of the largest producers of teachers for the state every year. These PSTs were concurrently enrolled in one of the following teacher certification programs between the fall semester of 2012 through the spring semester of 2018: Early Childhood Through Sixth Grade Core Subjects (50% of PSTs), Middle Grades (4–8) Math/Science (26%), or Middle Grades English Language Arts/Social Studies (24%). Below, we describe in more detail the various data used in our analyses. Our key data source is observational evaluations of PSTs during clinical teaching. In the TEP, all PSTs are required to complete three semesters of field placements, including a clinical teaching requirement during the final semester. During clinical teaching, PSTs work in the classroom of a mentor teacher and are expected to take over much of the classroom responsibilities. PSTs receive four 45-minute formal observations held approximately every 3 weeks. These observations are conducted by field supervisors, who are part-time contractors of the university hired solely for the purposes of this position. While primarily intended for developmental purposes to provide PSTs with intermittent feedback about their teaching practice—PSTs are required to return a brief reflection about their evaluation—observations can also serve as a red flag system to identify and provide support to struggling individuals. Per state law, observation scores are automatically distributed to both the mentor teacher and principal at the clinical teaching placement site. In the semester prior to their clinical teaching, PSTs are required to submit their school district preference to the program. Conditional on a district's willingness to accept student teachers, PSTs are often placed in their preferred district. However, school and mentor teacher placements are determined by the individual districts according to their own placement criteria. Among the roughly 2,100 PSTs in our sample, we observe 129 unique districts, 608 placement schools, and 1,740 mentor teachers. Because clinical teaching placements are spread throughout the entire state, the university hires and assigns supervisors according to geographic regions. In any given year, there are roughly 50 supervisors who conduct observations for 350 PSTs in 170 schools. Within PST, the same supervisor conducts each of the four observations. We observe 144 unique supervisors across the study period, though 48 of these supervisors only rated a single PST. Among the other 96 supervisors, the median was assigned eight to nine PSTs in four different schools. Under state law, supervisors must hold current teacher certification in the same area as the PST's classroom, have 3 years of teaching experience, and complete a state-approved training in teacher observation. Additionally, supervisors may not be employed at the clinical teaching placement school. Traditionally, supervisors are retired educators wanting to remain involved in the profession. The university also requires the completion of an online new supervisor training and twice a semester, supervisors facilitate meetings with the PST and mentor teacher to collectively discuss PST development. Unfortunately, we know little about the demographics or prior experiences of supervisors, except that 77% are women. Over the study period, the observation rubric used by the TEP to evaluate PSTs changed three times. The most recent change coincides with the state's implementation of the Texas Teacher Evaluation and Support System (T-TESS) in the 2016–2017 school year. The current rubric evaluates teachers in pedagogical areas of planning, instruction, and learning environment. While these domains are the same as the state rubric, the individual items are different. For each of the four observations, PSTs receive ratings for each item on a 1 to 4 scale. Supplemental Appendix Tables B1 to B3 (in the online version of the journal) show item-level descriptive statistics for each of the rubrics. Additionally, we conducted an exploratory factor analysis for each rubric and found that items across all domains loaded strongly on a single factor. Thus, despite changes in the rubric over time, scores appeared to be capturing a single latent construct. However, the mean item-level score has decreased somewhat over time and the standard deviation has increased. To obtain a consistent measure for our analysis, we take the average score across all items for each observation instance, which we subsequently standardize within semester and year. For each PST, we can access basic demographic information, including race/ethnicity, gender, and a categorical measure of family income. As shown in Table 1, our PST sample is 84% White and 95% female, both of which are slightly above the national averages (80% and 89%, respectively) for elementary school teachers. We also access admissions data, including a categorical measure of family income, SAT score, and whether the PST is a transfer student or first-generation college student. Overall, our sample is fairly affluent and high-achieving, with a mean SAT score of 1100 (with 14% missing) and 44% of participants from families earning more than $100,000 per year. A relatively high percentage of PSTs (36%) are transfer students, largely driven by individuals completing program prerequisites at a nearby community college. Finally, we can access each PST's cumulative undergraduate GPA, with a mean of 3.5/4.0. Teachers applying for initial licensing in Texas during the study period were required to pass two certification exams as part of the Texas Examinations of Educator Standards. The first exam (henceforth referred to as "content") varies by program. PSTs seeking elementary certification (EC-6) were required to pass a core subjects exam that covered all content areas, whereas middle grades PSTs took an exam that corresponded to their specific content area (math/science or ELA/social studies). The second exam is the Pedagogy and Professional Responsibilities (henceforth referred to as "pedagogy") exam. PSTs were required to complete the content exam prior to clinical teaching—as per Texas law—and the pedagogy exam prior to graduation to receive their teaching certificate. In our sample, 99% and 98% of PSTs passed the pedagogy and content exams, respectively. In addition to whether the PST passed the exam, we also have the scaled scores, which were used to construct a standardized score for pedagogy and content. Standardization is straightforward for pedagogy, since there is a single exam. For content, we standardized within each exam and pooled scores into a single measure. Each year, the TEP provides a list of graduates to the Texas Education Agency, which then provides a matched list with information that includes school placement and job title. We use this matched list to construct a binary indicator of whether a PST enters the K–12 public school system. Additionally, the full list of graduates (beginning in 2012) is matched each year, which allows us to observe delayed entries. As shown in Table 1, 83% of graduates become employed as K–12 public school teachers. Among the remaining 17%, we cannot observe whether they are employed in private schools or jobs outside of education. This placement information also allows us to construct indicators for whether graduates enter the same school or district as their clinical teaching placement, conditional on any employment in a K–12 public school in Texas. These outcomes are far less common, with only 17% (37%) of employed graduates entering the same school (district). Our first research question examines the reliability of PST observation scores. We draw on the generalizability theory (G theory) framework to decompose the variability in average observation scores into various components of the evaluation process (PSTs, placement schools, and supervisors), their interaction, and measurement error. G theory is a well-established framework for examining score reliability (e.g., Brennan, 2001; Shavelson & Webb, 1991) and has been used in prior work to examine the reliability of classroom observation instruments (e.g., Casabianca et al., 2015; Hill, Charalambous, Blazar, et al., 2012; Hill, Charalambous, & Kraft, 2012; Mashburn et al., 2014). G theory can be seen as an extension of classical test theory. Whereas classical test theory views an observed score as decomposed into a true score and a single random error term (encompassing all sources of error), G theory views an observed score as drawn from a universe of admissible observations that consists of one or more facets (i.e., sources of variation). A generalizability study (G study) aims to estimate variance components for these facets along with the object of measurement (PSTs). Our study context consists of PSTs who receive four rubric-based classroom observations, where PSTs are nested within both placement schools and supervisors (i.e., all four of their observations are in the same classroom and performed by the same rater). Supervisors are assigned to PSTs largely according to geographic proximity, but some placement schools have multiple supervisors over the study period due to supervisor turnover across years or multiple PSTs in the same school and year having different supervisors. Based on this structure, we implement a three-facet (supervisor, school, and order) G study design with PSTs (object of measurement) nested within both supervisor and school. We treat supervisors and schools as crossed facets, though we acknowledge that this potentially generalizes from an idiosyncratic set of schools. In essence, our design seeks to isolate the variation in observation scores that can be attributed to actual differences between PSTs, as opposed to variation driven by differences between placement schools and supervisors. We also considered including the mentor teacher as an additional facet, but given limited variation, we present these results as a robustness check in Supplemental Appendix Table A2. These results show little evidence that mentor teachers contributed substantially to variation in PST observation scores. Finally, Supplemental Appendix D shows that our main results hold for a restricted sample that drops PSTs whose placement school had a single supervisor or whose supervisor had a single PST over the study period. We estimate the following model via restricted maximum likelihood:[MATH](1) where [MATH] is a standardized observation score and [MATH], [MATH], [MATH], [MATH] indexes PSTs, placement schools, supervisors, and observations, respectively. The term [MATH] denotes the seven sources of variation in our G study and [MATH] is a fixed slope for observation order (1–4). While G studies often conceptualize changes in average scores between observation occurrences as a random facet (i.e., source of error capturing, e.g., differences in the average difficulty of items for two test forms), we instead adjust for the average improvement trajectory as a fixed characteristic of the observation system. That is, we do not treat the four observation occurrences as sampled from a larger theoretical population of occurrences. We do, however, estimate variance components for the interaction between observation order and supervisors or schools. These terms capture the extent to which variation in observation scores is driven by differential rates of improvement among PSTs with certain supervisors or school placements. Modifying the G study framework to account for trends over time helps to more accurately identify the sources of variation in the scores (also see Casabianca et al., 2015, for a related application). The nested structure of our data where a PST remains in a single placement school and is observed by a single supervisor, while likely representative of many clinical teaching systems in TEPs throughout the country (American Association of Colleges for Teacher Education, 2018), presents several challenges for isolating the PST variance component. First, we cannot estimate variance components for PST × School or PST × Supervisor (nor their three-way interaction). In substantive terms, this means that we cannot shed light on the extent to which variation in observation scores may reflect match effects between PSTs and their placement school (e.g., a PST performs particularly well because their teaching style matches that of their mentor teacher). Similarly, we cannot capture match effects between PSTs and supervisors (e.g., supervisors tend to rate certain PSTs more highly due to their teaching style). An additional concern is the potential for nonrandom sorting of PSTs to their placement schools. As noted in our description of the data, PSTs can indicate their district preference for clinical teaching, though not the school site. Since PSTs are only observed in a single school, between-school differences in true PST quality could be captured by the school variance component, leading to an understatement of the PST variance component. To investigate this concern, we conduct variance decompositions for plausible proxies of PST quality, including undergraduate GPA, SAT score, and certification exam scores. The results, shown in Supplemental Appendix Table A3, contain little to no evidence of sorting based on these observable characteristics, as both the supervisor and school variance components are very small across all of the proxy measures. Of course, this does not completely rule out sorting on unobservables that are uncorrelated with these proxies. Our second research question examines the relationships between observation scores and the characteristics of PSTs and their clinical teaching placement schools. Here, we estimate models of the following form:[MATH](2) [MATH] contains two sets of characteristics. The first are alternative measures of PST qualifications, which include certification exam scores, undergraduate GPA, and SAT composite score. The second are demographic characteristics, including gender, race/ethnicity, family income, and indicators for transfer student and first-generation college student. Placement school characteristics ([MATH]) include school level, locale, and the proportion of students qualifying for free/reduced-price lunch. We also considered models that included school-level averages of student racial composition, but we found a high level of multicollinearity with free/reduced-price lunch status and no evidence of improved model fit. We also include random effects for the key sources of heterogeneity: PSTs, placement schools, and supervisors. A potential concern with this mixed model specification is that it introduces bias through a violation of the assumption that these sources of heterogeneity are uncorrelated with the independent variables. As a check, we run Hausman tests to compare estimates from fixed effects and random effects models at various levels: supervisor, school, and supervisor-by-school. The results, shown in Supplemental Appendix Table A4, show no evidence that the random effects assumption is violated. Additionally, Supplemental Appendix Table A5 shows that our findings are robust to a wide range of fixed effects specifications, including those modeling unit-specific trends. Our final research question examines the association between PST observation scores and two employment outcomes: (1) employed at any school in the K–12 public school system in Texas as a full-time teacher and (2) conditional on employment, whether it is the same school as where the PST completes their clinical teaching. Specifically, we estimate linear probability models of the following form:[MATH](3) where [MATH] is a PST's average score across the four observations. [MATH] and [MATH] are vectors of PST demographic and school characteristics, respectively. In all models, we include cohort-by-program fixed effects to account for any program-specific differences in hiring dynamics as well as year-to-year shocks to the labor market. We also estimate specifications that include fixed effects for clinical teaching placement school ([MATH]). While our models are inherently descriptive, comparing the results with and without school fixed effects helps to elucidate the extent to which the observed patterns are driven by nonrandom sorting of PSTs to clinical teaching placements. For example, one explanation for a positive relationship between PST quality and entry would be that high-quality PSTs seek out clinical teaching placements in schools or districts that have a strong history of hiring student teachers after they graduate. If the pattern persists conditional on school fixed effects, however, we can feel more confident that this sorting behavior is not the major driver of this relationship. To account for potential correlation in district-level hiring practices, we cluster standard errors at the school district level. Table 2 shows the estimated variance components for PST observation scores. In addition to our preferred model, which includes the full sample of roughly 2,100 PSTs across six academic years, we show subsample results for PSTs under each of the three rubrics used for observations across the study period. The general pattern of the variance components is similar across the four columns, and thus we focus our interpretation on the full sample in Column 1. The results in Table 2 indicate that only a small portion of the variation in PST observation score reflects actual differences between PSTs. We find that more than half (55%) of the total variation is explained by differences between supervisors, captured both by the random effects for supervisor (43%) and supervisor-by-order (12%). This latter component, in particular, demonstrates that supervisors differ not only in terms of their average ratings but also in the improvement trajectories of the scores of their PSTs. In comparison, 20% of the total variance in observation scores is explained by differences between PSTs. Placement schools account for little of the total variation (3.4%), and the small magnitudes of the school-by-supervisor and school-by-supervisor-by-order variance components (2.8% and 2.7%) further demonstrate that supervisors' rating standards are very consistent across schools. To further probe the importance of supervisors in explaining variation in PST observation scores, we estimate hierarchical linear models (HLMs) that include random intercepts and slopes for supervisors, schools, and PSTs. We also estimate the covariance between the slope and intercept at each level, which reveals the extent to which PSTs who initially receive higher scores experience lower growth over the course of the semester. We describe the details of these models in Supplemental Appendix C. Figure 1 shows graphically the patterns from the HLMs, with the full results shown in Supplemental Appendix Table C1. Specifically, we obtain the predicted random effects for each level, then construct the estimated slopes and intercepts for each PST by adding their individual predictions to those of their supervisor and school. While the plot includes all of the PSTs, we also differentiate PSTs for three supervisors (A, B, and C) and schools within supervisor A to illustrate the logic of the variance decomposition. First, we can see a strong negative correlation between the intercept and slope. PSTs who initially receive lower scores "improve" at higher rates, which is driven by a large negative correlation between the supervisor intercept and slope (r = −0.84). While this correlation could suggest a ceiling effect whereby PSTs with supervisors who give very high initial ratings cannot demonstrate growth on the rubric, few PSTs receive high scores on the first observation, and we found limited evidence that ceilings effects are the driver of this pattern. Visually, the tight clustering of PSTs by supervisor corresponds to the large variance component for supervisors. The school-level variance is illustrated by clustering among schools that are nested within supervisors. As we show for Supervisor A, school placement helps explain some additional variation within supervisor, as PSTs within the same school tend to be clustered together. Finally, the PST-level variation is represented by the differences among PSTs within the same school. However, the amount of school-level and PST-level variation is dwarfed by the differences among supervisors. In practical terms, how should we interpret these large differences between supervisors? One interpretation is that these results reflect arbitrary differences in supervisors' rating standards. Alternatively, some supervisors may provide more effective coaching or mentoring, which leads to greater improvements in PSTs' scores. The latter of these explanations, however, is inconsistent with the strong negative correlation between supervisor intercept and slope. Specifically, this correlation shows that supervisors essentially trade off between giving higher initial scores versus increased scores with each additional observation. To demonstrate this point, we can simply decompose PST scores from their first observation, which occurs roughly 3 weeks into the semester—presumably before the supervisor can be expected to have a large effect on PST performance. Here, we find that 66% of the total variation is explained by supervisors, which further supports the interpretation that the supervisor variance component is largely unrelated to PSTs' actual performance. Taken together, the results in Table 2 and Figure 1 provide evidence that much of the variation in observational evaluations is unrelated to actual differences between PSTs. Instead, these scores largely capture different components of error—most notably, arbitrary differences in the rating standards of supervisors. This finding helps to motivate analyses that isolate different components of the observation scores. Specifically, we estimate simplified variance decomposition models (including intercepts for PST, placement school, and supervisor) and draw on the predicted random effects (best linear unbiased predictions or "BLUPs") as predictors for our subsequent analyses. Conceptually, these BLUPs represent the "contribution" of an individual supervisor, school, or PST to a PST's observation score—analogous to the interpretation of teacher random effects in a value-added model as a teacher's contribution to student test scores. To illustrate the utility of using the BLUPs as predictors in our remaining research questions, we present a conceptual framework in Figure 2. In Panel A, we consider a simple test of concurrent validity that estimates the correlation between PST observation scores and an alternative measure of PST qualifications. Because a large component of observation scores in part reflect supervisor effects—which we hypothesize are unrelated to PST quality—the observed correlation is attenuated relative to the true correlation between PST quality (more specifically, the component of PST quality captured by observational evaluations) and the alternative measure. By partitioning the variance into supervisor, school, and PST effects, we can effectively obtain a disattenuated correlation, which provides a better indication of the extent to which observation scores and the alternative measure capture a common underlying construct of PST quality. Panel B considers the relationship between PST observation scores and employment in a K–12 teaching position. Here, we hypothesize that the observed correlation may conflate two distinct processes. First, PSTs or potential employers may respond to observation scores as a signal of effectiveness. Second, higher-scoring PSTs may be more or less likely to become employed as a K–12 public school teacher, but the score merely serves as a proxy measure for PST quality, rather than a piece of information used in the employment decision. By modeling employment as a function of supervisor, school, and PST effects—rather than the observation score as a single measure that conflates these effects—we can potentially differentiate between these processes. Specifically, we propose that a relationship between the supervisor BLUP and employment would provide evidence that the observation scores are used in the decisionmaking process, since the supervisor BLUP is unrelated to the true performance of PSTs. If employment outcomes are correlated with PST BLUPs but not supervisor BLUPs, it is more likely that the observation scores are simply a proxy for PST quality and not used in the hiring process. Next, we move to models that model observation scores as a function of the characteristics of PSTs and their clinical teaching placement schools. Table 3 shows results from our mixed models, with the random effects parameters included at the bottom. We also check the robustness of these results to various fixed effects specifications, which are shown in Supplemental Appendix Table A5. Column 1 shows the baseline variance components estimates and we successively add covariates. Column 2 shows that observation scores are only weakly associated with pedagogy certification exam scores and unrelated to content certification exam scores (conditional on the pedagogy score). Both observation and certification scores are standardized, such that the coefficients indicate partial correlations of .075 and .005, respectively. Column 3 examines two measures of general academic ability: undergraduate GPA and SAT composite score. We find a small positive relationship between undergraduate GPA and observation scores; a 0.3-unit increase in GPA (roughly 1 SD, or moving from a 3.0 to 3.3) is associated with a 0.07 SD increase in observation scores, on average. However, we find no relationship between observation scores and SAT scores. Columns 4 and 5 examine PSTs' demographic characteristics, with and without controls for certification scores and academic achievement. Similar to prior work examining in-service teachers (e.g., Campbell & Ronfeldt, 2018), we find in Column 4 that men score lower than women (−0.27 SD) and PSTs of color score lower than white PSTs (−0.08 SD). These gaps are attenuated only slightly when controlling for certification scores and academic achievement. There is also a gap between transfer and nontransfer students (−0.09 SD), but no relationship between observation scores and first-generation status. Finally, we find a U-shaped relationship for family income. PSTs from lower-income families (<$60,000) score 0.13 SD lower than their peers from middle-income families ($60,000–$100,000), as do PSTs from families with annual incomes above $100,000 (−0.06 SD). In general, however, PST characteristics explain little of total variation in observation scores. Column 6 adds characteristics of each PST's clinical teaching placement school. Again, we extend findings from prior work on in-service teachers (Campbell & Ronfeldt, 2018; Steinberg & Garrett, 2016) to the preservice context by showing that PSTs' observation scores appear to measure, in part, the characteristics of schools and the students they serve. Specifically, we find that PSTs in lower-income and rural schools tend to receive lower observation scores, though these associations are small in magnitude. For instance, the predicted difference between a PST in a school with 20% versus 80% FRPL-eligible students is 0.09 SD, or roughly 0.03 points on the 1-to-4 rating scale. One explanation for the weak relationship between PST observation scores and measures of PST qualifications (certification exam scores and academic achievement) is that the observation scores contain substantial noise from the supervisor and school variance components. Next, we reexamine these correlations by replacing the average observation score with the estimated random effects (BLUPs) for supervisors, placement schools, and PSTs. For the sake of parsimony, we use the intercepts-only model (Table 3, Column 1). Models that include BLUPs for intercepts and slopes are extremely similar but less precise, given the very high correlations between the slope and intercept parameters for supervisor and school. The BLUPs are, effectively, predictions (shrunken toward the mean) for the contribution of PST, school, and supervisor to a PST's observation score. We then rescale the BLUPs into PST-, school-, and supervisor-level standard deviations using the model-based estimates of the standard deviation of each random effect. Finally, we regress each of the proxy measures on the BLUPs, which isolates the sources of variation from supervisors, schools, and PSTs. These results are shown in Table 4. For each measure (standardized), we compare the correlations using the average observation score (odd columns) to those using the BLUPs (even columns). Across all four measures, we observe substantially stronger correlations with the PST BLUP as opposed to the average observation score. For instance, the correlation between observation score and pedagogy certification exam score is .078, compared to .186 when isolating the PST BLUP. We find similar results for the context certification exam. For undergraduate GPA, the correlation is strongest with the PST BLUP (.14), but there is also a positive correlation with placement school BLUP (.10) and a small negative correlation with supervisor BLUP (−.05). These latter correlations are consistent with the results in Supplemental Appendix Table A3 showing that a small portion (4.5%) of the total variance in undergraduate GPA is explained by sorting to placement schools. For SAT scores, we find marginal correlations with the PST BLUP (.07) and the supervisor BLUP (−.06). In sum, the results in Table 4 suggest that PST observation scores, certification exam scores, and academic achievement do capture a common underlying construct, which we hypothesize is related to PST quality. When we isolate the portion of observation scores that is attributable to PSTs, these correlations strengthen. That said, even these adjusted correlations are fairly weak—for instance, our estimates imply that only about 3.5% of the variation in observation scores attributable to PSTs can be explained by their score on the pedagogy exam. Our final research question considers a different property of clinical teaching observation scores as a measure of PST quality: their connection to employment outcomes. As noted before, we do not observe job applications and cannot definitively distinguish between demand and supply dynamics (e.g., see Engel et al., 2014; Jacob et al., 2018). That said, we can provide descriptive evidence about the extent to which higher-scoring PSTs are more or less likely to be employed as K–12 public school teachers after graduating. We examine two binary outcomes: employment in any K–12 public school in Texas and, conditional on any employment, working in the same school as where the PST completed their clinical teaching. Figure 3 shows simple visual evidence that higher-scoring PSTs are both more likely to be employed as a K–12 public school teacher in Texas and in the school where they completed their clinical teaching. For both of these outcomes, we find that the relationship is approximately linear. As shown by the solid line, the probability of employment as a K–12 public school teacher is high even among very low scoring PSTs. By contrast, employment in the same school as clinical teaching is fairly uncommon—roughly one in six PSTs in our sample—with higher rates among higher-scoring PSTs. Table 5 shows corresponding estimates from linear probability models. Controlling for program-by-cohort fixed effects in Column 1, we find that a 1 SD increase in average observation score is associated with a 4.4 percentage point (5% of the base rate) increase in the probability of employment in any school. In the parallel specification in Column 5, a 1 SD increase in observation score is associated with a 6.7 percentage point (39% of the base rate) increase in the probability that a PST is employed in the same school as their clinical teaching placement. These results hold when adding controls for PST demographic characteristics (Columns 2 and 6), but we observe some attenuation when controlling for clinical teaching placement school fixed effects (Columns 3 and 7), though these estimates are also less precise. While there is a clear association between observation scores and employment outcomes, it is unclear whether the scores themselves are used as a signal of PST quality in a decision-making process (i.e., the scores send a signal to principals or PSTs themselves), or whether the scores are merely a proxy measure for PST quality in an unrelated process (e.g., the scores are positively correlated with principals' judgments of PST quality based on other application materials). We can test these potential dynamics by replacing average observation score with the variance components (BLUPs) for supervisor, school, and PST. If the scores serve as a signal of PST quality, we would expect the supervisor BLUP to be a salient predictor of employment. If the scores are merely a proxy measure but not used by PSTs or principals in decisions about employment, we would expect the PST BLUP to be the only significant predictor. Columns 4 and 8 show that while the coefficients are positive and significant for PST and supervisor BLUP, the magnitude is largest for supervisor. This suggests that PST observation scores are consequential for PST employment outcomes and we are not simply observing a correlated process. That said, we cannot definitively distinguish between the agency of principals (or other school/district actors involved in hiring decisions) and PSTs here. Either way, the results imply that being assigned a supervisor with stricter rating standards slightly decreases the likelihood that a PST is employed as a K–12 public school teacher in Texas, and substantially decreases the likelihood of being employed in the same school as their clinical teaching placement. Observational evaluations during clinical teaching provide a unique opportunity to measure PST performance in a classroom setting. While largely intended to enhance the development of PSTs through specific feedback and to identify those who require additional support prior to entering the profession, evaluations may also send an important signal to PSTs or their potential employers about PST quality, which can ultimately influence employment decisions. Both of these processes—development and evaluation—are predicated on the notion that observation scores accurately measure PST quality. Our results raise a number of serious concerns on this front. First, we find that little of the variation in PST observation evaluations during clinical teaching is attributable to PST quality. Instead, differences in PSTs' scores largely reflect differences in the rating standards of their field supervisors. Beyond a simple difference in rater severity, we also find that supervisors effectively trade off between giving higher initial scores and higher growth over the semester, meaning that growth over time within PST is also an unreliable measure of PST improvement. Despite the tremendous variability induced by supervisor rating practices, we do find that the observation scores contain some signal about real differences between PSTs. Isolating the PST-level portion of the scores, we find positive correlations with certification exam scores and undergraduate GPA. In general, however, observable PST characteristics explain little of the variation in scores. A second concern is that, consistent with prior findings from the in-service context (Campbell & Ronfeldt, 2018), we find race and gender gaps in PST observation scores. On average, men score 0.26 SD below women, and PSTs of color score 0.08 SD below their White classmates. The practical magnitude of these gaps is modest, translating to 0.10 and 0.03 points, respectively, on the 1 to 4 rating scale. When benchmarked to the amount of variation in observation scores attributable to differences between PSTs, however, these gaps represent 59% and 19% of a standard deviation in the distribution of PST quality, respectively. These gaps largely persist across a wide range of model specifications, including those that adjust for academic achievement, clinical teaching placement school context, and certification exam scores. The final concern—and arguably the most important—is that the inaccurate signal created by the observational evaluation process has implications for PSTs' employment outcomes. We find that lower-scoring PSTs are both less likely to become employed as K–12 public school teachers in Texas and, conditional on being employed, less likely to be hired at the school where they completed their clinical teaching. By distinguishing between components of the observation scores attributable to supervisors versus PSTs, we show that this pattern is likely driven by a response to the score as a signal of PST quality. However, we cannot definitively determine whether this response is on the part of PSTs, hiring schools, or both. While it is presumable that principals consider the scores in choosing to hire PSTs who complete clinical teaching in their school (since the scores are provided to them under state law), PSTs may also be responding to a positive or negative signal about their own effectiveness in making their labor supply decision. These results have distinct implications for TEPs, which occupy a critical role as the primary training ground for new teachers. Effectively fulfilling this role requires accurately measuring the performance of PSTs to provide targeted feedback and to identify those who need additional support. Our analysis suggests that attention to the validity and reliability of observational evaluations—specifically, reducing supervisor heterogeneity in rating standards and addressing race and gender biases—are important to improving the measurement of PST quality. As a related concern, supervisors in our sample did not appear to differentiate among different dimensions of PST practice in their ratings. Solutions to these issues might include more intensive and standardized training for field supervisors (Henry et al., 2013). Even absent changes in rating practices, using multiple raters for evaluations could greatly improve score reliability. It might also provide a form of implicit accountability to encourage consistency between raters. Online professional development or frequent communication with supervisors could be first steps toward changing this structure, echoing best practices of investing in evaluators (Weisberg et al., 2009). Echoing prior work from the in-service context, the existence of race and gender means there may also be unintended consequences of observational evaluation systems (Campbell & Ronfeldt, 2018), particularly when those scores can be used for hiring and retention decisions. Beyond basic fairness, this concern is particularly relevant given calls to increase the diversity of the teacher workforce (e.g., U.S. Department of Education, 2016). While our results most strongly link observation scores to employment in the same school as clinical teaching, negative signals received by PSTs in the form of lower observation scores may in part discourage them from ever entering K–12 schools. Given these implications, we also acknowledge that this study faces several limitations, each of which suggests important avenues for future work. First, our analysis of PST quality is based on a single TEP in Texas. While examining a specific context affords us access to unique data with a rich set of variables collected before, during, and after each PST's time in the program, our results may not generalize to other TEPs in Texas or nationally. Given the limited prior work in this area, it is important for future studies to establish whether these patterns hold in other contexts. That said, we replicate a number of patterns from other contexts, which provides some suggestion that our findings may generalize to other TEPs. An additional limitation is that we do not observe PSTs' performance as full-time teachers. Incorporating evaluation measures used in the in-service context, such as observation scores or value-added, would allow for a more robust analysis of the predictive validity of PST observation scores. We reiterate that the causal mechanisms driving the employment results are unclear. The higher rates of employment among PSTs with higher observation scores could reflect PST preferences, hiring school preferences, or both. As an example, while we posit that principals' preferences to hire effective teachers helps to explain the tendency for higher-scoring PSTs to become full-time teachers in their clinical teaching school, this pattern could also reflect greater willingness among higher-scoring PSTs to accept a job offer in that school. Connecting data from TEPs with application data could help to uncover these mechanisms. Finally, given their importance as a source of variability in observation scores, we are limited in that we have little information about field supervisors, including their demographic information and prior work experiences. These characteristics may be important in explaining the substantial heterogeneity in rating standards. Further, while supervisors were required to complete a state-approved observation training, we do not know when that training was obtained or the extent to which the training varied by geographic context. Relatedly, while not formally involved in the evaluation process for PSTs in our sample, we acknowledge the importance of mentor teachers in shaping PSTs' experiences in their clinical placement. While we provide some suggestive evidence that mentors are not contributing substantially to variation in PST observation scores, we are fundamentally limited in that the vast majority of mentor teachers were observed only once in our study. Further, given evidence that PSTs who have an effective mentor teacher tend to be more effective themselves as full-time teachers (Goldhaber et al., 2020a; Ronfeldt et al., 2018), it is important to understand the extent to which mentor teachers may also influence PSTs' employment decisions. Andrew Kwok https://orcid.org/0000-0002-0401-0420
10.3102_0002831221991138	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831221991138	College Acceleration for All? Mapping Racial Gaps in Advanced Placement and Dual Enrollment Participation	 This article documents the patterns of White-Black and White-Hispanic enrollment gaps in Advanced Placement (AP) and Dual Enrollment (DE) programs across thousands of school districts in the United States by merging several data sources. We show that the vast majority of districts have racial enrollment gaps in both programs, with wider gaps in AP than DE. Results from fractional regression models indicate that geographic variations in these gaps can be explained by both local and state factors. We also find that district-level resources and state policies that provide greater access to AP and DE are also associated with wider racial enrollment gaps, implying that greater resources may engender racial disparity without adequate efforts to provide equitable access and support for minority students.	 Advancement Placement (AP) and Dual Enrollment (DE) are the two most popular programs that allow students to earn college credits while in high school (College Board, 2017). In the 2015–2016 school year, for example, 71% of high schools offered at least one AP course and 69% offered DE opportunities (U.S. Government Accountability Office [GAO], 2018). Both are fast growing. The number of DE participants grew from 680,000 in the 2002–2003 school year to 1.4 million in 2010–2011 (the most recent national count of DE participants), and the number of AP examinees doubled from 1 to 2 million in the same timeframe (College Board, 2017; Supplementary Appendix Figure A1 available in the online version of the journal). The fast growth of AP and DE programs is rooted in the several advantages these college acceleration strategies could potentially offer, including increasing students' competitive edge in the college application process, reducing the cost and time it takes to receive a postsecondary degree, better preparing students for college coursework and therefore easing students' transition from high school to college (e.g., An & Taylor, 2019; Hemelt et al., 2019; Klopfenstein & Thomas, 2009). Despite the myriad benefits AP and DE programs presumably offer, and the fast growth of these programs nationwide, a number of reports identify noticeable racial disparities in students' participation in these programs (e.g., Education Trust, 2013; ExcelinEd, 2018; U.S. GAO, 2018). However, little is known about how racial gaps are distributed geographically and what factors may mitigate or exacerbate these disparities. These racial gaps could vary depending on a number of economic, demographic, and policy variables. Understanding factors that contribute to, or mitigate, racial gaps in students' AP and DE participation could provide insights on policies that can be potentially implemented at scale to reduce these gaps. Using a newly available national census of AP and DE participation among U.S. high school students in the 2015–2016 school year, this study provides a detailed descriptive analysis of the patterns of White-Black and White-Hispanic enrollment gaps in AP and DE programs within thousands of school districts in the United States. We begin by describing the geographic patterns of overall AP and DE participation rates and racial gaps among school districts to provide a rich portrait of how communities across the country have developed these college acceleration opportunities with varying success at providing equitable access. Our results reveal substantial differences in geography between AP and DE participation rates, where AP participation appears to be overrepresented in more coastal and urban areas, whereas DE participation is more concentrated in the middle of the country. Beyond the geographic differences between AP and DE participation overall, both programs have wide variations in racial participation gaps between White students and their Black and Hispanic peers across school districts, where the White-Black AP enrollment gaps are particularly pronounced. Yet, although the majority of districts have racial gaps in both AP and DE participation rates, a nontrivial number of districts are associated with high enrollment rates among minority students as well as low White-minority gaps in AP and DE participation, providing some encouraging signs that the problem can be improved. In light of the substantial variation in overall enrollment rates as well as racial equity gaps in AP and DE participation rates across districts, the rest of the article focuses on understanding the extent to which these variations can be explained by observable local and state factors. Building on the existing studies on AP and DE enrollment as well as the broader literature on racial disparities in educational choices and outcomes, we focus on six sets of factors that theories and existing literature suggest may be correlated with racial disparities in AP and DE participation: (1) student academic preparation prior to high school, (2) family socioeconomic background, (3) racial composition in a district, (4) between-school income segregation and racial segregation, (5) average characteristics of high schools in a district, and (6) state-level AP and DE policies. Using fractional regression models, we find that the six categories of variables are all correlated with racial gaps in AP and DE participation, though to different extent. Among all the factors examined, differences in pre–high school achievement gaps between White and minority students are the strongest predictors of racial gaps in both AP and DE participation in a district. We find that controlling for White-minority achievement gaps almost reduces AP and DE racial participation disparities to zero. Second, even conditional on prior achievement gaps, a set of variables that measure the average characteristics of high schools in a district provides marginal benefits in predicting racial gaps in AP and DE participation. Most strikingly, we find that school resources that are associated with higher overall AP participation, such as the availability of gifted/talented programs in a district, average per-student instructional expenditure, and greater number of AP courses offered, also tend to be associated with wider racial gaps in AP enrollment rates, implying that greater resources may give rise to wider racial gaps without intentional efforts to provide equitable access and necessary structural support for racially minoritized students. The argument that resources and access alone are not sufficient in addressing equity gaps is also supported by our results on state-level predictors, where districts in states with stronger mandates on access to AP and DE programs (e.g., states require high schools/districts to offer AP and/or DE opportunities) are associated with wider White-minority participation gaps. Last, among the other state-level variables examined, districts in states with stronger financial support for DE participation are associated with smaller DE racial participation gaps, highlighting the importance of removing financial barriers to participation in college acceleration programs for students from less affluent backgrounds. During the past six decades, there has been an increasing nationwide support for programs that allow high school students to earn college credit while in high school. The largest of these programs are Advanced Placement (AP) and Dual or Concurrent Enrollment (DE), which together enroll millions of high school students each year (College Board, 2017). AP is offered by the College Board and covers college-level curriculum content. It offers students the potential to earn college credits after students achieve a minimum score on a course-specific exam. Since its inception in 1955, AP has grown substantially: With more than 2.6 million exam takers in the academic year of 2015–2016, AP has become the largest mechanism through which high school students earn college credit in the United States. DE is the second largest, with roughly 1.4 million students participated in DE in 2010–2011. Different from AP programs, which are intended to be taken by high school students and are exclusively taught by high school teachers, DE is a broad category including many types of college course-taking arrangements, and are taught by either college instructors or college-approved high school teachers and through different modalities including at the college, at the high school, and online. Researchers have noted several benefits of college acceleration programs on students' postsecondary outcomes, particularly their potential to improve college attendance among underrepresented students (Berger et al., 2013; Klepfer & Hull, 2012). In addition to the policy and theoretical support for expanding programs that allow high school students to earn college credits, a number of studies have also provided empirical evidence for the benefits of AP and DE on student academic outcomes. Numerous studies of the AP program have compared the academic performance of non-AP and AP students, and generally found that AP students outperform their non-AP peers in a variety of academic achievement measures, such as ACT and SAT scores, college attendance rates, admission to selective colleges, college grade point averages (GPAs), college graduation rates, and time to degree (e.g., Ackerman et al., 2013; Flowers, 2008;; Klopfenstein, 2010). Interestingly, quotes from college administrators suggest that regardless of the score received on an AP exam, college admissions decisions may be favorably affected by a student's AP participation alone (College Board, 2013). In a similar vein, a number of quasi-experimental studies also identified positive impacts of DE participation on a variety of academic outcomes, including high school graduation, college enrollment, college persistence, college GPA, and postsecondary degree completion (e.g., Allen & Dadgar, 2012; An & Taylor, 2019; Hemelt et al., 2019; Karp et al., 2007; Miller et al., 2018; Speroni, 2011). A handful of studies also examined whether the benefits of DE vary for subgroup populations of students (e.g., students from low socioeconomic backgrounds vs. those from more affluent backgrounds) and the results are mixed (e.g., An, 2013; Karp et al., 2007; Miller et al., 2018; Speroni, 2011). Given the likely benefits of AP and DE enrollment on college success for students, racial gaps in participation rates would serve as important indicators of educational inequality. Unfortunately, persistent racial disparities exist in AP enrollment and success rates, where Black students are most underrepresented: According to the 10th Annual AP report by College Board (2013), Black students represent only 9% of AP test takers in 2013 despite making up 15% of the 2013 graduating class. In a similar vein, disparities are also observed in dual-credit participation by race. Using the High School Longitudinal Study of 2009, National Center for Education Statistics (NCES, 2019) reported lower participation rates in DE programs among Hispanic students (30%) and Black students (27%) than White or Asian students (both 38%). These national patterns of racial gaps in DE participation are echoed in studies using data from particular states. For example, based on administrative data from Texas, Miller et al. (2017) found that while DE participation rates generally increased during 2000 to 2015 for all students, there was a persistent racial gap in participation rates, which seem to enlarge over time. While the existing evidence on national and state-level patterns of racial gaps in AP and DE participation provides useful information about overall educational inequality in college acceleration opportunities, these aggregate statistics are less informative about whether these gaps are smaller or larger across smaller geographic units, such as school districts, therefore making it difficult to identify local contexts and factors that produce and sustain these gaps. In this article, we address this knowledge gap by providing detailed descriptive analyses of the patterns of racial gaps in AP and DE participation across thousands of school districts, and by identifying state-and district-level factors that are correlated with these gaps. We draw on a rich body of literature and theories about racial disparities in educational choices and outcomes to provide a framework outlining the complex relationship between both school-related and nonschool factors that may be associated with racial gaps in AP and DE participation. Specifically, we focus on six broad categories of factors: (1) student academic preparation prior to high school, (2) family socioeconomic background, (3) racial composition in a district, (4) between-school segregation, (5) average characteristics of high schools in a district, and (6) state-level AP and DE policies. Below we discuss each category briefly. It is well known that most schools use performance standards to determine eligibility for college acceleration programs. Gaining the opportunity to participate in an AP class, for example, often requires teacher referral and demonstration of academic proficiency in a prerequisite course or (and) by having a minimum GPA. Similarly, most states use specific academic eligibility criteria for DE participation, such as acquiring teacher-written recommendations, having a minimum high school GPA, or passing state-determined postsecondary assessments. Because of these criteria, students with lower academic preparedness and performance levels are less likely to enroll in AP and DE programs. Since underrepresented minorities on average have lower achievement test scores than White students (Fryer & Levitt, 2004; Hemphill et al., 2011; Reardon & Galindo, 2009; Reardon et al., 2015), the substantial and persistent racial achievement gaps are likely to result in racial gaps in participating in AP and DE programs. For example, using statewide data that track cohorts of Florida public high school students, Conger et al. (2009) found that the racial disparities in AP enrollment rates are reversed when they condition on students' pre–high school achievement. In other words, Black and Hispanic students were more likely to enroll in AP coursework than White peers with similar academic achievement. A broad and substantial body of literature has documented the strong association between family socioeconomic background and student educational choices and achievements (e.g., Dahl & Lochner, 2012; Duncan et al., 2011). An important driving force underlying such association is the variation in family's economic resources. For example, Berliner (2009) documented the ways schools, students, and families are affected by dimensions of intense, concentrated, and isolated poverty. Family stress, food insecurity, crime, environmental contaminants, and residential mobility could all weaken parents' ability to help children succeed in school. As a result, racial differences in income distribution imply that racial minority students may have less economic resources at home than their White peers, which could lead to different educational choices and outcomes. Taking AP and DE participation as an example, the costs associated with the programs, such as tuition for DE classes, fees for AP exams, and the costs associated with commuting to local college campuses may impose greater challenges for minority students who are more likely to come from low-income families on average. In addition to economic resources, sociological explanations of the relationship between family socioeconomic background and student educational achievement have also emphasized differences among families in access to cultural capital that enables children to succeed in school (Bourdieu, 1977; Lareau, 2001). Originally proposed by Bourdieu (1977), the cultural capital theory indicates that parents from lower socioeconomic background may encounter greater constraints in the skills, knowledge, and norms that are required to assist their children effectively in making educational choices and succeeding in school (Farkas, 2003; Tramonte & Willms, 2010). In terms of participation in college acceleration programs, for example, parents with less education may have less experiences with these programs in their own education, and may have limited access to adequate information about college preparation and the process for enrolling in AP and DE courses. Indeed, using the HSLS: 09, NCES (2019) reported that students whose parents had higher levels of education more commonly took DE courses in high school; 42% of students whose parents had earned a bachelor's degree or higher took these courses, whereas 26% of students whose parents had below bachelor's degree took advantage of these opportunities. This implies that racial disparities in parental education are likely to contribute to racial gaps in AP and DE participation. In addition to students' family socioeconomic background in shaping students' access to and participation in college-accelerated programs, racial differences in students' schooling experiences and opportunities may also result in racial gaps in AP and DE participation. A large volume of research has stressed how social inequalities are embedded in schooling experiences (e.g., Bourdieu, 1977; Hanushek, 1989). One of the major channels through which schools reproduce social inequality is through unequal school resources and opportunities that are linked to socioeconomic and racial/ethnic composition at a school. For example, using PISA (Programme for International Student Assessment) data from 2006, Montt (2011) highlighted ways schools reproduce inequality and a key finding relevant to our study is that achievement inequality seems to be a function of characteristics of educational systems themselves, such as variations in opportunities to learn and, particularly, the extent of between-school segregation. Along a similar vein, there are between-school differences through which lower-resourced schools are less likely to have access to college acceleration opportunities. Indeed, college acceleration opportunities, such as AP and DE courses, are resource intensive. They require adequate and informed counseling staff, high-quality teachers, and supplementary course materials, all of which increase per-pupil expenditures. As a result, lower-resourced schools, which are attended disproportionately by low-income and minority students, often have fewer AP and DE course offerings. Indeed, using data from the Common Core of Data (CCD) and the Civil Rights Data Collection (CRDC), two recent reports—one by the U.S. GAO and the other by ExcelinEd—identified gaps in access to AP and DE at high-minority and high-poverty high schools. For example, GAO noted that over 80% of low-poverty schools offered at least one AP course, as compared with about 60% of high-poverty schools. In regard to DE courses, 73% of low-poverty schools offered DE coursework, as compared with 54% of high-poverty schools. ExcelinEd also identified racial disparities in access to AP and DE coursework, finding that 38% and 31% of high-minority high schools did not offer AP and DE coursework, respectively, whereas 48% and 33% of low-minority high schools did. These results indicate that between-school income and racial segregation may result in racial disparities in participating in college acceleration programs. Together with between-school segregation, a district's racial composition is part of the local context that shapes the constraints and opportunities available to students. Both cognitive developmental theories and social psychological theories provide frameworks for understanding how school racial composition may influence students' engagement and performance. Drawing on the concept of "disequilibrium" originally proposed by Piaget (1985)—the notion that contradiction and discrepancy spur cognitive growth—the cognitive developmental perspectives posit that critical thinking and cognitive growth is fostered when a student encounters cognitive conflicts or contradictions (Gurin et al., 2002). Accordingly, racial diversity in the student body provides opportunities of exposure to diverse perspectives and discrepancies with past experiences, which may lead to mindful thoughts and cognitive development. Social psychological theories provide another framework for understanding how school racial composition may influence student school engagement. Specifically, this perspective stresses the fundamental human need to feel connected or belong to a community (Baumeister & Leary, 1995). Thus, having more same-raced peers who share similar cultural backgrounds can help an individual to feel more connected to school (e.g., Benner & Crosnoe, 2011; Benner et al., 2008). Integrating these two theoretical perspectives together, it seems that students may potentially benefit from both diversity in overall student composition and decent representation of same-raced peers. A line of research attempts to uncover whether peer racial composition, as opposed to educational resources related to it, affects student achievement causally and the findings are fairly mixed (e.g., Angrist & Lang, 2004; Hanushek et al., 2009; Rivkin, 2000). For example, based on the sophomore cohort of the High School and Beyond Longitudinal Survey, Rivkin (2000) used a value-added approach to measure school quality. The study found that school racial composition per se is not related to Black students' education or labor market outcomes. On the other hand, using a rich panel data of more than 200,000 students enrolled in over 3,000 public elementary students in Texas, Hanushek et al. (2009) exploited racial composition changes as a result of students switching schools and the cohort-to-cohort fluctuations in demographic composition. They found that a higher percentage of Black schoolmates reduces achievement for Black students to a much greater extent than for White students, implying that a greater concentration of Black students in a district may exacerbate racial achievement gaps between Black and White students. Aside from disparities as a result of racial segregation between schools that vary in regard to educational resources, a student's decision to enroll in AP and DE programs may also be influenced by policies and local contexts within a school. For example, using student-level data from Texas Schools for the 1998–1999 academic year, Klopfenstein (2004) found that students across all racial groups are more likely to take an AP course if they attend a school that is smaller, in an urban area, and implements a program that incentivizes teachers to receive additional training and students to take more difficult courses. She argued that these school characteristics may also reduce racial disparity in AP enrollment, as students from lower socioeconomic family backgrounds are likely to disproportionately benefit from these resources. In addition, she also found that the presence of magnet programs at a school is associated with larger gaps between White and Black students in AP participation. She argued that this might be due to within-school ability tracking that operates along lines of race. Indeed, a large volume of research has documented that within-school tracking and other forms of homogeneous ability grouping, such as gifted programs within a school, can exacerbate educational inequity by creating a situation where students are segregated along lines of race and socioeconomic background (Gamoran, 2010; Lucas & Berends, 2002; Oakes et al., 1992). Research consistently indicates that racial minority students and socioeconomically disadvantaged students are less likely to participate in gifted education programs and are more likely to be assigned to lower academic tracks (Kettler & Hurst, 2017; Taliaferro & DeCuir-Gunby, 2008). Students placed into lower tracks are rarely afforded an equal opportunity to catch up due to the curricular differences and tend to show increased achievement gaps over time compared with peers with similar initial achievement but were assigned to a higher track (Gamoran & Mare, 1989; Hoffer, 1992; Schofield, 2010). In addition, being placed into lower tracks may stigmatize students by labeling them as less academically capable (e.g., Carter, 2012; Modica, 2015; Oakes, 2005). This may harm the development of a positive academic identity and lead to lower educational aspirations and motivation, which could in turn result in lower rates of college participation and enrollment in college acceleration programs (Hauser & Anderson, 1991; Smith et al., 2010). Finally, state-level AP and DE policies may either exacerbate or ameliorate racial gaps in AP and DE participation. For example, roughly three-quarters of states include AP and/or DE participation and performance measures in district accountability reports (Education Commission of the States [ECS], 2016). These measures are often required to be broken down by demographic groups. Even though mandates like this do not set thresholds to hold districts accountable for AP and DE participation, they do signal to schools that participation and inclusion is important. In addition, considering the additional costs associated with AP programs (such as AP test fees) and DE programs (such as tuition and the cost of books), financial support for students and institutions can also influence the level of participation in these programs, particularly among students from less affluent backgrounds (Dounay, 2007; Klopfenstein & Thomas, 2010). By 2016, 29 states had offered fee reductions or waivers to low-income students taking the AP exam, and 19 states either subsidized tuition costs or fully covered them to encourage DE participation. We linked multiple publicly available data sources to document geographic patterns of AP and DE racial/ethnic enrollment gaps among school districts in the United States. We describe each data source below: CRDC is a biennial survey of all public schools and school districts. The data collection on the 2015–2016 school year targeted 17,370 districts and 96,440 schools with 99.8% of districts certifying their submitted data. The CRDC has collected information on AP course taking and school characteristics previously, and the 2015–2016 survey included questions about DE program participation for the first time. American Community Survey (ACS) is an annual, nationwide survey that includes demographic, social, economic, and housing characteristics for school-age children. All iterations contain data for nation, states, and school districts. The data most relevant for our study come from the Education Demographic and Geographic Estimates (EDGE), which includes tabulations of demographic and socioeconomic characteristics of families who live in each school district in the United States and who have children enrolled in public school. Common Core Data (CCD) is an annual survey of all public elementary and secondary schools in the United States. The data include basic descriptive information on schools and school districts, including enrollment counts for each grade at each school. Stanford Education Data Archive (SEDA) is a publicly available data set about American schools, communities, and student success. The data set includes a range of detailed data on educational conditions, contexts, and outcomes in school districts and counties across the United States. We specifically used district-level measures of academic achievement, in addition to racial and socioeconomic composition in our analysis. Integrated Postsecondary Education Data System (IPEDS) is a system of interrelated surveys conducted annually by the U.S. Department of Education's NCES (2019). IPEDS gathers information from every college, university, and technical and vocational institution that participates in the federal student financial aid programs. We specifically used latitude and longitude data from IPEDS for each college to calculate the nearest college to each high school. The 2015–2016 CRDC data provide new insight into the number of students participating in AP and DE. However, a major limitation of this dataset is that it only provides enrollments at the school level (instead of at the school-by-grade level). As a result, for high schools that also offer eighth grade and below, using total school enrollment as the denominator to calculate AP and DE participation rates is inappropriate, since CRDC specifically instructed districts and schools to only report AP and DE participants among students who are in Grades 9 through 12. In Supplementary Appendix B, we describe the selection criteria we used to identify eligible high schools to be included in our analytic sample, as well as our methodology for estimating the ninth- to 12th-grade enrollment counts for high schools that offer eighth grade or below. Following the framework outlined in the section "Correlates of Racial Gaps in AP and DE Participation," we estimate sources of variation in AP and DE racial participation gaps that fall within one of the following six broad categories: (1) student academic preparation prior to high school, (2) family socioeconomic background, (3) racial composition in a district, (4) between-school segregation, (5) average characteristics of high schools in a district, and (6) state-level AP and DE policies. Below, we briefly describe the variables included in each category. The full list of explanatory variables used and the data source for them are presented in Supplementary Appendix Table A1 (available online). To capture academic preparation prior to high school, we include in our analysis a variable that measures the average pre–high school achievement in a district. This variable is available in the SEDA data and is based on standardized test scores taken by over 200 million students in Grades 3 through 8 for subjects English language arts and math. White-minority achievement gaps are calculated as the standardized difference in achievement between White and minority students. We construct two variables to measure the average socioeconomic background of a district. The first measure is proportions of students eligible for free or reduced-price lunch, which was retrieved from CCD and indicates the percent of students in each school district eligible to receive a free or reduced-price lunch. The second measure is proportions of adults with a bachelor's degree or higher in a district. This variable is available in the SEDA data set and was constructed using ACS-EDGE data for families with school-age children enrolled in public schools. White-minority gaps in both measures are also calculated and included in models that predict racial gaps in AP and DE participation. We also construct three sets of variables to measure the local school context, including: (1) measures of racial and income segregation across schools within a school district constructed using the Thiel index, where higher values indicate greater levels of segregation; (2) indicators of racial/ethnic composition of a school district, measured as the proportion of Black or Hispanic students within a district; and (3) measures of average characteristics of high schools in a district, including proportions of students in a school district enrolled in urban schools, proportions enrolled in magnet schools, proportions enrolled in gifted and talented programs, average per-pupil instructional expenditures among high school students, average student-teacher ratios, and average student-counselor ratios. Given that DE opportunities rely on partnerships with local colleges, we also calculate the distance in kilometers to the closest 2– or 4–year public institution that offers dual enrollment or concurrent enrollment for each high school in our analytic sample and then take the average across districts. Similarly, we also calculate the average number of AP courses offered at school to capture a district's access to AP opportunities. Last, we include state-level variables to reflect policies that either directly or indirectly influence AP and DE participation. For AP enrollment, we identify nine relevant policies, grouped into three broad categories: (1) accountability and mandates surrounding access, (2) financial incentives and program support, and (3) accountability and mandates related to student outcomes. For DE enrollment, we identify 12 policies and group them into the same three broad categories listed above. Each category includes three values that indicate whether a state has strong, moderate, or weak AP (DE) policies for that category. A state is identified as having "strong" policies if it has at least half of the policies in place within a category, "moderate" if it has more than one but fewer than half of the policies in place, and "weak" if it has none or only one policy in place. Details about the number and description of distinct policies included in each broad category are provided in Supplementary Appendix Table A2 (available online). The outcome measures in our study, such as average AP participation rates in a district, follow a fractional response nature, which typically arises from averaged binary outcomes. Specifically, students either participated or did not participate in AP or DE programs. These binary responses were used to generate participation rates at the district level (see section "Constructing Measures for AP and DE Participation" and Supplementary Appendix B, for more detail). The AP and DE participation rate is therefore a continuous variable that is bounded between 0 and 1. Standard linear models are not appropriate in modeling fractional responses, as they can generate predictions that are greater than one and smaller than zero (Papke & Wooldridge, 2008). Building on the existing literature (Wooldridge, 2011), we instead use fractional logit regression with a logistic link that captures nonlinear relationships, especially when the value of the outcome measure is close to 0 or 1 to examine associations between district-level and state-level predictors and district participation rates. It is important to note that our second set of outcome measures—racial gaps in participation rates—have a wider range than a normal fraction (i.e., they range from −1 to 1). We therefore perform a linear transformation to bound the racial gap variables between 0 and 1. Specifically, let W be the original outcome measure of racial gaps in AP or DE participation rates that is in [−1, 1]. Then, define Y = (W+ 1)/2, where Y would be in [0, 1]. This transformation allows us to use fractional logit regression to model the relationship between the transformed outcome measure Y and district-level predictors, which writes as follows:[MATH] where[MATH] which could be further written as[MATH](1) or,[MATH] In other words, the relationship between the racial gaps in DE participation Wi and district-level predictors Xi would be equal to two times b1 from Equation 1. To make the coefficients easier to interpret, we report the average marginal effect. As a result, the coefficient indicates the average changes in the predicted probabilities given a one unit change in an independent variable (in the case of a continuous independent variable), or as the independent variable changes from 0 to 1 (in the case of a binary independent variable). The top panel in Table 1 shows summary statistics for AP and DE participation among all the school districts in our sample. On average, districts had 11% participation rates in both AP and DE, with standard deviations of 12% and 14%, respectively, indicating that there are substantial variations across districts. Figure 1 further uses boxplot to show the distribution of district AP and DE participation rates visually by state (left and right panels, respectively), where states are ranked in descending order by the states' median district participation rate. Overall, there is substantial variation in district AP and DE participation both within and across states. Interestingly, it seems that the states with the highest AP participation rates concentrate on the coastal areas (e.g., Maryland, California, Florida, etc.) while the states with the highest DE participation rates are in the middle of the country (e.g., Iowa, Wyoming, Kansas, etc.). To provide a more detailed overview of the distribution of AP and DE participation rates across all the districts in our sample, Figure 2 presents choropleth maps of district AP and DE participation rates. These maps show districts geographically in progressively darker shades of gray, based on quintiles of district AP and DE participation rates. Since we use the same coloring scheme for both the AP and DE choropleth maps, it allows us to compare participation rates between the two maps to gauge places of higher and lower AP or DE participation as well as regions that primarily offer AP or DE programs by comparing the color of a specific district or region between the AP and DE maps. For example, consistent with the patterns shown in Figure 1, districts in the middle of the country have higher participation rates in DE compared with AP, whereas districts in the coastal areas tend to have higher participation rates in AP. The middle and bottom panels in Table 1 show summary statistics for racial gaps in AP and DE participation among school districts that met our inclusion criteria. There are larger racial gaps in AP participation (9.8 and 6.9 percentage points for the White-Black and White-Hispanic gaps, respectively) than in DE participation (4.7 and 4.2 percentage point gaps for the White-Black and White-Hispanic gaps, respectively). The standard deviation for these gaps ranges between 7.2 to 9.1 percentage points, suggesting that there is large variation across districts in the size of racial gaps. Figure 3 illustrates the distribution of district racial gaps in AP and DE participation within each state. Separately for White-Black and White-Hispanic gaps for AP and DE participation, states are ranked in descending order by the states' median district gap. The figure reveals three main patterns. First, the 25th percentile of racial gaps are higher than zero in most of the states, suggesting that nationwide, the majority of districts have racial equity gaps in both AP and DE participation. This is more clearly shown in Figure 4 which presents choropleth maps of district White-Black and White-Hispanic gaps in AP and DE participation rates. Second, there is substantial variation between districts within a state. At one extreme, some school districts have White-Black and White-Hispanic gaps that are greater than 50%. At the other end, however, some school districts actually have larger minority student enrollment in AP and DE programs than White enrollment. Finally, it seems that a number of states that have the highest AP and DE participation overall, as shown in Figure 1, also have relatively higher racial gaps in AP and DE enrollment. For example, among the top 10 states in overall AP enrollment, seven of them are among the top one third of states with the largest White-Black AP enrollment gaps. In light of the results presented above, we then examine whether there are districts where minority students have both high AP and DE participation rates and where White-minority gaps are small. In identifying these "star districts," we first narrow down to districts where the AP (DE) participation rates among the specified minority group reach above the national median. We then further zoom in on districts with a White-minority gap below 1 percentage point in AP (DE) enrollment rates to identify districts where minority students do well in both absolute levels and relative to their White peers. Among the thousands of districts examined, we identify 318 star districts in term of AP enrollment among Black students (i.e., districts that have above-median AP enrollment among Black students and also below 1 percentage point White-Black AP participation gap), 649 districts in AP enrollment among Hispanic students, 595 districts in DE enrollment among Black students, and 968 in DE enrollment among Hispanic students. The overlap among the four categories of districts is fairly small, where only 15 districts are "stars" in all four categories. To better understand the characteristics of districts with high minority enrollment rates and smaller racial gaps, Table 2 provides the summary statistics for these "star districts" using both the district- and state-level variables grouped into six broad categories. To ease interpretation, we standardize all the continuous variables (such as number of AP courses offered). Thus, a positive value for a continuous variable indicates that the districts have an average value that is above the national average. In a similar vein, for the binary variables (such as whether the district is in a state with strong financial incentives for AP/DE participation), we report the difference between the average value and national average. Although the summary statistics vary widely across the four groups of "star districts," shared patterns still emerge for a number of district-level and state-level variables. Overall, it seems that these districts tend to have substantially smaller White-minority gaps in both pre–high school achievement and income (as measured by eligibility to free or reduced-price lunch at school). They also tend to be more racially diverse than the national average, more likely to be located in urban areas, closer to local postsecondary institutions, and have a greater number of AP courses offered. Interestingly, these districts do not seem to be necessarily better resourced in general than the national average. For example, districts in all four categories have a student-teacher ratio that is above the national average, and a per-pupil instructional expenditures below the national average. Finally, these districts seem to concentrate in states with stronger accountability regarding access and financial support for AP and DE participation. Taken together, these raw descriptive patterns seem to suggest that star districts tend to have lower racial baseline gaps, greater access to college acceleration opportunities, and have stronger financial support. In the next section, we use regression adjusted models to identify the relationship between specific variables and AP (DE) gaps holding other factors constant. We begin by estimating the correlations between these factors and overall AP and DE participation in a district using fractional logit regression models explained in the section "Empirical Model for Exploring Correlates of Racial Gaps." Since all the continuous variables are standardized, the coefficient of a continuous variable indicates the changes in AP or DE participation rates given a 1–standard deviation change in that variable. The results are presented in Table 3 and reveal three general patterns. First, a handful of local-level factors are correlated with greater levels of participation in both AP and DE programs. Specifically, districts that have greater levels of between-school income segregation are related to lower levels of participation in AP and DE programs. In contrast, per-pupil instructional expenditures and academic acceleration opportunities before high school, such as the proportion of students enrolled in gifted and talented programs, are associated with higher levels of AP and DE participation. Second, there are a number of cases where the local-level variables are associated with both AP and DE enrollment, but in opposite directions. For example, the proportion of educated adults with school children in a district is associated with an increase in AP participation rates but a decrease in DE participation rates. Similarly, the average number of AP courses offered at a school is associated with higher participation rates in AP programs but are negatively associated with DE enrollment. These patterns suggest that AP and DE programs may serve as substitutes to each other when schools are allocating resources among different college acceleration programs. These patterns also suggest that students with more educated parents may interpret the value these programs have to offer differently. Last, among the state-level variables, having strong accountability mandates seems to be an important predictor for both AP and DE participation— districts in states with strong accountability and mandates for access to AP (DE) programs are associated with higher AP (DE) participation rates than states without or with weak accountability. Additionally, strong financial incentives are important for AP participation. Specifically, states that offer moderate to strong financial incentives for participating in AP programs, such as reducing or waiving exam fees for low-income students, have AP enrollment rates that are five percentage points larger than states without or with weak financial incentives. Table 4 presents results that further use the local- and state-level variables to predict racial gaps in AP and DE participation. In addition to all the variables included in Table 3 that examines general AP and DE participation, we further add four additional variables into the model to better capture racial gaps in pre–high school academic achievement and in their socioeconomic status. Columns 1 and 2 present the results for White-Black enrollment gaps and columns 3 and 4 present the results for White-Hispanic gaps. Following our conceptual framework presented in the section "Correlates of Racial Gaps in AP and DE Participation," below, we discuss the key findings for each of the six categories of variables examined. Among all the district-level and state-level variables examined, the White-minority gap in pre–high school academic achievement, averaged across Grades 3 to 8, is the strongest predictor of the participation gaps for both Black and Hispanic students and across both AP and DE programs. The size of the coefficient is particularly large for the White-Black gap in AP enrollment. Specifically, a 1–standard deviation decrease in the White–Black pre–high school achievement gap would be associated with smaller gaps between White and Black students in AP participation by almost four percentage points. The summary statistics of the variable indicates that Black students are more than 2 standard deviations lower than White students on average. This implies that adjusting for pre–high school differences in achievement between White and Black students would almost explain away the White–Black AP enrollment gaps completely. That is, districts where Black and White students have similar levels of pre–high school achievement would be equally likely to enroll in the AP program holding other factors constant. Even conditional on student achievement level, average family socioeconomic backgrounds are still significantly associated with racial gaps in AP and DE participation. Specifically, districts with higher average poverty levels— measured as proportions of students receiving a free or reduced-price lunch—are generally associated with wider racial gaps in AP and DE participation. Such relationships are more robust for AP than DE participation gaps. Unsurprisingly, the White-minority gap in eligibility to free or reduced-price lunch is also associated with wider gaps in AP enrollment. In addition, a larger White-minority gap with regard to the proportion of adults in a school district with a bachelor's degree is also associated with larger racial participation gaps for both AP and DE programs. These results imply that racial gaps in college acceleration programs may be partly driven by disparities in family socioeconomic background. We do not observe a clear pattern between racial participation gaps and between-school segregation in income (as measured by proportions of students eligible for free or reduced-price lunch), which is only weakly correlated with the White-Hispanic AP enrollment gap. While districts with greater degrees of between-school racial segregation are associated with smaller racial gaps in DE participation for both Black and Hispanic students, the coefficients are both small in size. These results suggest that income and racial segregation are not necessarily directly linked to racial participation gaps in AP (DE) programs. Rather, a major source of such disparity is likely to be unequal resources and opportunities that are linked to socioeconomic and racial/ethnic composition at a school. As a result, the correlation between segregation and racial AP (DE) participation gaps would be substantially reduced once differential access to resources and peer compositions are adjusted. Indeed, the raw correlation between racial gaps in AP (DE) participation and between-school racial segregation is much stronger and statistically significant (e.g., the raw correlation coefficient between White-Black segregation and White-Black AP enrollment gap is 0.315); however, the association reduces sharply to zero as we include other measures of racial disparities in access to resources, such as the White-Black gap in eligibility to free or reduced-price lunch in a district. Greater proportions of Black students in a district are consistently associated with wider racial gaps in AP and DE participation, though the effect sizes are fairly small. Using White-Black AP and DE participation gaps as an example, a 19–percentage point increase in the proportion of Black students at the district-level (which represents 1–standard deviation increase) is associated with a 1.1 percentage point increase in the White-Black AP and DE participation gap (columns 1 and 2). The same pattern also holds for White-Hispanic gaps in AP and DE participation (columns 3 and 4), although with smaller sizes of effect. In contrast, proportions of Hispanic students are not associated with racial gaps in either AP or DE participation. The most striking finding from this category of predictors is that a set of factors that are correlated with greater AP participation overall in Table 3—namely, proportions of students enrolled in urban schools, proportions of students enrolled in a gifted and talented program, average per-pupil instructional expenditures among high school students, and offering more AP courses—are also associated with wider AP racial enrollment gaps, and for the most part, for both Black and Hispanic students. Taking AP course offering as an example, a one standard deviation increase in the average number of AP courses offered per school in a district (approximately seven courses) would be associated with a wider White-Black AP enrollment gap by more than two percentage points, and White-Hispanic gap by close to one percentage point. Similar to the results presented in Table 3 where state-level policies are associated with overall AP and DE participation rates, these policies are also important predictors of racial enrollment gaps in AP and DE programs. Specifically, states with stronger accountability measures for access and student outcomes have larger White-minority gaps for both AP and DE compared with states with weak accountability measures, with point estimates ranging from one to two percentage points. In contrast, stronger financial incentives for DE participation are associated with smaller racial enrollment gaps in DE programs. The associations are particularly pronounced for White-Black DE participation gaps: When there are state mandates in place for local or state agencies to cover full or part of students' tuition for DE programs (strong financial incentives), the White-Black enrollment gaps in DE programs are almost four percentage points smaller than districts where parents and/or students are solely responsible for the costs associated with participating in DE programs (weak financial incentives). Taken together, these results suggest that financial resources and support may be critical in expanding access and participation in DE programs among minority students. This study takes advantage of recent data from the U.S. Department of Education's CRDC to provide a national perspective on racial gaps in participation in two major programs intended to help high school students get a jump start in college. While there have been a number of studies that examine students' enrollment in AP programs, our study makes the first attempt in describing racial inequity at the district level in both AP and DE at the national scale, and are able to shed light on similar or differential patterns across school districts between the two largest college acceleration programs. First, our results suggest that the majority of districts have nontrivial racial gaps in both AP and DE programs, with more pronounced gaps in AP than DE and with wider White-Black gaps than White-Hispanic gaps in both programs. Yet, even for White-Black gaps in AP participation, the largest gap on average among the four, our descriptive findings identify several hundred districts where Black students both have high participation rates—defined as reaching above the national median—and White-Black participation gaps are near-zero or even negative. The variation we display presents an opportunity to examine and scale innovations to both expand access to college acceleration opportunities and close gaps in participation. A sensible next step would be for researchers, practitioners, and policymakers to collaborate in identifying and documenting the policies and practices among districts (and their partner colleges in the case of DE coursework) that are highly effective in serving students through college acceleration programs. Second, we find suggestive evidence that AP and DE programs may serve as substitutes to each other for schools—when they are allocating resources among different college acceleration programs, and for parents—when they are considering which program to enroll their children in. While there have been a growing number of studies that assess the benefits of AP and DE separately, there is less information about what factors families and students consider when they choose between different types of college acceleration programs. To help students and schools make informative decisions, future research may wish to examine whether the relative benefits of different college acceleration programs vary depending on specific student characteristics, such as academic ability, family socioeconomic background, and type of college intended. Third, among all the factors examined, the White-minority achievement gap prior to high school is the strongest predictor of racial gaps in AP and DE participation. Once we take into account the White-minority achievement gap, White and minority students are predicted to be equally likely to participate in AP and DE classes. This finding is consistent with a number of other studies that identified substantially smaller or even reversed racial gaps in models that condition on measures of academic achievement (e.g., Conger et al., 2009). We also find that conditional on the achievement gap, differences in district- and state-level contexts are only able to explain a small amount of variation in equity gaps. However, it is not entirely clear how to unpack this finding. One possible explanation is that the pre–high school achievement gap has "caused" minority students to have a smaller chance to be admitted into AP and DE programs. For example, the emphasis on performance criteria in determining a student's eligibility could serve as a strong obstacle for many students to take advantage of AP and DE opportunities and influence minority students disproportionately. A potential problem with this approach is that students' school grades only have low to moderate correlations with their performance on AP examinations, suggesting that a student's probability of success in AP programs may vary along dimensions other than school grades (College Board, 1998). Thus, incorporating additional measures of students' likelihood of success in AP and DE programs and multiple eligibility criteria for AP and DE participation would enable districts to improve the accuracy of the screening process and may also have implications for the racial gaps in AP and DE participation (Richardson et al., 2016). An alternative explanation, however, is that the factors underlying racial achievement gaps, such as school resources and policies, largely overlap with the factors that explain racial gaps in AP and DE participation. This type of scenario suggests that a single policy or practice may be insufficient to eliminate or narrow the current racial gaps in AP and DE participation, and the goal of integrating minority students into college acceleration programs can hardly be achieved alone without comprehensive and persistent efforts to reform the social and school structure where racial achievement gaps are rooted. Fourth, our results indicate that a handful of local factors that are associated with higher overall program participation, such as a larger number of AP courses offered and higher per-student instructional expenditures, are associated with wider racial gaps in AP enrollment. This is consistent with our descriptive results that districts with higher AP and DE participation overall also tend to have larger racial gaps in AP and DE enrollment. Along a similar vein, districts in states with stronger mandates in access to AP and DE programs have larger White-minority gaps than districts in states with weak mandates, indicating that White students, as compared with minority students, might be in a better position to take advantage of these types of incentives. An important implication of these findings is that districts with greater resources surrounding college acceleration programs, while achieving strong AP and DE participation overall, may also engender racial disparity if there are inadequate efforts to prioritize equitable access to and success in such programs. For example, racially minoritized students—particularly those from lower-income families—may, as a result of accumulated educational disadvantage, experience other barriers in accessing college acceleration programs even when they meet the performance criteria, such as limited guidance about how to take advantage of AP and DE programs. In addition, research indicates that better-resourced schools are often more likely to use academic tracking and other forms of homogeneous ability grouping, such as gifted programs (Loveless, 2009). While these programs may provide targeted instruction early on to prepare students for more advanced coursework such as AP, they may also serve as a driver of segregation within schools along lines of race and family socioeconomic background (Roda, 2015; Rui, 2009). To the extent that curricular rigor influences students' educational aspirations and college choices (Cabrera & La Nasa, 2000; Hauser & Anderson, 1991), the underrepresentation of minority students in gifted programs and high ability tracks could engender racial disparities in their probability of participating in college acceleration programs even when they are eligible to. Thus, in addition to increasing educational resources and offering abundant college acceleration opportunities, it is also critical for districts and schools to be committed to integration and make intentional efforts to alleviate racial gaps in utilizing college acceleration resources. A number of studies have documented several promising efforts in this regard. Among these discussions, one promising avenue that has received great emphasis is more effective advising processes from teachers and school counselors, such as targeting students for advising, providing advisors with professional development opportunities to learn about college acceleration programs, and making information easily available to both parents and students (e.g., Flores & Gomez, 2011; Goldhaber et al., 2015; Kerr, 2014; Kettler & Hurst, 2017; Quintero, 2019; Whiting & Ford, 2009). For example, Kerr (2014) documented the efforts made by teachers in a high school to close the non-White/White AP enrollment gap. Specifically, the teachers created a space for middle and high school teachers to collaborate which, first and foremost, made middle school teachers aware of the racial disparities that existed in advanced courses and their role in closing them. In addition, given the critical role teachers' recommendation plays in determining a student's eligibility to AP and DE programs, it is important that school districts provide professional development opportunities around explicit and implicit bias training to ensure that those who advise students for college acceleration programs do not hold biases that disadvantage underrepresented minorities. Prior literature has documented the way in which implicit biases can foster negative attitudes and lead to stereotypical beliefs about individuals from different backgrounds (Carnes et al., 2012; Gershenson & Dee, 2017). A number of studies focused on teaching and advising practices surrounding AP and DE and identified mixed findings regarding the existence of these biases and how they affected who was targeted and encouraged to pursue college acceleration opportunities. For example, based on interview data with teachers and counselors in Texas, Miller et al. (2018) found little evidence to support the existence of implicit biases or discrimination in advising practices. On the other hand, using an adapted audit study where the authors asked a sample of school counselors to make AP recommendations based on transcripts that had identical information but were randomly assigned student names suggestive of gender and race, Francis et al. (2019) found that Black female students were less likely to be recommended for AP Calculus and were rated as being the least prepared. Other research has also shown the extent to which teacher biases and expectations are systematically related to student racial/ethnic background and how this affects the course recommendation process (Archer-Banks & Behar-Horenstein, 2012; Campbell, 2012; Francis et al., 2019; Grissom & Redding, 2016; Ho & Cherng, 2018; Oakes, 2005). Implicit bias training has the potential to make teachers and counselors aware of their role as gatekeeper and learn about ways to move past biases so that all eligible and potentially successful students are encouraged to enroll in college acceleration courses. Finally, districts in states with stronger financial support to DE participation are associated with smaller White-minority DE participation gaps. This is not surprising given that the financial hurdles in regard to DE participation faced by students from less affluent families have been well documented (e.g., Karp et al., 2004; Museus et al., 2007; Roach et al., 2015). These barriers include both direct costs of participation, such as the tuition associated with college courses, and indirect costs, such as textbooks and transportation to local colleges. Results from our analysis further highlights the importance for policymakers to take into consideration the financial constraints of potential participations, especially those from less affluent backgrounds, in the allocation of funding and targeted support. Our study is subject to a number of limitations and caveats. First, dual enrollment participation is new for the 2015–2016 CRDC data collection. Dual enrollment programs and courses are offered in different modalities and taught by both high school and college instructors, compared with AP coursework, which is more standardized. Although the CRDC has provided detailed definitions regarding what counts as DE and requires that schools and districts certify the accuracy of their data on submission, there may still be measurement errors (beyond what may be expected in this type of national administrative data) on the reporting of DE participation. Additionally, the broad definition of DE used by the CRDC also prevents us from differentiating between different types of DE programs in this study. Since DE programs vary substantially in multiple dimensions, the type of students enrolled in different DE programs may vary considerably. As a result, the way various local- and state-level factors predict overall participation and racial gaps in DE participation may largely depend on the specific characteristics of particular DE programs. Therefore, it is critical for future data collection to include detailed information regarding the specific attributes of a DE course or program. Furthermore, since the primary focus of our article is on racial gaps in AP and DE participation, we had to restrict our sample to districts with at least one racial group having a nonzero participation rate. As a result, our findings may not speak to the factors that are associated with the availability of AP (DE) programs in general, especially local factors that lead to complete absence of AP or DE opportunities in a district. Finally, though the focus on this study was on access to college acceleration programs, educational leaders and policymakers should also be focused on student success in AP and DE courses, tracking student progression and momentum into and through college. For example, previous research, including documentation from the College Board, indicates that there remain racial and socioeconomic gaps in AP exam pass rates, even among AP course participants (College Board, 2017). Our study uses a broader definition of access to AP, counting students as participating if they ever took an AP course. As a result, the racial gaps reported in our study are likely to be larger if we were able to also take into account disparities in the number of students who take and pass the AP exam. Despite these limitations and caveats mentioned above, our analysis provides several important policy implications regarding college acceleration programs. Although we cannot make any causal claims based on the observed correlations between racial gaps in AP and DE participation and district-level and state-level factors, they point to potential channels that may affect minority students' participation in college acceleration opportunities. In light of the positive relationship between these opportunities and important student educational outcomes, such as subsequent college enrollment and performance, channels identified in this study could contribute to racial gaps in education attainment and labor market opportunities. Further analysis of these associations through longitudinal research designs that enable researchers to draw causal conclusions would be a valuable direction for future work and has the potential to inform broader policy discussions around college access and completion.
10.3102_0002831221996669	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831221996669	The Role of College-Bound Friends in College Enrollment Decisions by Race, Ethnicity, and Gender	 This study examines the association between college-bound friends and college enrollment using restricted transcript data from the High School Longitudinal Study. Propensity score matching and school fixed effects models suggest that having close college-bound friends is positively associated with enrolling in college. However, Black and Latino male students are much less likely to benefit from having college-bound friends than others, suggesting that structural and cultural factors that are tied to race, ethnicity, and gender may limit the beneficial potential of friends, especially for these male Black and Latino students. Implications for addressing racial and ethnic disparities in college enrollment and for the role of friends in college enrollment decisions are discussed.	 Black and Latino students demonstrate a clear and persistent disparity in their college enrollment rates relative to their White and Asian counterparts (de Brey et al., 2019; Kena et al., 2015). Among the potentially important strategies for improving college enrollment rates for Black and Latino students is improving their access to friendship networks that contain valuable knowledge and information about the college-going process (Perna & Titus, 2005). Adolescent friendships are crucial for the development and direction of students' educational trajectories (Carbonaro & Workman, 2013, 2016; Coleman, 1961; Giordano, 2003; Hallinan & Williams, 1990; Hauser et al., 1983; Patacchini et al., 2011). Yet we know little about whether students can use close college-bound friends (CBF) as assets for college enrollment or whether this association varies by important characteristics such as race, ethnicity, and gender. To this end, this article examines heterogeneity in the association between having close CBF and college enrollment for a nationally representative sample of approximately 24,000 students in the United States. Knowledge about whether friends, perhaps the most important drivers of adolescent behavior, are assets that are beneficial for college enrollment allows for a deeper understanding of the college-going process for Black and Latino students. Scholars of status attainment have for decades highlighted the role played by friends and other influential actors in predicting educational attainment (Haller & Portes, 1973; Hauser et al., 1983; Lin, 1999; Sewell et al., 1969). There now exists evidence for the association between the educational orientation of peers and enrolling in college (Crosnoe & Muller, 2014; Engberg & Wolniak, 2010; Holland, 2011; Langenkamp & Hoyt, 2017b; Massey et al., 2011; Sokatch, 2006) and between peers and educational outcomes while in college (Sacerdote, 2001, 2011; Wolniak & Engberg, 2010). What is still relatively unknown, however, is whether close friends—who may represent more intimate and influential ties than peers—are associated with the decision to enroll in college (Hallinan & Williams, 1990). Furthermore, college enrollment decisions may also vary by students' characteristics such as race and ethnicity (Acevedo-Gil, 2017; Auerbach, 2004; Engberg & Wolniak, 2009; Hill, 2008; Langenkamp & Hoyt, 2017a) or by gender, yet little is known about heterogeneity in the association between close CBF and college enrollment by race, ethnicity, or gender (Perna & Titus, 2005). The key questions that this article seeks to answer are the following: (1) Is having close CBF associated with students' likelihood of enrolling in college and (2) If so, does this association vary by race, ethnicity, and gender? Based on previous studies, I expect CBF (defined by students' answers directly asking about close friends who intend to go to college) to be positively associated with these outcomes. I also expect that the association between CBF and college enrollment varies by race and ethnicity, with Whites benefiting more from having CBF. In terms of gender, I expect males to express greater sensitivity to CBF than females, especially among Latino students. Here, I make five contributions to the literature: (1) theoretically distinguishing between peers and friends, (2) analyzing heterogeneity by race and ethnicity, (3) analyzing heterogeneity by gender, (4) analyzing college selectivity, and (5) using a counterfactual causal framework and sensitivity analyses to estimate treatment effects. Researchers often distinguish between "friends" and "peers" when examining how assets embedded in students' social context benefit their academic outcomes. Whereas friendships represent affective ties that rest on a foundation of mutually shared expectations of intimacy, trust, reciprocity, and altruism (Call & Mortimer, 2001; Giordano, 2003), peer relationships (i.e., classmates and other students who are not friends) often do not. Here, I focus on friends because of their potential to influence academic outcomes through the transmission of resources and information (Carbonaro, 1998; Carbonaro & Workman, 2013, 2016; Hallinan & Williams, 1990; Ream & Rumberger, 2008). Because friends influence educational outcomes, including educational expectations (Hauser et al., 1983), they may be an important asset for students' likelihood of enrolling in college. Adolescence is an especially salient time in the life course when friends matter a great deal for guiding student behavior (Coleman, 1961; Newman & Newman, 1976). Reciprocity, trust, shared expectations, intimacy, emotional investments, affection, and group enforcement of norms act as conduits for transmitting valuable information and resources between students (Coleman, 1988; Hallinan & Williams, 1990; Parsons, 1963). Giordano (2003) outlined three explanations for why close friends may be particularly influential during adolescence. First, constant communication and face-to-face interaction between close friends can enhance the opportunity for friends to influence each other. Second, given that homophily and shared interests are common in student friendship networks (Kubitschek & Hallinan, 1998), close friends may confer heightened levels of influence. Finally, students will conform to expectations of close friends in order to preserve those most valuable social ties. Close friends are therefore most likely to guide students' behavior because the affective bonds between them increases the trustworthiness and validity of the information that passes between them. Friends are likely to share resources and information with each other given constant contact and closeness in a process of "diffusion" (Payne & Cornwell, 2007). Diffusion relies on direct ties such as those formed through the bonds of friendship (DiMaggio & Garip, 2012). Friends not only self-select into friendships based on shared interests and academic performance (Flashman, 2012) but also become more similar over time and come to influence one another's academic outcomes (Kandel, 1978; Lomi et al., 2011) Friends may directly encourage and motivate one another to study hard, focus, and remain on a college path throughout high school. Friends also provide companionship and camaraderie that may ease the oftentimes isolating academic path to college during adolescence. Close friends who are also college bound are likely to confer influence on students' college enrollment behavior because of the pervasive social pressure to conform to the normative expectation of attending college. Previous research suggests that friends' characteristics are associated with high school academic performance (Crosnoe et al., 2003), advanced course taking (Riegle-Crumb et al., 2006), and college readiness (Alvarado & An, 2015). Scholars of education often break down college-going into distinct stages: planning, applying, being accepted, and enrolling into college (DesJardins et al., 2006; Grodsky & Riegle-Crumb, 2010). Among the reasons that friends may be important at the planning, applying, and enrolling stages of the college-going process are that (1) they can create pressures to conform and (2) students may adjust their educational plans after comparing themselves to the plans of their friends. Consequently, friends can fulfill the task of being role models for each other and can construct the normative environment that can signal group expectations and values (Buchmann & Dalton, 2002; Woelfel & Haller, 1971). Thus, students may refer to their friends and act accordingly to (1) keep pace with their friends' educational investments and (2) avoid short and long-term marginalization from the group (Sewell et al., 1969). Indeed, friends can have a positive influence on students' high college expectations (Carbonaro & Workman, 2016). In addition to (and distinct from) friends, scholars have found that high school peers can influence college attendance (Babcock, 2008; Bifulco et al., 2011; Bifulco et al., 2014; Engberg & Wolniak, 2010; Fletcher, 2013; Mullen, 2010; Palardy, 2013; Perez & McDonough, 2008; Person & Rosenbaum, 2006; Wells et al., 2013). However, Hallinan and Williams (1990) argued that friends may have a stronger impact than peers in the college-going process. Following earlier research (Alvarado & An, 2015; Alvarado & Turley, 2012; Calvo-Armengol et al., 2009; Cherng et al., 2013; Crosnoe et al., 2003; Hallinan & Williams, 1990; Langenkamp, 2010; Riegle-Crumb et al., 2006), I focus on friends, rather than peers, because of the direct influence they may have on educational outcomes through strong affective ties. Research suggests that race and ethnicity may moderate the association between social context and college-going decisions (Langenkamp & Hoyt, 2017b; Laura Walter Perna & Titus, 2005). Still, research on the heterogeneous effects of social context by race and ethnicity remains nascent and mixed (Cheng & Starks, 2002; Fletcher, 2011). Two processes—one structural and one cultural—may explain heterogeneity in the association between social context and educational outcomes. Among the structurally rooted processes are stereotype threat, oppositional culture, and neighborhoods. Black and Latino students may underperform academically as a result of stereotypes about their academic ability (Steele & Aronson, 1995), possibly leading the most high-achieving Black and Latino students (who are presumably considering college) to think as themselves as unworthy of being role models for their friends. Structural discrimination in the wider society may lead to the development of an oppositional culture among Black and Latino students that eschews an educational system that does not reflect and often marginalizes them (Fordham & Ogbu, 1986). This may reduce the ability of CBF to influence Black and Latino students if they feel as if such friends would come at the cost of social standing among co-ethnics. Both the supply (stereotype threat) and demand (oppositional culture) of college influencers may then combine to reduce the degree of influence of CBF among Black and Latino students. The influence of friends may also be tempered among Black and Latino students who grow up in disadvantaged and unsafe neighborhoods due to the development of lack of trust as a survival strategy (Giordano, 2003). Instead of turning to friends, Black and Latino students in these contexts may turn to their families as primary sources of influence (Portes & Zhou, 1993). One relevant culturally rooted process is familism, briefly defined as the cultural preference for placing family goals above individual goals (Moore, 1970; Sabogal et al., 1987; Triandis et al., 1982). Although familism is often tied to Latino immigrant educational processes (Lopez-Turley, 2006; Ovink & Kalogrides, 2015; Turley et al., 2010), it may also apply to Black students (Giordano et al., 1993; Larson & Asmussen, 1991; Stack, 1974). There is an ongoing tension in the literature about whether familism acts to either increase or decrease educational investments. Elements of familism such as strong intimate ties among family members (nuclear and extended), loyalty, obligations, and a sense of familial responsibility may positively affect educational investments by protecting students from deviant peers (Germán et al., 2009) and by providing social and moral support for Latinos embedded in the college choice process (Ceja, 2004, 2006; González et al., 2003). Moreover, familism may be used to increase educational investments if educational institutions can harness and direct the power of influence that families have on Black and Latino students toward educational goals. Given the limited amount of information that parents can provide Latino students, in particular, about college (Ceja, 2006), creating spaces where Latino families and students can gain knowledge and confidence about college together in concert with their intersectional lived experiences and identities through "college-conocimiento" may lead to increased investments in students' education (Acevedo-Gil, 2017; Auerbach, 2004). Similarly, connecting Black and Latino students' education with narratives about the struggle and oppression in their history could be another way to leverage the influence of friends to improve educational investments (Cammarota, 2004; O'Connor, 1999). Familism may also suppress educational investments by placing pressure on students to stay close to home for college. (Desmond & Turley, 2009; Lopez-Turley, 2006; Ovink, 2014; Ovink & Kalogrides, 2015; Perez & McDonough, 2008; Turley et al., 2010). Yet evidence suggests that it may not be familism per se that deters Latino students from college enrollment but rather socioeconomic background and prior achievement. These factors may explain most of the gap between Latinos' and Whites' college enrollment (Ovink & Kalogrides, 2015). Moreover, in those Latino families that demonstrate an interaction between high levels of familism and high levels of parental education, students' educational outcomes may actually increase (Valenzuela & Dornbusch, 1994). Gender may also moderate the association between CBF and college-going decisions; however, research on this is also nascent and mixed (Riegle-Crumb, 2010; Wells et al., 2013). In particular, despite the fact that females overall report having more academically focused friendship groups that may lead to higher odds of college matriculation (Riegle-Crumb, 2010), Latina students may face cultural expectations regarding familial obligations that could dim the influence of their high-achieving friends compared to Latino males. This highlights the way that race, ethnicity, and gender often interact to produce acute forms of inequality. Therefore, my analysis adopts an intersectional approach that seeks to uncover variation in the benefits of having CBF by race, ethnicity, and gender (Collins, 2002; Hancock, 2016). Despite the fact that the Latina population in the United States has grown sharply, Latinas remain underrepresented in their college enrollment, retention, and 4–year degree completion (de Brey et al., 2019; Otero et al., 2007; Sy & Romero, 2008). The role of familism in educational decisions may be more acutely felt among Latinas than Latinos (Ovink, 2014). Moreover, given that many Latinos are first-generation college students, Latina students may be especially likely to turn to high school counselors for guidance about college (Riegle-Crumb, 2010). Unfortunately, research suggests that Latino students, who may in fact need access to counselors more than others (Stanton-Salazar, 1997, 2001), are also the least likely to have it (Perez & McDonough, 2008; Perna et al., 2008). Once in college, Latina students report increased responsibilities and expectations to further support the family, despite feelings of increased levels of independence (Sy & Romero, 2008). College selectivity is increasingly becoming a crucial determinant of life outcomes and a source of stratification along racial and ethnic lines (Alon & Tienda, 2007; Posselt et al., 2012). Still, high-achieving yet low-income students often do not apply to any selective colleges (Hoxby & Avery, 2012). The fact that many of these students are also Black and Latino may lead to their underrepresentation at selective and highly selective colleges (de Brey et al., 2019). Compared to a high school degree, college graduates benefit from increased returns in terms of economic outcomes, health, stable families, and access to advantageous social networks (Hout, 2012). Moreover, students who are the least likely to go to college are the most likely to benefit from doing so (Brand & Xie, 2010). Despite similar earnings for enrollment in 2- and 4-year colleges (Kane & Rouse, 1995), attending any 4-year college may increase the likelihood of attaining a BA degree (Kane & Rouse, 1999; Rouse, 1995) and therefore lead to higher levels of occupational prestige, lifetime earnings, and health and well-being. Enrolling in a selective or highly selective 4-year college is valuable because of its positive association with well-being in adulthood (Alon & Tienda, 2005; Brand & Halaby, 2006; Gerber & Cheung, 2008; Hout, 2012; Long, 2008; Zhang, 2005). Yet Latino students are the least likely to enroll in any 4-year colleges (Fry, 2011; Hurtado et al., 1997). Latinos who do enroll in 4-year colleges disproportionately enroll in less selective colleges (Fry & Taylor, 2013). Still, net of controls, Latinos are more likely than Whites to enroll in more selective colleges (Langenkamp & Hoyt, 2017b). Given this backdrop, it is important to control for as many variables that may increase the probability of having CBF and enrolling in college as possible, especially selective ones, for Black and Latino students. Here, I respond to concerns about selectivity that are endemic to the literature on the influence of friends (Durlauf, 2002; Hauser, 1970) by using a counterfactual causal framework, propensity score matching (PSM), that matches respondents who did and who did not receive the treatment, having close CBF, using a host of theoretically driven observed characteristics from the High School Longitudinal Study (HSLS). Given that matching models are only as good as the variables that go into them (Cook et al., 2009; Heckman, 2005; Morgan & Harding, 2006; Rosenbaum, 2002; Shadish et al., 2002), I also invoke a formal sensitivity analysis to address threats from an unobserved confounder and I control for unobserved confounders at the school level using school fixed effects (FE). Still, the problem of reflection, whereby the outcomes of two friends affect each other simultaneously, threatens to render estimates spurious (Manski, 1993). The reflection problem arises when regressing outcomes such as test scores of a respondent on the test scores of her friend. Here, I am not comparing outcomes of respondents to outcomes of friends. Instead, I am using the expectations of friends to predict behaviors of respondents. This distinction perhaps limits threats from reflection given that friends' expectations and students' eventual behaviors are at least somewhat different. Still, my analysis would be strengthened by direct measures of friends' expectations, rather than respondents' interpretations of friends' expectations, regardless of the ability of friendship bonds to provide accurate estimates of friends' plans. That is, my predictor may represent respondents' endogenous preferences for like peers (McPherson et al., 2001), thereby making my measure of friends simply a measure of homophily. Despite my efforts to adjust for factors that may affects friendship selection based on shared interests and academic performance, these data are simply unable to completely adjudicate between homophily and friends—therefore warranting caution when interpreting my results as causal. I use restricted transcript data from the HSLS of 2009, an ongoing nationally representative survey of approximately 24,000 students nested within 944 public and private high schools who were in the ninth grade in the fall of 2009. The Institute of Education Sciences, U.S. Department of Education, administers the HSLS. The HSLS collected data from parents and school administrators about sociodemographic, academic, and school-level characteristics in the fall of the ninth grade. Subsequently, students, parents, and administrators were interviewed in the spring of 11th grade. HSLS fielded an update in the fall of 2013, immediately after scheduled high school graduation, to obtain information about postsecondary behaviors and high school achievement data from transcripts. The HSLS is especially useful in answering the questions I have raised because of the rich information it contains on eighth-grade achievement; ninth- and 11th-grade sociodemographic, academic and school characteristics; transcript data on coursework and achievement; data on college application and financial aid; as well as data on close CBF in high school and college enrollment. These data are arguably the ideal data set to study college outcomes among the most recent cohort of college students across the United States. The HSLS also provides key methodological advantages in terms of both the longitudinal design and national representativeness. Missing data ranged from 0.05% to 9.66%. Rather than drop cases with missing values, I imputed missing data using the ice command in Stata (Allison, 2002; von Hippel, 2007). I round all sample sizes in accordance with National Center for Educational Statistics (NCES) guidelines for analyzing restricted data. The treatment variable is a binary (0, 1) indicator for whether students had close CBF in the spring of 11th grade. That is, the HSLS asked students, "How many of your close friends plan to attend a 4-year college?" Although I cannot distinguish between the level of closeness (e.g., best, second best, etc.), the HSLS question is explicitly asking about close friends and not mere acquaintances. Students could choose from the following four options: less than half, about half, more than half, and all of them. I operationalized CBF as having at least 51% close friends who planned to attend a 4-year college (i.e., those who answered more than half or all of them). Therefore, students received a 1 on the binary indicator if they answered that either more than half or all of their close friends planned to attend a 4-year college and a 0 if they answered that less than half or about half of their close friends planned to attend a 4-year college. CBF reflects respondents' own understanding of the college-going expectations of their close friends. The four outcomes I analyze represent students' self-reported college enrollment that HSLS measured during the fall of 2013. The first outcome indicates whether students enrolled in any college (e.g., less than 2-year, 2-year, or 4-year). Students were coded 1 if they were in any college and 0 if they were not in any college in the fall of 2013. That is, the first outcome contrasts students in any level of college versus those who were not in college. The second outcome indicates whether students were in a 4-year college (coded 1) versus if they were in a less than 2-year or in a 2-year college (coded 0). That is, the second outcome contrasts students who were in a 4-year college to those who were in college but not in a 4-year college. The third and fourth outcomes refer to the selectivity level of the 4-year college that students were enrolled in. That is, students were first asked about which specific 4-year college they were enrolled in and then the NCES later assigned selectivity rankings to the college that students named using Integrated Postsecondary Education Data System selectivity rankings for those colleges. Specifically, the third outcome indicates whether students were enrolled in a selective 4-year college. Students were coded 1 if they were enrolled in (1) a highly or (2) a moderately selective 4-year college and 0 if they were enrolled in (3) an inclusive 4-year college or (4) a 4-year college whose selectivity was unclassified. The fourth outcome indicates whether students were enrolled in a highly selective 4-year college. Students were coded 1 if they were enrolled in a highly selective 4-year college and 0 if they were enrolled in (5) a moderately selective 4-year college, (6) an inclusive 4-year college, or (7) a 4-year college whose selectivity was unclassified. Means and standard deviations for all variables are broken down by race-ethnicity in Table 1. I include eighth-, ninth-, and 11th-grade covariates in this analysis that account for achievement, sociodemographic background, college expectations, and school climate. I also include transcript data on grade point average (GPA), ACT and SAT scores, and any financial aid students received for college that NCES collected in fall 2013. Collectively, these covariates are important to include in this analysis because of their ability to predict CBF as well as the college enrollment outcomes. Specifically, I account for students' academic interest and achievement with a dichotomous indicator of whether the student took an advanced math or science course in the eighth grade, a dichotomous indicator of whether students received an A or B in eighth-grade science or math, and continuous measures of ninth- and 11th-grade math test scores. I also include a dichotomous indicator for whether students have an educational or career plan and whether students are sure they will go to college as well as indicators for the level of education that parents expect for their children. Furthermore, given previous research on the influence of classmates and school climate on educational outcomes, I include a vector of covariates that address the socioeconomic and academic climate of the school. These covariates also address structural arrangements in schools that may affect college enrollment. For example, I include continuous measures of the percentage of students receiving free or reduced-priced lunch, the percentage of students taking Advanced Placement (AP) courses, the number of certified full-time math and science teachers, dichotomous indicators of whether the school offers advanced science and math courses (i.e., AP calculus AB/BC, computer science, AP computer science A, AP computer science B, AP computer science AB, Advanced/II/AP/IB (International Baccalaureate) biology, Advanced/II/AP/IB chemistry, Advanced/II/AP/IB physics), whether the school requires completion of specific math or science courses for graduation, and a scale for whether the school has a proscience climate. All of these school-level covariates are measured in the ninth grade. Finally, I include a covariate for whether the school uses a tracking policy to place students in ninth-grade courses. Following the counterfactual causal framework (Morgan & Harding, 2006; Morgan & Winship, 2014; Shadish et al., 2002), the "effect" is defined as the difference in outcome between the scenario in which an individual has CBF and the counterfactual scenario in which a similar individual does not have CBF. I estimate students' propensity to have CBF conditional on achieving proper covariate balance using observed characteristics. To be clear, the estimates I present are not strictly causal. Specifically, I used PSM, as developed by Rosenbaum and Rubin (1983b; Rubin, 1974, 1978, 1980). Previous scholars have summarized the strengths and weaknesses of this approach for estimating effects in the absence of randomized data (Becker & Ichino, 2002; Caliendo & Kopeinig, 2008; Stuart & Rubin, 2008). I compared students with CBF with the control group of students without CBF and estimated the average treatment effect on the treated (ATT). I chose the ATT because I am primarily interested in the treatment effect on those for whom the treatment is intended (Heckman et al., 1997). Caliendo and Kopeinig (2008) further discussed the utility of the ATT over the ATE when using PSM. Moreover, with regard to PSM, Morgan and Winship (2014, p. 173) argued that "the only target parameter that can be estimated with any degree of confidence is the ATT." For further discussion of the primacy of the ATT in PSM, see Austin (2011) and Imbens (2004). The strength of matching lies in its ability to reduce the role of observed covariates on any remaining differences between students who had CBF and students who did not have CBF after matching if selection into treatment depends exclusively on observed variables (D'Agostino, 1998). The benefits of matching over traditional regression are that matching (1) allows for the explicit assessment of covariate balance to minimize bias between treatment and control groups, (2) is nonparametric, and (3) allows for the formal testing of confoundedness in treatment effects (Caliendo & Kopeinig, 2008; Heckman et al., 1998). The drawbacks of PSM are that (1) it only addresses selection on observed variables (allowing bias from selection on unobserved variables to remain) and (2) it does little to address simultaneity bias (e.g., where students are friends because they are both college-bound, drawing into question whether their friends' college boundedness can affect college outcomes). I explicitly address the first drawback through a formal sensitivity analysis. The second, however, largely remains a limitation of the current study. To be clear, the PSM technique is not better than the variables that go into it (Morgan & Harding, 2006; Rosenbaum, 2002; Shadish et al., 2002). Also, other data sets, such as Add Health, identify friends and would therefore facilitate the calculation of the predicted likelihood that friends are college bound; however, Add Health is more limited in its educational data. Unfortunately, the HSLS does not identify specific friends. First, I select the covariates upon which the matching will be based (Augurzky & Schmidt, 2001). I compiled a vector of variables that is based on the relevant literature that would conceivably predict treatment assignment as well as impact the outcomes. Table 1 lists the vector of covariates I used for matching save for the treatment and outcomes. I entered all covariates into the selection model as main effects. Second, I estimate the predicted probability of receiving the treatment by calculating the propensity score using a logit model (Leuven & Sianesi, 2003) and then I match individuals who had similar propensities to have CBF through kernel matching. All analyses are restricted to observations that fell in the region of common support (Caliendo & Kopeinig, 2008). Third, I evaluate whether the groups being compared have equal (or sufficiently balanced) distributions of relevant observed covariates (Dehejia, 2005). I evaluated balance by inspecting pre- and postmatching standardized bias scores for variables in the aggregate as well as individually. Finally, I compare the outcomes of the respondents in the treatment and control groups. I compare the propensity score model treatment effects across racial and ethnic groups by invoking a post hoc difference in means test that is recommended by Paternoster et al. (1998, see Equation 4). A key limitation of the PSM technique lies in its inability to reduce the bias in estimated treatment effects that stems from unobserved confounders that affect both treatment assignment as well as the outcome (Stuart & Rubin, 2008). As a result, I conducted a formal sensitivity analysis (for details, see Supplemental Appendix A, available in the online version of the journal) of the statistically significant ATTs. Shadish et al. (2002, p. 165) write that "[w]hen sensitivity analysis is combined with matching on propensity scores, these new statistical developments provide an important new analytic tool in the arsenal of quasi-experimentation." Supplemental Appendix B (available in the online version of the journal) summarizes results from unadjusted logistic regression interaction models of having CBF and race and ethnicity run first for all students and then run separately by gender. These results demonstrate the total differences between the race and ethnic impacts of CBF for all students, then for females, and then for males. At first glance, the results for all students suggest that all Black and Latino students (males and females) benefit less than White and Asian students from CBF for all four outcomes. However, upon closer inspection of the separate impacts of CBF for females and males, a slightly different picture emerges. That is, the gender-specific results suggest that both race and ethnicity and gender may play a role in the effect of having CBF. These racial and ethnic and gender differences highlight the potential for intersectional variation in the benefits students accrue from having CBF. For instance, the impact of CBF on enrolling in any college appears to be even smaller for Black and Latino male students compared to their co-ethnic female counterparts. This pattern of Black and Latino males benefiting less from CBF than their co-ethnic female counterparts persists throughout almost all the models (save for Black females and males enrolling in a selective college). In fact, in many of the models, Black and Latina females show no statistically significant difference compared to White co-gender students in the impact of CBF. These results provide a coarse foundation that will serve as the basis for the more rigorous PSM models. Table 2 summarizes results from the propensity score models that match students who had CBF with those who did not, based on their similar observed propensity to have CBF after adjusting for all the covariates in Table 1. The table distinguishes across racial and ethnic subgroups by rows and across college enrollment outcomes by columns. For each racial and ethnic subgroup and for each outcome, I provide information in regards to not only the ATT but also the standard error, t statistic, the number of untreated and treated respondents in the region of common support (i.e., the area of propensity scores wherein matches were made), the number of treated off support, as well as the mean standardized bias prior to matching and after matching. The first finding is for the performance of the propensity score model to improve the quality of the treatment and control matches. Table 2 demonstrates a clear decrease in the bias after matching (i.e., suggesting an improved "apples to apples" comparison than traditional regression) based on observed characteristics. That is, the PSM model's achieved postmatching bias score comes very close to the theorized absolute 0 mean bias level, which would suggest that treatment and control cases that the PSM is comparing are as similar as possible based on observed covariates. While there is no standardized threshold for postmatching bias in the literature, a postmatching bias of approximately 5% is generally viewed favorably (Caliendo & Kopeinig, 2008). Figure 1 further demonstrates an improvement in balance (i.e., the postmatching moves toward the center line [0% bias]) for each of the observed covariates individually for all students. Examining the ATTs for all students combined suggests that CBF increases the probability of enrolling in any college by 6 percentage points, enrolling in a 4-year college by 10 percentage points, enrolling in a selective 4-year college by 3 percentage points, and enrolling in a highly selective 4-year college by 7 percentage points. However, after examining the ATTs by race and ethnicity, there are differences in the degree of impact that CBF has on college enrollment decisions. For example, when examining the first outcome, enrolling in any college, White, Asian, and Black students benefit from CBF by 7, 8, and 4 percentage points, respectively. However, Latino students' enrollment in any college does not appear to benefit from CBF. Moreover, post hoc difference in means tests (Paternoster et al., 1998) suggest that the benefits for Asian students and Black students are each statistically indistinguishable from the benefits for White students (p < .05). This suggests that while White, Asian, and Black students benefit from CBF in terms of their likelihood of enrolling in any college, Latino students do not. With regard to enrolling in a 4-year college, the results suggest that White, Asian, Black, and Latino students indeed benefit from CBF by 10, 7, 8, and 12 percentage points, respectively. Post hoc difference in means testing also reveals that the benefits for Asian students, Black students, and Latino students are not statistically distinguishable from the benefits for White students. Therefore, it appears that all students benefit from CBF to an equal degree in terms of their likelihood of enrolling in a 4-year college. However, with regard to enrolling in a selective 4-year college, only White students appear to benefit from CBF by 4 percentage points. Asian, Black, and Latino students do not appear to benefit from CBF in terms of enrolling in a selective 4-year college. Finally, in terms of enrolling in a highly selective 4-year college, only White students and Asian students appear benefit from CBF by 7 and 8 percentage points, respectively. Moreover, their estimates for CBF are statistically equivalent. Supplemental Appendices C and D (available in the online version of the journal) summarize the results for female and male students (by both race and ethnicity), respectively. These results sufficiently convey the main findings of this article: Not only do race and ethnicity matter for the influence of CBF, but so does gender. Furthermore, echoing the results from the unadjusted interaction effects models, the PSM models demonstrate that males do not benefit as much as females do from CBF. Supplemental Appendix D demonstrates that Black and Latino males benefit from CBF far less than their male White and Asian counterparts do. In fact, in only one instance do Black and Latino male students benefit at all from CBF, that is, CBF increases the probability of 4-year college enrollment for Latino males by 10 percentage points. Otherwise, Black and Latino males do not benefit at all from CBF. Meanwhile, Supplemental Appendix C demonstrates that among female students, Black and (especially) Latina students benefit from CBF in their college enrollment outcomes. For example, Black females' probability of enrolling in any college increases as a function of CBF by 9 percentage points. However, Black females do not benefit in terms of any of the other more selective, and perhaps more socioeconomically advantageous, college enrollment outcomes. Moreover, Latina females benefit from CBF not only for enrolling in any college but also for enrolling in a 4-year college and for enrolling in a selective college (and all three of these effects are statistically equivalent to those of female White students). Neither female nor male Black and Latino students benefit from CBF in terms of enrolling in a highly selective college. Moreover, only female White and female Asian students benefited from CBF in terms of enrolling in highly selective colleges. The only findings that I can compare between females and males are those for the benefits of CBF on enrolling in a 4-year college. Post hoc significance testing reveals that these effects are statistically indistinguishable. That is, female and male Latino students benefit from CBF to a similar degree in terms of enrolling in a 4-year college. In general, the gender analyses reveal that CBF mainly affect White and Asian students, leaving Black and Latino female and (especially) male students without as much to gain from membership in these college-going friendship networks. Although the sensitivity analysis is no panacea for selection bias, I test the robustness of the ATTs to incremental erosions of the ignorability assumption (DiPrete & Gangl, 2004; Rosenbaum & Rubin, 1983a). This sensitivity analysis for PSM is limited, however, in that it detects only the impact of a single unobserved confounder on the treatment effect. Table 3 summarizes the results from the sensitivity analysis. I report the sensitivity analysis on the results from Table 2 (female and male students combined) because the gender-specific results are subsumed within this general sensitivity analysis. Supplemental Appendix E (available in the online version of the journal) summarizes the detailed results of the sensitivity analysis. While suggestive of robust treatment effects, the sensitivity analysis alone does little to build confidence in the ATTs. To that end, I compare the sensitivity analysis results with observed covariates to get a sense of what kind of variable must be lurking unobserved to render my results null. Supplemental Appendix F (available in the online version of the journal) summarizes the results from separate selection models that predict CBF based on observed covariates. Comparing each of the "killer" confounders with observed covariates that I have already included in the PSM model gives some confidence that the results may be qualitatively robust to unobserved bias. Even the least robust result (i.e., the benefit of CBF on enrollment in any college among Black students) suggests that researchers must find an as of yet unobserved covariate that has the same impact as being in poverty to undermine my result. Indeed, it may be difficult to imagine a variable that I have not yet included that (1) is as strong as or stronger than the observed covariates, (2) perfectly predicts the outcome, and (3) can affect selection to the point of undermining the treatment effects. As a second method of testing the robustness of the treatment effects, I supplement the results in Table 2 with results from models that decrease the number of individuals in the control group I use to construct the counterfactual outcome vis à vis the weighted averages in kernel matching. Table 4 summarizes the treatment effects after contracting the bandwidth to 0.03, yielding a smaller and more similar (in terms of propensity scores) group of controls with which to match to treated respondents. This exercise has the advantage of improving the "apples to apples" comparison, but it also has the disadvantage of a less smooth estimated density function, decreased fit, and increased variance between the estimated and the true underlying density function (Caliendo & Kopeinig, 2008). As expected, Table 4 demonstrates that decreasing the bandwidth parameter by a factor of 3 improves the comparisons between treated and nontreated respondents, as evidenced by the decreases in levels of mean postmatching bias. However, the qualitative change to a more stringent set of comparison groups did alter the findings somewhat. For instance, Table 4 demonstrates that CBF no longer benefits enrolling in 4-year or highly selective colleges among Asian students. Although the standard errors did not change much, the coefficients changed enough to render these associations null. Among Black students, the change in bandwidth also erased the benefit of CBF on enrolling in any college. Once again, a change in the coefficient size caused the ATT to disappear. Nevertheless, among Whites and Latinos, there was no significant overall impact on the estimated associations, despite decreasing the bandwidth from 0.09 (Table 2) to 0.03 (Table 4). Previously, the treatment indicated whether a student had 51% of close friends who were college bound or not. Alternatively, I created a second treatment variable that indicated whether all of the student's close friends were college bound. Table 5 summarizes the results from the models that use the alternative CBF specification for the gender-neutral sample. Despite racial and ethnic heterogeneity in the number of college enrollment outcomes that CBF positively affects, every racial and ethnic group benefits from CBF in at least one college enrollment outcome. The overall pattern of results and conclusions of this analysis, therefore, are generally robust to alternative specifications of the treatment indicator. Previous research suggests that including school FE may substantially limit bias from selection into friendships (Calvo-Armengol et al., 2009). Supplemental Appendix G (available in the online version of the journal) summarizes logistic regression models with school FE to address single- and multiple-variable unobserved confounding at the level of the school. Overall, the school FE findings suggest that (1) there continue to be CBF benefits net of unobserved school-level confounders and (2) the benefits of CBF continues to vary by race and ethnicity, generally showing stronger CBF benefits for White students. These findings from the school FE models in Supplemental Appendix G are in line with those from the PSM models in Table 2. However, when comparing the results from the school FE models to those from the PSM models, one must note that the sample comparisons are restricted to schoolmates in the school FE models and the sample comparisons are restricted to those on the region of common support in the PSM models. This may lead to some discrepancy in the findings. Supplemental Appendix H (available in the online version of the journal) summarizes PSM models with school FE added to the list of covariates included in the PSM models that Table 2 summarizes. The results in Supplemental Appendix H demonstrate that among all students, CBF benefits all college enrollment outcomes except for selective college enrollment. This contrasts with the PSM results in Table 2 (that did not have school FE) where CBF had a positive association with all college enrollment outcomes. It appears that unobserved school confounders explained away the association with enrolling in a selective college. For White students, the addition of school dummies to the PSM models did not alter the main conclusion—CBF has a positive association with applying to every type of college. For Asian students, it appears that unobserved school confounders explained away all the associations—leaving CBF without any statistically significant benefits on college enrollment outcomes. For Black students, the association for CBF remained intact for applying to any college, but unobserved school confounders explained away the association with applying to a 4-year college. However, Appendix H also demonstrates that CBF does positively benefit enrolling in a highly selective college once the PSM model adjusts for unobserved school confounders. Finally, for Latino students, the results do not change at all—CBF continues to have a positive benefit on enrolling in a 4 year college, net of unobserved school confounders. In general, the PSM results with additional adjustments for unobserved time-invariant school-level confounders changed the main conclusions very little. That is, White students continue to benefit the most from CBF while non-White students benefit much less. The results from PSM may yield similar results as traditional ordinary least squares (OLS), controlling for the same covariates (Bifulco, 2012; Cook et al., 2008; Guarino et al., 2014; Shadish et al., 2002; Shadish et al., 2008). Furthermore, often one may more easily interpret results from OLS models (Wooldridge, 2005). To test for the similarity between the PSM results in Table 2 and OLS, I run supplementary analyses using conventional OLS regression for models stratified by race and ethnicity (see Supplemental Appendix I, available in the online version of the journal) and models with interaction effects for the moderating effect of race and ethnicity on the association between CBF and college enrollment (see Supplemental Appendix J, available in the online version of the journal). The results from the OLS analyses largely align with the PSM results. One source of variation in the results is the fact that the PSM analysis restricts the sample to those on the region of common support and to those within the specified bandwidth. Theory and empirical research suggest that assets embedded in students' social contexts may yield positive educational benefits in the college-going process. This article demonstrates that having CBF is positively associated with enrolling in any college, 4-year colleges, selective colleges, and highly selective colleges. This article further demonstrates that race, ethnicity, and gender moderate these associations, provides estimates that are robust to unobserved confounding and alternative model specifications, and capitalizes on longitudinal and restricted transcript data. This article demonstrates that assets such as information and resources nested within friendships based on trust, and reciprocity can yield statistically significant positive benefits for college enrollment. The findings of a positive association between CBF and college enrollment outcomes align with theory on the influence of friends (Coleman, 1988; Giordano, 2003; Parsons, 1963) and demonstrate that students indeed reap benefits to being embedded in social networks that are based on close, intimate ties and are potentially rich in knowledge and resources about the college-going process. These results also align with those of Hallinan and Williams (1990) who found that friends were instrumental in the college-going process and with more recent research (Carbonaro & Workman, 2016; Cherng et al., 2013). The findings are also in line with recent findings on the role of peers on educational outcomes (Bifulco et al., 2011; Bifulco et al., 2014; Fletcher, 2011, 2012; Fletcher & Tienda, 2009). The findings for limited benefits for CBF among Black and Latino students align with previous research for various college-going outcomes (Alvarado & An, 2015; Alvarado & Turley, 2012; Langenkamp & Hoyt, 2017b; Perna et al., 2008). Related, previous research also finds that Latino and Black students are often nested in social networks that are low on information and resources regarding college-going (Perez & McDonough, 2008; Person & Rosenbaum, 2006). The findings suggest that structural and cultural forces may partially suppress the benefits of friends for Black and Latino students. For example, residence in neighborhoods where nonkin intimate ties are fleeting, or where those ties may even place pressure on students to participate in dangerous activities, may erode the benefits of friends, favoring family influences instead (Germán et al., 2009; Giordano, 2003; Portes & Zhou, 1993). Furthermore, stereotype threat and oppositional culture may affect the supply and demand, respectively, of students who embrace the educational system, thus limiting the influence of CBF for Black and Latino male students (Fordham & Ogbu, 1986; Steele & Aronson, 1995). Black and Latino students are also put in less advanced courses in schools and are therefore less likely to be around students who may be on a college pathway (Darity et al., 2001; Klopfenstein, 2004; Ndura et al., 2003). Furthermore, Black and Latino students are more likely to be disciplined, suspended, placed in special education, and expelled from school (Diamond & Lewis, 2015)—all of which suggest that strong structural forces impede their ability to use friends as assets in the college-going process. Cultural frames may also limit the benefits of friends for Black and Latino students. For example, familism, regardless of whether it has a positive (Germán et al., 2009) or negative (Turley et al., 2010) impact on educational outcomes for Black and Latino students, may also suppress the benefits of friends, even if those friends are college bound. That is, some students may be more likely to turn to family over friends or be more likely to weigh the advice they receive from families more heavily when it comes to decisions about college. To be clear, my results do not explicitly demonstrate that any of these mechanisms—neighborhoods, stereotype threat, oppositional culture, or familism—are explaining my results. Given the difficulty of school actors to change structural conditions that Black and Latino students are exposed to, college preparatory initiatives could treat the rich cultural frames (e.g., familism) that Black and Latino families bring to bear in the college-going process as assets. In addition to potentially increasing Black and Latino college enrollment by capitalizing on any given increase in the procollege influence of the family (Acevedo-Gil, 2017), such an approach could also increase the benefit of co-ethnic CBF; thereby enhancing the procollege orientation of the entire community of influence surrounding these students. The findings from the current study somewhat contrast with expectations that Latina females may benefit less from CBF due to familial influences (Desmond & Turley, 2009; Lopez-Turley, 2006). That is, I find that Latina females benefit more from CBF than Latino males do, which aligns with Riegle-Crumb et al.'s (2010) findings that Latinas have more academically oriented friends than Latinos do. Policies that aim to increase college enrollment for Black and Latino males may consider structural factors that may lead to patterns of academic underperformance (Fordham & Ogbu, 1986; Massey et al., 2011; Owens & Massey, 2011; Steele & Aronson, 1995). My findings also support Riegle-Crumb et al.'s (2010) findings regarding gender heterogeneity and contrast with Wells et al.'s (2013) findings. The findings for college selectivity, where neither Black nor Latino students demonstrate statistically significant benefits for CBFs, are particularly striking. Given the enhanced returns to selective college attainment, there is reason for concern regarding the limited ability of CBFs to increase enrollment in these institutions for Black and Latino students. This analysis is not without its limitations. For example, this study does not claim to find strictly causal effects of CBF on college enrollment outcomes. Another limitation is the fact that the HSLS does not identify specific characteristics of friends, limiting the ability to calculate predicted likelihoods that friends are college bound to address the issue of simultaneity. It is also important to consider that variation may exist in the ability of educational expectations to translate to educational gains by race and ethnicity and by gender. For example, despite increases in students' expectations to attain a college degree (Reynolds et al., 2006) and the decline of the association between social background (e.g., parents' education) and college expectations (Goyette, 2008), scholars have found that Black and especially Latino students have become increasingly less likely to realize their plans to attain a college degree than White students (Reynolds & Johnson, 2011). Meanwhile, females have been able to make gains on males in being able to translate their expectations to college degree attainment. Although not about expectations of friends, the results from these studies may be relevant in that Black and Latino students (to the extent that they have co-ethnic friends) may not benefit as much as Whites from CBF because their friends, even though college bound, may lack the resources to translate college educational expectations into college educational attainment equally as Whites. This finding of the differential success of college expectations to translate to college attainment by race and ethnicity from previous research is in line with my findings. Finally, given that neighborhoods affect educational and many other aspects of well-being across the life course (Alvarado, 2016a, 2016b, 2018, 2019, 2020; Sharkey & Faber, 2014), future research should examine how neighborhoods stratify access to (and the benefits students accrue from having) CBFs. Overall, the findings from this article suggest that college preparatory initiatives should recognize the nonlinearities in the relationship between CBFs and college enrollment by race, ethnicity, and gender. The benefits of CBFs for college enrollment are less for Black and Latino students than for White and Asian students, especially for males and especially for selective and highly selective colleges. College preparatory initiatives may be most successful at enrolling higher numbers of Black and Latino students if they recognize and capitalize on the wealth that is nested within diverse cultural frames and approaches to educational investments (Acevedo-Gil, 2017; Auerbach, 2004; Gamoran et al., 2012; Langenkamp & Hoyt, 2017a; Perna & Titus, 2005). It is important to continue to study the impact of friends on educational outcomes if scholars are to inform policy on how to best address educational inequalities. Initiatives aimed at integrating primary and secondary schools that are coupled with deep commitments to fostering interactions that lead to friendship formation may facilitate opportunities for Black and Latino students to benefit more from CBFs. Furthermore, if family is particularly important for some students, then school personnel should reconsider college access to include families in more meaningful ways. Friendships, perhaps when combined with a culturally sensitive approach to college-going, may be one essential piece of the puzzle that is necessary to ameliorate racial and ethnic disparities in college enrollment.
10.3102_0002831221997571	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831221997571	English Learner Labeling: How English Learner Classification in Kindergarten Shapes Teacher Perceptions of Student Skills and the Moderating Role of Bilingual Instructional Settings	 Prior research has shown that English learner (EL) classification is consequential for students; however, less is known about how EL classification affects student outcomes. In this study, we examine one hypothesized mechanism: teacher perceptions. Using a national data set (Early Childhood Longitudinal Study—Kindergarten Cohort of 2010–2011 or ECLS-K:2011), we use coarsened exact matching to estimate the effect of kindergarten EL status on teachers' perceptions of students' academic skills. We further explore whether that impact is moderated by instructional setting (bilingual vs. English immersion). We find evidence that EL classification results in lower teacher perceptions. This impact is, however, moderated by bilingual environments. In bilingual classrooms, we do not find evidence that EL classification results in diminished perceptions. This study adds to research on teacher perceptions and the effects of EL classification.	 With large achievement and attainment gaps between students classified as English learners (ELs) and those who are not, scholarly and practitioner attention has turned to consider the extent to which these gaps may, in part, be driven by the very services and treatments apportioned to EL students. Quasi-experimental studies examining the effects of kindergarten EL classification on later academic achievement have come to varied conclusions: Some show positive effects (Shin, 2018), while others show negative ones (Umansky, 2016). Likewise, studies measuring the effects of remaining an EL rather than exiting EL status have demonstrated a range of effects on later achievement, course placement, behavioral outcomes, graduation, and postsecondary enrollment. These include neutral effects (Reyes & Hwang, 2019; Robinson, 2011), mixed effects (Cimpian et al., 2017; Robinson-Cimpian & Thompson, 2016), and negative effects (Carlson & Knowles, 2016; Johnson, 2019). Such studies illustrate that although educational ramifications may be varied, EL classification has tangible effects on students' experiences and opportunities in school, and as such, is consequential for students in both the short and the long term. In order to maximize the beneficial effects of EL classification and minimize harmful ones, it is necessary to understand the mechanisms that drive the educational effects of EL classification. Mechanisms associated with EL classification that may result in positive educational outcomes include access to instruction toward English language development (Baker et al., 2014), content instruction in students' home languages (Steele et al., 2017), and specially trained teachers (Master et al., 2016). Mechanisms associated with EL classification that may lead to damaging educational outcomes include linguistic isolation (Gifford & Valdés, 2006), tracking into low-level classes (Estrada, 2014; Kanno & Kangas, 2014), and placement into classes with less experienced teachers (Gándara et al., 2003). Another, albeit infrequently examined, potential mechanism of negative EL classification effects relates to teacher perceptions of student ability and their expectations for students' future outcomes. Drawing on labeling theory (Link & Phelan, 2013), scholars have highlighted how "English learner" is a deficit-oriented classification—it identifies students by their lack of English proficiency (Gutiérrez & Orellana, 2006; Wiley & Lukes, 1996)—which may trigger treatments that harm rather than benefit students (Flores et al., 2015; Martínez, 2018). Research has identified how some teachers hold downwardly biased academic perceptions of their EL students and interpret students' lack of English proficiency as a lack of academic skill or potential (Blanchard & Muller, 2015; E. B. Garcia et al., 2019; García & Guerra, 2004; Katz, 1999; Olsen, 1997; Pettit, 2011; Valenzuela, 1999). The large body of teacher perception and expectancy research from the past 50 years (see Jussim & Harber, 2005) indicates that downwardly biased perceptions and/or expectations could negatively affect EL student outcomes. However, differences in teachers' perceptions of EL and non-EL students are not necessarily the result of EL classification. Teachers may have lower academic perceptions of their EL students that accurately reflect differences in the average skill levels of their EL, compared with their non-EL, students. As such, while teacher perceptions could drive differences in student outcomes between EL and non-EL students, it could also be the case that real differences in student skill levels drive observed differences in teacher perceptions. While other minoritized and/or stigmatized groups have been studied in great detail (e.g., Ferguson, 2003; Rubie-Davies, 2010) teacher perceptions and expectations of EL students have received comparatively little attention as far as large-scale and quantitative research is concerned (for two exceptions, see Blanchard & Muller, 2015, and E. B. Garcia et al., 2019). This study addresses that gap by drawing on the Early Childhood Longitudinal Study—Kindergarten Cohort of 2010–2011 (ECLS-K:2011), a nationally representative data set that asks teachers a series of questions about their perceptions of individual student skill levels across a range of academic content areas and grades. In this study we were able to take advantage of a unique policy characteristic that creates what we will argue is a natural experiment to test the hypothesis that EL classification affects teachers' perceptions of their students. Specifically, states and districts not only use a range of different assessments to measure English proficiency, they also set and implement different English proficiency thresholds for EL classification. As a result, in some locales, students with a given true English proficiency level are classified as ELs while, in other locales, students with the same true English proficiency level are not classified as ELs. There is, therefore, a set of students who fall into a band of English proficiency levels who are, in effect, randomly assigned to EL or non-EL status based on their district or state of enrollment. Because the ECLS-K:2011 data include information about both EL identification and a universally administered measure of English proficiency, we are able to identify this group of students where EL classification is as good as random. Using coarsened exact matching analysis to match students with the same English proficiency levels (and other characteristics) but different language classifications, we then estimated the causal effects of EL classification in kindergarten on teachers' perceptions of student academic skill levels. In addition, we examined a factor that may moderate the impact of EL classification on teacher perceptions. By law, EL-classified students must be afforded both instruction in the English language and accessible grade-appropriate core content instruction (Lau v. Nichols, 1974). However, schools and districts have enormous flexibility in how they structure services for EL students. Most are served in English instructional programs, that is, programs where instruction, be it science, math, or other content, is provided in English. A much smaller proportion of EL students are served in whole, or in part, in bilingual programs, where content instruction is provided in students' home languages. It is plausible that the type of instructional program a teacher works in moderates the impact of EL classification on teacher perceptions. Specifically, a large body of research has found that bilingual instruction is beneficial for EL students (National Academies of Sciences, Engineering and Medicine [NASEM], 2017; Steele et al., 2017). While relevant theory posits that this effect is likely due to the increased comprehension and accessibility of content, it also suggests this beneficial effect may be due to an asset orientation in which bilingual classroom teachers hold more positive beliefs about their EL students (Baker, 2011; Ruiz, 1984). As such, we tested whether teacher perceptions of EL students' academic skill levels differed depending on whether the teacher and student were in a bilingual or an English instructional classroom. Scholarship addressing the impact and importance of teacher perceptions on student outcomes and experiences has a long and rich history. Beginning with a seminal work that catalyzed teacher perception and expectancy research (Rosenthal & Jacobson, 1968), hundreds of correlational and experimental studies, reviews, and meta-analyses have looked at factors that influence teachers' perceptions of their students, and how teachers' perceptions affect student outcomes such as test scores or measures of intelligence (Dusek & Joseph, 1983; Hinnant et al., 2009; Jussim et al., 1996; Jussim & Harber, 2005Sorhagen, 2013; F. A. López, 2017). These effects of teacher perceptions on student outcomes have been explained via mechanisms including grade retention (Burkam et al., 2007), track placement (Oakes, 2005), within-class ability grouping (Tach & Farkas, 2006), and instructional quality and characteristics (Page, 1987). Importantly, teacher perceptions and expectations have been found to be systematically lower for minoritized and/or stigmatized groups of students, including African American, Latinx, and low-income students (Auwarter, & Aruguete, 2008; Ferguson, 2003; Meissel et al., 2017; McKown & Weinstein, 2008; Ready & Wright, 2011; Rubie-Davies, 2010; Tenenbaum & Ruck, 2007). A core question has been whether and to what extent teachers' differential perceptions reflect real differences in skill level versus being the result of stereotypes or bias. Taken together, the results of various studies examining this question show that teachers' perceptions of students' skills tend to be relatively accurate (Jussim et al., 1996; Jussim & Harber, 2005; Llosa, 2008; Madon et al., 1998; Meisels et al., 2001; Ready & Wright, 2011) but that teachers' accuracy is lower (and bias is higher) when they do not share their students' background characteristics (Farkas, 2003) and when students come from more highly stigmatized groups (Downey & Pribesh, 2004; McKown & Weinstein, 2008; Ready & Wright, 2011; Tach & Farkas, 2006). Official labels or classifications assigned by the schooling system have been shown to affect teacher perceptions. In particular, research has shown that special education labels negatively affect teachers' expectations of students (Bianco, 2005). This problem of biased and inaccurate expectations and perceptions of stigmatized, minoritized, and/or labeled groups is compounded by the fact that these same groups of students have been found to be more vulnerable to the effects of negative teacher expectancy (Ferguson, 2003; Jussim et al., 1996; Jussim & Harber, 2005; Van den Bergh et al., 2010). The research on teacher perceptions of EL-classified students is nascent (Pettit, 2011, provides a review of this literature). Findings suggest that teachers have different perception and expectation patterns depending on the specific population of interest such as immigrant students, students who speak a language other than English at home, or EL-classified students. Findings also differ with regard to type of perception, such as perceptions of appropriate curricula or instructional methods, or students' personal attributes, academic knowledge, or future prospects. For example, using a nationally representative sample, Blanchard and Muller (2015) found that teachers were more likely to perceive immigrant students as hard working compared to nonimmigrant students whose home language is not English. At the same time, teachers tended to believe that these nonimmigrant students were less likely to complete college than students speaking English at home, a finding that is also reflected in qualitative research (Dabach et al., 2018). With regard to scholastic outcomes, research has found that teachers have relatively accurate assessments of EL students' English proficiency levels (Llosa, 2008) but that they underestimate multilingual students' academic skills (Ready & Wright; 2011), with underestimation varying by grade level and student ethnicity. Perceptions of EL-classified students, the vast majority of whom are Latinx or Asian, are tied to student characteristics (Llosa, 2008), including race and ethnicity, with research demonstrating that teachers often hold stereotypes of Asian students as "model minorities" while holding stereotypes of Latinx students as "underachieving" (Lee & Zhou, 2015, N. López, 2003; Ochoa, 2013). In addition to research pointing to teachers having lower academic perceptions of their multilingual students, there is evidence that these perceptions are linked to both teachers' instructional choices and student outcomes. Murphy and Torff (2019) found that teachers believed that rigorous instructional methods involving critical thinking skills were less appropriate and beneficial for EL compared to non-EL students, while F. A. López (2017) found that teachers' expectations and beliefs about Latinx students were associated with their instructional practices. Research that has looked specifically at EL-classified students is sparse but indicates that, on average, teachers have comparatively low perceptions of EL-classified students (E. B. Garcia et al., 2019; García & Guerra, 2004; Katz, 1999; Pettit, 2011; Valenzuela, 1999; Walker et al., 2004). This important research, largely ethnographic and qualitative, has rarely, however, accounted for measures of student skill level and therefore is not able to analyze the effects of EL classification on teacher perceptions. An exception is E. B. Garcia (2019), who found that teachers hold downwardly biased perceptions of EL students' executive function skills, accounting for direct measures of those skills. There is limited research that suggests that teachers' perceptions of EL students may vary by academic domain. Specifically, teachers may have more positive views of their EL students' math skills compared to other academic domains, due to a belief that math skills rely little on language proficiency (Hansen-Thomas & Cavagnetto, 2010; Whiteford, 2009). In a study by Hansen-Thomas and Cavagnetto (2010), for example, 70% of surveyed teachers reported a belief that math was EL students' easiest subject, and about a quarter of teachers explicitly stated that math knowledge was "universal," transcending language. By contrast, teachers' perceptions of the academic skills of EL students may be more negative in the area of language arts. Several studies have documented how students' use of their home language and their use of code-switching practices are inaccurately interpreted by teachers as weaknesses in language arts and literacy skills (Escamilla, 2006; Salerno et al., 2019). The context in which teachers and students find themselves is associated with both the degree of bias or accuracy in teacher perceptions and expectations and the degree to which these factors influence students' outcomes. For example, teachers in classrooms serving lower socioeconomic students and those in classes with lower average achievement are more likely to underestimate students' skills (Ready & Wright, 2011). In addition, younger students, students in settings with more differentiated services, and students in moments of transition are more vulnerable to teacher perception effects (Jussim & Harber, 2005). Research has also suggested that racial congruence moderates teacher perception effects (Fox, 2015; Oates, 2003). Just as the broader literature has found that context matters for teachers' perceptions, context also likely matters in teachers' perceptions of EL students. Several studies have shown that teacher perceptions of EL students vary according to school characteristics such as grade span (Gallo et al., 2014) and teacher characteristics, including how teachers understand their role, teachers' education level, their training and level of experience working with EL students (Byrnes et al., 1997; Dabach, 2011; Pettit, 2011; Walker et al., 2004; Yoon, 2008; Youngs & Youngs Jr., 2001). Yoon (2008), for example, found stronger EL student-teacher relationships in classrooms where teachers considered themselves teachers of all students rather than of mainstream students only or of a given subject area. Research has not examined how teacher perceptions and expectations may differ according to linguistic instructional environment and specifically depending on whether the classroom environment is bilingual versus exclusively English. Yet a robust body of work has identified beneficial effects of bilingual education (August & Shanahan, 2006; NASEM, 2017; Steele et al., 2017), and many have theorized that at least part of this benefit may derive from a more asset-oriented environment in bilingual classrooms, which values students' linguistic, familial, and cultural backgrounds as educational resources (Baker, 2011; Matthews & López, 2019; Ruiz, 1984). Teachers' asset orientation in bilingual settings, in turn, has been attributed to teacher education and preparation to successfully work with multilingual students; teachers' critical awareness regarding educational equity and opportunity; and student-teacher congruence, familiarity, and closeness (Fránquiz et al., 2011; Hopkins, 2013; F. A. López, 2017). These findings on the beneficial effects of bilingual education, combined with the larger research indicating that teachers' perceptions are moderated by school and classroom context, suggest that the effects of EL status on teacher perceptions may be systematically different in bilingual versus monolingual English instructional environments. This study sought to fill two important gaps in the literature: First, it estimated the causal effect of kindergarten EL classification on teacher perceptions across the early elementary grades. Second, it looked at whether teachers' perceptions of students classified as ELs in kindergarten differed systematically in bilingual versus English instructional environments. We posit that kindergarten EL status could affect teacher perceptions in two distinct ways. First, teachers may hold downwardly biased perceptions of their students' abilities as a direct result of the EL label (E. B. Garcia et al., 2019). We refer to this as a direct effect of EL classification on teacher perceptions. Second, EL status may result in diminished instructional access or opportunity to learn in ways that negatively affect student academic outcomes (Callahan, 2005; Johnson, 2019). EL students, for example, might be placed in lower level instructional groups, or might be called on less frequently than their peers (Harklau, 1999). In this case, EL status could affect teacher perceptions of academic skills indirectly, through real changes in student skill level. We refer to this as an indirect effect of EL classification on teacher perceptions. Of note, while direct effects indicate teacher bias, indirect effects do not indicate bias, per se, because teachers' lower perceptions accurately reflect students' skills. Both mechanisms may occur in tandem, and may, in fact, be intertwined. For example, if teachers have biased perceptions of EL students, and as a result alter their instruction to those students in ways that diminish student learning, then subsequent measures of teacher perceptions might reflect a combination of direct and indirect effects. This article opened by theorizing that EL status might affect student outcomes via teacher perceptions. Of note, if we find evidence that EL status affects teacher perceptions exclusively through student outcomes (i.e., an indirect effect) this would suggest that teacher perceptions are not the cause of lower student outcomes, but instead are a result of lower student outcomes. Meanwhile if we find evidence of direct effects, or a combination of direct and indirect effects, that would support the hypothesis of teacher perceptions as a potential mechanism for EL status effects on student outcomes. Our research questions were as follows: Research Question 1: Does kindergarten EL status affect teachers' perceptions of students' academic skills among multilingual students across the early elementary grades? If so, what descriptive evidence do we have as to the mechanisms of that effect (i.e., direct vs. indirect)? Research Question 2: Do bilingual classrooms operate as a moderator of the impact of kindergarten EL classification on teachers' perceptions of students' academic skills? This study drew on the ECLS-K:2011 data set, a federally collected, nationally representative sample of students who entered kindergarten in the 2010–2011 school year. The data set contains longitudinal information on this cohort of students through the fifth grade. For the purposes of this study, we included data from kindergarten, first grade, and second grade. We stopped at the second grade due to relatively high attrition of students from the EL category by the third grade (Saunders & Marcelletti, 2012). Our sample of interest included students who spoke a language other than English at home based on either or both teacher and parent reports in kindergarten (Garrett & Hong, 2016). Throughout this article we will refer to these students as multilingual students. While many of these students were in the process of developing English (and others may have also been in the process of developing their home language), we call them multilingual because they were operating in, and developing, more than one language. We further limited the sample to those multilingual students who attended public schools where identification of EL students is mandated. This subsample of ECLS-K:2011 included 3,885 students. We omitted students who were missing one or more kindergarten variables of interest, leaving an analytic sample of 2,155 students (Pepinsky, 2018). Descriptive statistics of the analytic sample are shown in Table 1. There were large differences between those in the sample of multilingual students who were classified as EL in kindergarten and those who were not. Non-EL multilingual students had, on average, higher kindergarten English proficiency levels, higher family socioeconomic status, and were less likely to be in bilingual classrooms. Most relevant to this study, non-EL students had considerably higher academic skill levels in kindergarten. Therefore, it is not immediately evident whether differences in teachers' perceptions of students' skills (see Table 1) reflected real skill differences across the two groups, or if they were, by contrast, caused in part by EL classification. Because of these differences in the baseline measures of EL and non-EL multilingual students, it was critical to identify a counterfactual group of kindergarten non-EL students with similar characteristics to the EL students, as we did in the present study. The ECLS-K:2011 data collection included a host of questions in which teachers recorded their perceptions of students' academic skills over time. We used these teacher perception variables from the spring of kindergarten, after teachers had been working with their students for approximately one academic year, and then again at the end of first grade and the end of second grade. We draw on teachers' perceptions of students' skills in the areas of (1) language and literacy, (2) math, (3) social studies, and (4) science. Teachers were instructed to answer these questions based on their perception of student skill, independent of language: "Please answer the questions based on your knowledge of this child's skills. If the child does not yet demonstrate skills in English but does demonstrate them in his/her native language, please answer the questions with the child's native language in mind" (National Center for Education Statistics [NCES], n.d.). For each grade, we also constructed a composite measure by averaging all four academic domains because a principal component analysis indicated that there was one latent construct underlying a given teacher's assessment of a student across the four academic domains, with similar weights across the domains. Teacher perception measures differed by grade level. In kindergarten, teacher perceptions for math and language/literacy were measured via multiple items (e.g. "This child uses complex sentence structures"), each of which were answered on a 5-point Likert-type scale ranging from 1, which represented not yet proficient to 5, proficient. We created an overall score for each domain by taking the average of all the questions in that domain. Reliabilities of the average scores for both domains were high (math: eight items, α = .89; language/literacy: nine items, α = .94). Teacher perceptions in science and social studies were measured via a single question in which teachers were asked: "Overall, how would you rate this child's academic skills in each of the following areas, compared to other children of the same grade level?"; the 5-point Likert-type scale for this item ranged from 1, for far below average, to 5, far above average. For our across-domain kindergarten composite score, we averaged teachers' perceptions across the four domains of math, language/literacy, social studies, and science (α = .94). In first grade, math, language/literacy, and science were measured via multiple items, which were each answered on the same 5-point Likert-type scale as the multiple items in kindergarten. Again, we created an average score for each domain and an overall composite across the four domains. Reliabilities for the overall composite (α = .98), and for each domain were high (math: eight items, α = .96; language/literacy, nine items, α = .97; science: eight items, α = .97). Teacher perceptions in social studies were measured on a 5-point Likert-type scale via a single question, as in kindergarten. In second grade, teacher perceptions were assessed via one question for math, science, and social studies and three questions for language/literacy, each of which were answered on a 3-point Likert-type scale, which ranged from 1, for below grade level, to 3, for above grade level. For language/literacy, we took the average score of the three questions (α = .87); for the overall composite, we again took the average score across the four domains (α = .88). Because the scale for the second-grade perception variables was different from the kindergarten and first grade scales, we standardized all outcome variables in kindergarten, first, and second grade. This allowed us to compare effect sizes across grade levels. It also facilitated effect size interpretation by translating unique scales into standard measures of effect size. We standardized all outcome variables using their mean and standard deviation within the full ECLS-K:2011 data set. The primary predictor variable of interest was EL status. EL status was derived from a single question posed to teachers in a questionnaire in the spring of kindergarten. Teachers were asked about each multilingual student: "Does this child participate in an instructional program designed to teach English language skills to children with limited English proficiency?" While the question did not ask directly about whether a student was classified as an EL in school, it did ask whether the student was in an EL program. Thus, this measure may not have been a completely accurate measure of EL status, as some EL-classified students may not have been, in practice, receiving EL services. However, prior data suggest that the vast majority of EL-classified students are in some form of EL program (D.J. et al. v. State of California, 2015). In total, 1,214 out of 2,155 multilingual students (56%) were considered EL. The remaining 941 students were multilingual students who were not receiving EL services at school. For most of these students, this was presumably because their kindergarten English proficiency scores on local assessments surpassed established EL thresholds and they were therefore not eligible for EL services. In other cases, schools may have been failing to provide EL services to eligible students, parents may have opted out of EL supports, or teachers may not have understood or correctly answered the question. We argue that teacher report for this variable may actually benefit this study. This is because our research questions surround teachers' responses to students' kindergarten language classification and, as such, the sample should only include those students whose language classification was known by their teachers. We used the kindergarten measure of EL status because, as described above, we are interested in the effects of EL status over time. While students take an average of 5 to 7 years to reach English proficiency (NASEM, 2017), some of the EL-classified students in our sample exited EL status by the time they reached the first or second grade. As such, our estimates in first and second grade should be interpreted as the effects of kindergarten EL classification on later grade teacher perceptions, where some of the treatment group retained their classification and others lost it. Specifically, Table 1 shows that 66% of the treatment group retained their EL status in first grade, and 57% retained their status in second grade. Table 1 also illustrates that a nonnegligible proportion of multilingual students who were not reported as being in an EL program in kindergarten were reported to be in an EL program in first (21%) and/or second (16%) grade. While this runs counter to federal education policy (which only allows for students to be classified as ELs when they first enter a school district), it may reflect later classifications, students that moved between districts (a given student would only remain in the ECLS-K data set if they happened to move to another school sampled in ECLS-K), or data errors. Because the end result was that some proportion of the control group (non-EL students in kindergarten) likely did, indeed, receive EL classification in later grades, this likely biased our estimates downward in those later grades. While the shifting nature of EL status is a limitation of our study, we conducted a sensitivity check that accounted for later EL status. Our primary matching variables were two variables that measured oral English proficiency level and English reading skill level in the fall of kindergarten. As described below, school districts make determinations about EL status by assessing individual multilingual students' English proficiency levels using state or local assessments. In the ECLS-K:2011 data set, all students, including all multilingual students, were administered two measures of English proficiency: The Preschool Language Assessment Scale (PreLAS) and the English Basic Reading Skill (EBRS) Assessment. Taken together, they served as a baseline measure of students' incoming English proficiency. The first assessment, the PreLAS (α = .91), was used as a screener to assess each student's oral (speaking and listening) English proficiency and determine whether they should be given the rest of the ECLS-K:2011 assessments in English. The PreLAS consisted of 20 questions that assessed expressive vocabulary in English from picture prompts and whether students could follow simple instructions in English. Students who scored equal to or more than 16 were considered English proficient and given the rest of the battery of direct assessments in English (including assessments in reading, math, science, and executive functioning). Students who scored less than 16 took the EBRS but no other direct assessments in English. Spanish speaking students who did not meet the PreLAS threshold were administered baseline assessments in Spanish. The PreLAS distribution is skewed to the right. Among multilingual students, 17% scored the full 20 points, and the mean score was 14.9. The second assessment was the EBRS (α = .87). It consisted of 18 literacy questions in English covering topics including print familiarity, letter recognition, rhyming words, and word recognition. Two questions from the PreLAS were added to the EBRS final score, for a total possible score of 20 (Tourangeau et al., 2015). The EBRS was relatively normally distributed, with an overall mean score among the analytic sample of 11.3 (1.7% of the sample scored the full 20 points). Additional matching variables included student race/ethnicity, gender, and socioeconomic status along with an indicator variable for whether the student's district was in a rural setting. As we describe below, we found that once we matched students on these variables, there were no meaningful or significant differences in students' skill levels, as directly measured through ECLS-K: 2011 assessments. The one exception was that in some models there were small but significant differences between EL and non-EL groups on one of the two executive functioning assessments (an oral number reversing activity). As such we included that measure as a matching variable. In addition to these primary matching variables, we also included a host of other student, teacher, class, and school covariates as control variables in our regression model. In our main model, all control variables are from students' kindergarten year. Regarding student-level covariates, we included scores on kindergarten reading (reliability = .95) and math (reliability = .92) assessments (both item response theory–based theta scores), two kindergarten executive functioning assessments, age, special education status, whether the student repeated kindergarten, whether the student was chronically absent, and whether the student changed teachers midyear. For classroom and teacher-level variables, we included: whether the class was full or half day, the teacher's number of years of teaching experience, whether the teacher held a master's degree, and whether the teacher held a degree in an education-related field (Wayne & Youngs, 2003). We also controlled for class size and racial composition, proportion of EL students in the class, the proportion of the class the teacher considered to be lower-skill readers, and whether the teacher considered the class to be poorly behaved. For school-level variables, we included school size, average socioeconomic status, and the proportion of Black and Latinx students. As described below, our analyses that explore the mechanisms by which EL status might affect teacher perceptions added students' later academic achievement variables (English proficiency, reading, math, and executive functioning) to our models. Our second research question explored the moderating variable of bilingual program enrollment. To identify students in bilingual programs, we drew on questions asked of teachers in the spring of kindergarten. Specifically, teachers were asked the following question with regard to academic instruction in reading/literacy and math: "How often is a non-English language used by teachers, aides, or other adults?" There were five options given, ranging from 1 = never, to 5 = all the time. Using these questions, we created a dichotomous variable indicating that the teacher or another adult in the classroom used a language other than English in math or in reading/literacy for "about half the time" or more. We used this definition because a bilingual instructional model should devote a considerable amount of instructional time in core content areas to instruction in the home language (Baker, 2011). Using this definition, we identified that 14% of multilingual students were participating in a bilingual program in kindergarten (see Table 1). Because we focused our analyses on the effects of kindergarten EL status over time, we used an indicator of bilingual instruction from the kindergarten year. Most students who were in bilingual settings in kindergarten remained in bilingual settings in the first (63%) and second (55%) grades. Almost no students not in bilingual settings in kindergarten moved into bilingual settings in the first or second grades (<2%). There were very few students (N = 20) in the sample who were in a bilingual program and were not considered ELs as defined in this study. We discuss the methodological implications of this last point below. Federal law requires that all public schools identify incoming students with a home or primary language other than English (i.e., multilingual students, as defined in this study). Schools must then assess these students' English proficiency levels in order to determine whether students qualify for EL status (Every Student Succeeds Act [ESSA], 2015). By law, EL status identification procedures must be determined exclusively based on these two things: multilingual status and English proficiency level. However, states—and prior to implementation of ESSA (2015), districts—are able to set their own thresholds on the English proficiency measures they use to determine EL status. Moreover, different states use different English proficiency assessments. In the school year just prior to ECLS-K:2011 kindergarten data collection, a study found 25 separate English proficiency assessments used across U.S. states (National Research Council, 2011). Comparing 8 of those 25 tests, the study identified major differences between them, including different English proficiency standards, test item types, lengths, and content. They concluded that "we cannot simply assume that a student who scores at the intermediate or proficient level on one state's ELP [English language proficiency] test will score at the intermediate or proficient level on another" (National Research Council, 2011, p. 74). This context creates a natural experiment (Murnane & Willett, 2010) that we exploit in this study. Because of the variation in tests and thresholds, a student with a given true (unobserved) English proficiency level might be classified as an EL in one school in the ECLS-K:2011 sample, while another child with the exact same true English proficiency level may not be classified as an EL. A substantial body of research has confirmed these conclusions (Abedi, 2004, 2008; Linquanti & Cook, 2015; Lopez et al., 2016; Mavrogordato & White, 2017; Ragan & Lesaux, 2006; Sireci & Faulkner-Bond, 2015; Solórzano, 2008). This variation in EL classification rules and implementation amounts to exogenous variation in student classification assignment, once accounting for student English proficiency level. It is plausible to expect students with very high true English proficiency levels to score high on numerous assessments, exceed EL test thresholds, and therefore have a relatively low likelihood of being classified as an EL across different locales. Similarly, students with very low true English proficiency levels might score below the EL threshold across multiple assessments and have a high likelihood of being classified as EL across locales. However, for students with true English proficiency levels in the middle, one would expect significant variation across locales in EL or non-EL identification due to the variation across assessments and thresholds. In order to exploit this natural experiment, we needed a universally administered English proficiency assessment separate from those administered and used by schools to determine EL classification. ECLS-K:2011 provides just such an assessment (the PreLAS and EBRS). Our empirical strategy therefore homed in on a region of common support where EL and non-EL students had the same measured English proficiency levels (and were similar regarding other characteristics). Specifically, we examined whether teacher perceptions of student ability were different for students classified in kindergarten as ELs compared with students who had the same measured English proficiency level (and other characteristics) but were not classified as ELs. Figure 1 shows our region of common support (for ease of interpretation, we standardized, centered, and then averaged each student's PreLAS and EBRS scores). As a matching analysis, we can interpret our estimates causally only if, conditional on observed matching and control variables, classification as an EL (treatment assignment) is exogenous, or as good as random. If, for example, EL classification is assigned, in part, on teachers' or administrators' sense of student academic need (i.e., an omitted variable), then differences in teacher perception of student skill (our outcome) could be due to systematic differences in student academic need rather than EL classification assignment. We argue that the circumstances of this study are a near ideal use of matching and that, as such, our estimates of the effect of EL classification on teacher perception (Research Question 1) can be interpreted causally. First, as stated above, by law, kindergarten EL classification must be assigned only based on (1) multilingual status and (2) measured English proficiency level, both of which we can account for in our models. Second, prior research has demonstrated very high compliance with EL classification law. For example, Umansky (2016) found 89% compliance in kindergarten EL classification in one large school district, while Shin (2018) reported nearly universal compliance in a different district. These first two points are important because prior research has shown that causal estimates using observational data (and methods such as matching) align with those from experiments in cases where the selection process into the intervention is known and can be effectively modeled or proxied (T. D. Cook et al., 2008). Finally, once we account for multilingual status and measured English proficiency (along with key demographic characteristics described above) there are no remaining observable differences in the measured academic skill levels of EL and non-EL students (see Table 2; also described further below). This provides evidence that our matching and control variables fully account for treatment assignment and any remaining variation is as good as random. Coarsened exact matching (CEM), like all matching strategies, matches individuals in the treatment group (multilingual kindergartners classified as ELs) with students who are similar to them but who are in the control group (multilingual kindergartners not classified as ELs). It then examines the differences in outcomes between the matched sample of treated and control individuals. The purpose of matching is to reduce observed variable bias by removing from the sample and subsequent estimation any individuals who cannot be matched with individuals in the alternate group. This limited our analyses to the area of common support in which there were students with the same observed characteristics that fell into both the EL and non-EL categories (Murnane & Willett, 2010). Conducting this matching enabled us to achieve a better balance between the treatment and control groups (Iacus et al., 2012), thereby reducing observed variable bias (Murnane & Willett, 2010). Compared with other matching strategies, such as propensity score matching, CEM is a useful matching strategy because the matching algorithm is directly determined by the researcher and therefore can be theory and research based. In addition, results of matching, including the quality of matches and the sample size, can be evaluated prior to moving on to statistical estimators of research questions (Iacus et al., 2012). Specifically, CEM allows researchers to dictate the features of the matching algorithm in substantively meaningful ways both with regard to which variables to include in the matching process and with regard to the rules for how close the matches should be for each variable. In addition, researchers evaluate the quality of the matched sample before attempting to answer research questions. Specifically, in CEM, variables that are considered to predict the likelihood of being in the treatment group and that are correlated with the outcomes of interest are selected for matching. For each variable, one can require either exact matches or one can coarsen the variable into a selected number of bins and match within each bin. CEM then assigns weights based on how many matches there are per individual, and these weights are used in subsequent analytic models. In this study, we matched on kindergarten English proficiency level, executive function skill level, gender, race/ethnicity, rural locale, and socioeconomic status. All analyses were conducted using Stata version 15. In the matching algorithm race, gender and rural locale were set to be exact matches while we binned the continuous variables: English proficiency level, executive functioning skill, and socioeconomic status. Following Rosenbaum and Rubin (1984), we binned each of the continuous variables into quintiles (based on the sample distribution); matching by quintiles has been shown to eliminate more than 90% of bias. As described, we were then able to evaluate the quality of our matched sample. Table 2 shows the balance between the prematched sample and the postmatched sample on matching and other key variables. The region of common support covered 59% of the analytic sample (N = 1,262). The selected algorithm achieved a good balance between the treatment and control groups such that there were only very small, statistically insignificant, differences between the groups on all key matching and control variables, including kindergarten assessments in reading and math (see Table 2). The characteristics of the matched sample, which reflect the region of common support, were different from the full sample of multilingual students. This may be because ECLS-K:2011 oversamples specific subgroups such as Asian and Pacific Islander students (Tourangeau et al., 2015). The matched sample had a higher proportion of Latinx students, a smaller proportion of female students, was less likely to be in a rural location, had lower baseline reading and math skills, and had a lower average family socioeconomic level, compared with the full multilingual student sample. Compared with the full sample, the matched analytic sample more closely aligned with characteristics of the EL population in the United States (NCES, 2018). We then analyzed our matched data in a regression framework. This is considered a "doubly robust" model, in that we matched on key covariates and then performed a regression analysis with those and additional covariates to control for any remaining observed variation between the two groups. Research Question 1 asks about the impact of kindergarten EL status on teachers' academic perceptions of their students across grades. To answer this, we used the following model:[MATH](1) where PERCEP represents the set of teacher academic perception outcomes in grades kindergarten through second grade for student i, EL is our proxy for EL status in kindergarten, PreLAS and EBRS are our baseline measures of English proficiency, ACHIEVE is our set of baseline academic skill measures, and X is our wide array of additional student, family, teacher, class, and school covariates. Standard errors were clustered at the school level to account for students within schools. To account for the complex sampling design used for ECLS-K:2011 data collection, as well as our matching results, we followed DuGoff et al. (2014), creating a new weight for each observation equivalent to the product of the CEM weights and the ECLS-K:2011 sampling weights. The coefficient of interest is [MATH], which represents the estimated effect of kindergarten EL status on teacher academic perceptions, among multilingual students, holding constant students' English proficiency level, achievement levels, and a host of other characteristics. After conducting our main analyses, we sought to descriptively explore the mechanisms through which EL status affects teacher perceptions. As described earlier, we argue these effects could be (1) direct effects of EL classification in the form of teacher bias toward students carrying the EL label, (2) indirect effects of EL classification through the mechanism of altered educational experiences resulting in altered educational outcomes (that teachers then accurately perceive), or (3) some combination of both. In order to descriptively test these mechanisms, we ran analyses that added measures of students' later achievement to Equation 1. Our rationale was that direct effects of EL classification would remain once controlling for later student achievement. Indirect effects of EL classification, via effects of classification on student achievement, would not be picked up in a model that controlled for later achievement. Specifically, our first-grade models included spring of kindergarten achievement measures, and our second-grade models included spring of first-grade achievement measures. Research Question 2 asks about the role of bilingual education in moderating the effect of EL status on teacher perceptions. To answer this question, we used the following model:[MATH](2) where all variables are defined as in Equation 1. We removed the kindergarten EL variable from Equation 1 and replaced it with two variables, one indicating whether the kindergartner was an EL and in a bilingual class (EL_BIL) and one indicating whether the kindergartner was an EL and not in a bilingual class (EL_NOTBIL). As mentioned above, there were only 20 non-EL kindergartners in bilingual classrooms in the sample, which meant we could not include an interaction term of EL and BIL. Instead, this model allowed us to estimate teacher perceptions for three groups of students: multilingual non-EL students (the reference category), multilingual EL students in bilingual classes, and multilingual EL students not in bilingual classes. The coefficients of interest in this model are [MATH] and [MATH], which represent the estimated difference in teacher perceptions for kindergarten ELs in bilingual classes and not in bilingual classes, respectively, compared with teacher perceptions of non-EL multilingual students. We then ran contrast tests to test the differences between the three groups of students. Of specific interest, we report results of contrast tests which test whether the relationship of EL status to teacher perceptions differed for EL kindergartners in and not in bilingual classes. Importantly, results from these analyses are not causal estimates. Prior research suggests that students are not randomly distributed across bilingual and English only settings. Instead, bilingual program enrollment is associated with characteristics such as parental value for biliteracy and multiculturalism (Parkes, 2008). We cannot fully account for such differences, nor differences between the characteristics of bilingual and English only instructional settings. As such, in contrast to Research Question 1, the results for Research Question 2 are correlational. We conducted an array of sensitivity checks. First, we conducted our regression analyses without any matching (Sensitivity Check 1). Ordinary least squares analysis, in the absence of matching, does not provide causal estimates. Instead, we included it as a first step and as a point of comparison to our matching results. The remainder of our sensitivity analyses all included matching. In Sensitivity Check 2, we used an alternate matching method, propensity score matching, instead of CEM. We included all of the matching variables used in the main model, plus reading and math assessment scores, and the second executive functioning assessment score. Propensity score matching allowed us to keep the full analytic sample, but the resulting matched sample was not as compelling because the EL treatment group had slightly but significantly lower English proficiency levels than the non-EL control group. The remaining sensitivity checks all used CEM. Sensitivity Check 3 used the same matching variables as in the main model but added in the three additional direct assessments (reading, math, and the second executive functioning assessment). While scores on these three assessments were balanced across treatment and control groups without their inclusion, direct measures of academic skill are theoretically and empirically critical predictors of teacher perceptions of student academic skills and therefore merited inclusion as matching variables in a sensitivity check. The models used the same control variables. The resulting sample was somewhat smaller than the main model (48% of the analytic sample). As described, our sample of multilingual students was made up of students identified by either their teacher or their parent as having a home language other than English. For the fourth sensitivity check, we removed students who were only identified as multilingual by their parent and then proceeded with our main model matching and regression analyses. We removed these students because teachers might hold biased perceptions of multilingual students more broadly. In cases where they do, we only wanted to include in our treatment and control group students that teachers knew to be multilingual. While teachers knew their EL students were multilingual, by definition, they may not have known about their non-EL students' home languages. As such, our main model might have biased our estimates by including in our control group students that teachers did not consider multilingual. As noted earlier, there was a significant amount of movement both out of, and into, the EL status category across grades. Specifically, 20% of the control group were reported as being in an EL program in either or both first and second grades. This could have biased our results since the control group included EL students. As such, we conducted a fifth sensitivity check where the treatment group was defined as "ever-EL" students, that is, students who were reported as being in an EL program for at least one of the three grade levels. Control group students, by contrast, were defined as "never-EL" students. Finally, we addressed the movement in and out of the EL category through models that shifted the treatment variable to a time-varying variable indicating EL status in the current grade (Sensitivity Check 6). In these models we also shifted the classroom, teacher, and school variables to reflect the current grade. Of note, these models answer a somewhat different research question; they estimate the effect of EL status on teacher perceptions within a given grade level. Supplemental Appendix Table A in the online version of the journal presents descriptive statistics of the matched samples for the treatment and control groups for the sensitivity checks that involve matching (2–6). Results from all sensitivity checks are presented in Supplemental Appendix Table B in the online version of the journal and described at the end of the results section. Table 3 presents CEM estimates of the effects of kindergarten EL status on teacher perceptions of students' academic skills among multilingual students. Results were negative across all four academic content areas—language/literacy, math, social studies, and science—and across all three grades—kindergarten, first grade, and second grade. Results were statistically significant in all domains in first grade but were not statistically significant in kindergarten. In second grade, results were significant in math, and marginally significant in the composite outcome. These results suggest that EL classification in kindergarten had a negative effect on teachers' perceptions of student academic skill level in first grade, and in math in second grade. Negative effect sizes ranged from a tenth to a third of a standard deviation. On the composite outcomes, EL status resulted in lower teacher perceptions of approximately a quarter of a standard deviation in first grade and a seventh of a standard deviation in second grade (as noted, the later estimate was only marginally significant). Effects of EL status on teacher perceptions accounted for a considerable proportion of the average difference between teacher perceptions of multilingual EL and non-EL students (see Table 1). For example, EL status effects accounted for over half (59%) of the average differences in teacher perceptions in first grade. Across grades, there was no clear evidence supporting our hypothesis that EL status effects were larger in language arts than in math. Results from our models that added later grade achievement measures provide preliminary evidence that teacher perception effects in first and second grades were driven by both direct and indirect effects (see Table 4). Point estimates from this set of models represent the estimated direct effects of kindergarten EL status on teacher perceptions that remain once removing indirect effects. These point estimates remained negative, but they were smaller in magnitude than the point estimates from our main models (first- and second-grade composite outcome point estimates were 46% and 62% smaller, respectively). Four out of the six estimates that were significant or marginally significant in the main models remained so in these mechanism models. This suggests that a portion—but not all—of the effect of kindergarten EL status on teacher perceptions was explained by differences in student skill levels that emerged in the first and second grades between students who had had equivalent achievement levels in kindergarten. Table 5 shows CEM estimates from our moderator models, where we removed the EL status indicator and replaced it with two alternative indicators, one for EL students in bilingual classes and one for EL students not in bilingual classes. Non-EL kindergartners (98% of whom were not in bilingual classes) remained the reference category. Point estimates on the two indicator variables represent the estimated correlational difference between the relevant kindergarten EL group and the non-EL reference group. The table also includes results from contrast tests that examined whether there were significant differences between the two EL groups. In first and second grades, we found a negative association of kindergarten EL classification with teacher perceptions of student academic skill level among students who were not in bilingual classes. These point estimates were uniformly negative and were generally of larger magnitude than estimates in Table 3 that combined EL students in and not in bilingual classrooms. Estimates in first grade were, as in the main model, statistically significant, and those in the second grade were also statistically significant or marginally significant across all outcomes except for language. One outcome (science) was also statistically significant in kindergarten. By contrast, there was no evidence of a significant association of kindergarten EL status with teacher perceptions in any grade or academic domain when EL students were in bilingual classes (with the exception of first grade social studies). Unlike for EL students not in bilingual classes, teachers had comparatively higher perceptions of academic skill level for their EL-bilingual students compared with their non-EL, nonbilingual students, on average, in certain academic domains in kindergarten and second grade. Focusing on the composite outcomes, point estimates of the negative association of kindergarten EL status with teacher perceptions were magnitudes larger for EL students not in bilingual classes compared with those in bilingual classes in first and second grade. Contrast tests between the two kindergarten EL groups indicated that teachers had generally lower perceptions of EL students who were not in bilingual classes than they did of EL students who were in bilingual classes; however, by and large these tests did not reach statistical significance. Supplemental Appendix Table B in the online version of the journal presents results from our sensitivity analyses as described in the methods section. In all cases results paralleled those from our main analyses indicating negative effects of kindergarten EL classification on teacher perceptions across grades and academic domains, with minor differences in magnitude and statistical significance. Sensitivity Analysis 1, which involved ordinary least squares regression analyses without matching, was meant as a first examination among the full analytic sample. These results show a consistent, negative, and significant (or in a few cases marginally significant) relationship between EL classification and teacher perceptions across academic domains and grade levels. The remainder of the checks involved matching and are thus alternative causal estimates. Sensitivity Checks 2 and 3 both altered the matching algorithms but not the analytic samples. Results from Sensitivity Check 2, which employed propensity score matching, suggest slightly larger (and significant) negative effects in kindergarten compared with first and second grades, and first- and second-grade estimated effects were smaller than in the main model. Results from Sensitivity Check 3, which matched on the full battery of ECLS-K: 2011 assessments, suggest the opposite: larger and more significant results in first and second grades, compared with kindergarten, with results in kindergarten and first grade similar to the main model, but larger in second grade. Sensitivity Checks 4 and 5 altered the analytic samples. In both cases, effect sizes were larger and mostly significant in first and second grades, compared to kindergarten. When compared to the main model, point estimates were slightly larger in the latter two grades in the check that included only teacher-identified multilingual students (Sensitivity Check 4), and the check that used ever-EL students as the treatment group (Sensitivity Check 5). Finally, Sensitivity Check 6 examined within-grade effects of EL classification rather than looking at longitudinal effects of kindergarten EL classification. Results from these analyses were smaller than the main model (and some estimates were positive rather than negative), and did not reach statistical significance, adding to evidence that indirect effects of EL classification on teacher perceptions play an important role. This study sought to analyze the effects of EL classification in kindergarten on teacher perceptions of student skills and abilities in kindergarten, first, and second grade. While EL classification is designed to ensure the rights of a potentially vulnerable group of students (Gándara et al., 2004), scholars have highlighted how this classification is oriented around deficits (English proficiency) rather than assets (multilingualism, etc.; Martínez, 2018). As such, prior work has documented how EL classification can have a direct and negative effect on students' opportunities and outcomes in school (Carlson & Knowles, 2016; Cimpian et al., 2017). One theorized mechanism for this negative EL classification effect is systematic differences in teacher perceptions (Blanchard & Muller, 2015). Harnessing the variation in English proficiency thresholds used in different states and districts to determine EL status eligibility (National Research Council, 2011) as a natural experiment (T. D. Cook et al., 2008; Murnane & Willett, 2010), we used ECLS-K:2011 data and CEM to examine teacher perceptions over time of students who entered school with the same English proficiency and academic skill levels (as well as other student, class, program, and school characteristics) but different language classifications (EL and non-EL). The results suggest that, as theorized, EL status in kindergarten has a negative effect on teachers' perceptions of students' academic skills across multiple academic domains and grade levels. In our main models, results are weaker in kindergarten and second grade, and stronger in first grade. Effect sizes on composite measures range from a tenth of a standard deviation (kindergarten—not statistically significant) to a quarter of a standard deviation (first grade—statistically significant). Results from a host of sensitivity checks, including alternate methods (propensity score matching), algorithms, and analytic samples, converge on these findings of negative effects of EL classification on teacher perceptions, although effect sizes and significance levels vary somewhat across models. Effect sizes are, by and large, meaningful, accounting for a quarter to a half of the overall differences in teacher perceptions of EL and non-EL multilingual students. They also parallel those found in prior research on teacher perceptions. For example, Ready and Wright (2011) find that teacher perceptions of the literacy skills of Latinx students who speak a non-English language at home are underestimated by between a quarter and a third of a standard deviation, once accounting for direct measures of literacy skills. Results from our mechanism analyses, where we account for students' later skill levels, provide preliminary evidence that EL status affects teacher perceptions both directly, due to biases associated with the EL label, and indirectly through diminished opportunity to learn that results in lower student academic growth that is then accurately represented in later grade teacher perceptions (Garrett & Hong, 2016). We examine estimated effects both across content areas (in composite perception measures) and within content areas (in language arts, math, social studies, and science). While we hypothesized that kindergarten EL status might affect teachers' language arts perceptions more than math or other content areas, our results did not support this hypothesis. Results are fairly consistent across the four academic domains. The only grade level where point estimates differ meaningfully across domains is in second grade, where effects are considerably larger in math than in the other domains. But this difference is not reflected across the sensitivity checks and we therefore conclude that more work is needed to explore any differences in EL classification effects on domain-specific perceptions. Given that prior work has also demonstrated that the extent and characteristics of teacher bias vary based on contextual features, we sought to examine whether negative effects of kindergarten EL status on teacher perceptions are minimized or avoided in bilingual instructional settings. Previous research has found that these settings tend to, but do not always, have more positive and asset-based orientations of multilingual students (for important work on how bilingual environments may also perpetuate deficit orientations of EL-classified students, see Cervantes-Soon et al., 2017, Martínez-Roldán & Malavé, 2004; Valdés, 1997). Consistent with our hypothesis, we found that, when in bilingual settings, teachers do not have systematically different perceptions of their kindergarten EL students compared to their non-EL multilingual peers. These results give preliminary evidence that bilingual instructional environments may counteract the negative effect of EL classification on teachers' perceptions of their students' academic skill levels. The findings from this study contribute to theory on and understanding of teacher perceptions and the experiences and opportunities of EL-classified students. With regard to research on teacher perceptions, this study confirms and adds to existing work that finds that teachers are more likely to underestimate the abilities of students who already face societal and educational discrimination and unequal opportunity. For example, prior work has found that teachers tend to be more biased against African American students (Ferguson, 2003), special education students (Bianco, 2005), and girls (in certain domains; Hinnant et al., 2009). Like these groups of students, EL students also face societal discrimination and unequal opportunity (Gándara & Hopkins, 2010; Lippi-Green, 1997). Because we find suggestive evidence that EL status may influence teacher perceptions directly and indirectly via student outcomes, we come to mixed conclusions regarding the question of whether teacher perceptions account for negative effects of EL status on students' outcomes reported by previous studies (Carlson & Knowles, 2016; Umansky, 2016). While we find evidence that teachers have lower perceptions of EL students even after controlling for past and current student skill level, we also find suggestive evidence that teachers are accurately picking up on emerging differences in the skill levels of their EL and non-EL students over time (Jussim et al., 1996; Jussim & Harber, 2005; F. A. López, 2017). Our findings, therefore, paint a more complex and nuanced picture of how EL status may affect students' educational outcomes. Namely, lower teacher perceptions of EL compared with non-EL students appear to reflect both biases as well as real differences in academic trajectories that may be caused by unequal access to content (Estrada, 2014; Kanno & Kangas, 2014) and other mechanisms. Importantly, this study does not examine how negative teacher perceptions may alter EL-classified students' academic outcomes. This is an important area for future research especially because prior work shows that groups of students that face societal discrimination are particularly vulnerable to teacher perception and expectancy effects (Hinnant et al., 2009; Van den Bergh et al., 2010). Research in the field of EL education gives preliminary evidence of this vulnerability. For example, Callahan (2005) showed that track placement, often determined by teacher decisions and therefore subject to teacher perceptions, is a strong predictor of students' academic performance, stronger, in fact, than English proficiency level. This lends urgency to the need for future research that examines the effects of teacher perceptions on EL-classified students' educational and self-perception outcomes. With regard to the bilingual-setting moderator results, these results similarly contribute to existing work regarding how teacher perceptions are moderated by contextual features such as teacher-student racial congruence and the average socioeconomic status of students in the classroom (Oates, 2003; Ready & Wright, 2011). This study suggests that bilingual settings likely operate as one of these moderators of teacher perceptions. What this study cannot identify is what it is about bilingual settings that drives this moderating relationship. It is important to consider two possible explanations for our results: first, that something about bilingual settings may drive this association, or second, that bilingual settings may proxy for some other possible moderator. Regarding the first possibility, it is plausible that the specialized training and education that bilingual teachers receive toward working with EL students may lead to less biased perceptions of EL-classified students and/or instructional choices that do not impart an academic penalty on these students (Fránquiz et al., 2011; García & Guerra, 2004; F. A. López, 2017; Moll et al., 1992). In addition, teachers' linguistic skillsets may allow them to communicate with students and their families in fuller ways that offset bias and/or increase opportunity to learn (Loeb et al., 2014; Matthews & López, 2019). Regarding the second explanation, it is also plausible that individuals already predisposed to not be biased against their EL students disproportionately select into bilingual settings. For example, teachers who have an underlying value for multilingualism and diversity may select into bilingual settings. Likewise, bilingual teachers may be more likely to share their EL students' linguistic and cultural roots and this shared background may be associated with less bias and/or more beneficial instructional choices. In reality, both sets of factors may be in effect, with both teacher selection into bilingual settings and teacher preparation and training minimizing effects of EL status on teacher perceptions. Future research should disentangle these possible mechanisms. Either way, however, this study adds to a robust body of work on the benefits of bilingual instruction for multilingual students (Callahan & Gándara, 2014; Fránquiz et al., 2011; Steele et al, 2017). While matching is vulnerable to omitted variable bias, we believe the context of a natural experiment across locales, paired with a data set providing independent and directly measured multilingual student English proficiency level (along with a rich array of other variables) warrant causal interpretation of our results. However, if matched students classified as EL in kindergarten differ from those not classified as EL in ways that we cannot observe or control for but that are related to teacher perceptions, then our estimates may be biased. Future research should explore these questions using alternate quasi-experimental methods and data sets. Related, a second limitation of this study is that it relies on the assumption that the ECLS-K:2011 basic English proficiency assessments accurately measure students' English proficiency levels. If these measures are invalid or if they are too coarse to meaningfully differentiate between students, then our causal inference may be uncertain. This said, the fact that matching on English proficiency scores resulted in a treatment and control group that did not differ on measured reading or math scores provides at least preliminary evidence of the validity of the ECLS-K:2011 English proficiency assessments. Although these limitations need to be kept in mind, the results of this study have important implications for educators, education leaders, and policymakers. Because our results lend support to our hypothesis that EL status can affect teacher perceptions through both biases based on the label, and through altered instructional choices that negatively affect EL students' opportunities to learn, policy and practice implications should address both possible causal mechanisms. For example, interventions that attempt to decrease teacher bias—such as implicit bias training—may help teachers better understand, acknowledge, and ideally avoid bias against EL-classified students in their schools and classrooms (Polat et al., 2019). Similarly, instructional policies and practices that ensure that EL-classified students have equal access to content and instruction may avoid indirect effects of EL status on teacher perceptions that operate through students' affected learning trajectories. Our results also highlight the potential risk inherent in high-stakes decisions based on teachers' judgments of students' skills in the absence of established, unbiased, measures, policies, or procedures. Finally, the results of this study also support current efforts to expand students' access to bilingual instructional settings. As future research unpacks the mechanisms by which bilingual settings may counteract negative teacher perception effects, these mechanisms can hopefully be applied to nonbilingual settings as well, be they professional training in techniques to connect with students' families, or policy initiatives to increase the share of teachers who share linguistic and cultural backgrounds with multilingual populations. Ilana M. Umansky https://orcid.org/0000-0001-9907-834X
10.3102_0002831221999405	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831221999405	District-Level School Choice and Racial/Ethnic Test Score Gaps	 The rapid expansion of school choice is restructuring public education in the United States. This study examines associations between charter and magnet school enrollment, White-Black and White-Hispanic segregation, and test score gaps at the district level from 2009 to 2015 in third to eighth grade using the Stanford Education Data Archive and the U.S. Department of Education's Common Core of Data. Robust findings indicate that higher charter school enrollment is associated with larger White-Black test score gaps and this effect is mediated by White-Black segregation. There is also evidence that magnet school enrollment is associated with White-Hispanic test score gaps. Overall, this study suggests that the expansion of school choice may have negative implications for structural education equity.	 School choice is restructuring public education in the United States in the form of open enrollment, means tested vouchers, magnet schools, and charter schools. Magnet and charter schools in particular are expanding rapidly. Between 2001 and 2017, the number of students in the United States attending a magnet or charter more than tripled (National Center for Education Statistics [NCES], 2018). As of 2017, over 2.5 million public school students attended magnets and over 3 million attended charters, together comprising over 11% of all public school students in the United States (NCES, 2018). In many districts, magnets and charters encompass a significant proportion of the available public schools. For example, Los Angeles, Miami-Dade, and Houston school districts all have over 100 magnets (Polikoff & Hardaway, 2017) and in Washington D.C., Detroit, and New Orleans, charters make up more than 40% of public school enrollment (National Alliance for Public Charter Schools [NAPCS], 2012). Students typically apply and are admitted to magnet and charter schools via lottery, or in some cases, on a first come first serve or test score basis (Gleason et al., 2010; Goldring & Swain, 2019). Magnet schools typically offer a thematic curriculum and aim to promote racial/ethnic integration by enrolling students from multiple catchment areas within or across districts (Goldring & Swain, 2019). The missions of charter schools range from serving as lab schools to test novel teaching pedagogies to increasing competition in public education markets, which is hypothesized to incentivize public school improvement (Mickelson et al., 2013; NAPCS, 2012). Magnets are predominantly operated under the jurisdiction of the same public school districts as traditional public schools while charters usually have their own governing bodies that can include for-profit educational management organizations (Ertas & Roch, 2014; Mickelson et al., 2013). Thus, these schools likely have distinct impacts on district-level academic outcomes due to differences in enrollment, governing, accountability, and funding structures. Along with the rise of school choice is an increase in research examining the efficacy of magnet and charter schools, which is largely conducted at the local level and provides important insights to inform local school choice policy. However, when considering federal policy decisions and federal incentives and guidelines for states and districts, it is also important to understand how school choice influences district-level academic outcomes on a national scale. Furthermore, looking between schools within a district does less to advance a broader understanding of what contributes to structural education inequity because evidence suggests that most of what explains variance in achievement disparities occurs between rather than within districts (Reardon et al., 2019a). Structural education inequity, which in this article refers specifically to the institutional and systemic oppression of Black and Hispanic students concurrent with the privileging of non-Hispanic White students in public education, remains a critical issue in the United States (Nieto, 2005). Persistent gaps between White students' test scores and the test scores of Black and Hispanic students on a macro scale signal inequitable access to opportunity, resources, and support more broadly and therefore serve as one indicator of structural education inequity (Reardon et al., 2019b). There has been a general decline in racial/ethnic test score gaps since 1940; however, from 1990 to 2000 both White-Black and White-Hispanic test score gaps widened or stabilized (Lee, 2002). In the 2014–2015 school year, Black students in third through eighth grade scored an average of 1 SD below White students on standardized math and English language arts (ELA) tests. Hispanic students scored between 0.7 and 0.8 SD on average below White students on math and ELA tests, respectively (author calculations from the Stanford Education Data Archive (SEDA; Reardon et al., 2018)). Test score gaps vary significantly by school district, and patterns of racial/ethnic segregation are among the strongest correlates of these gaps (Reardon et al., 2019a). Prior research suggests that the influx of school choice is associated with increases in segregation at the district level (Frankenberg et al., 2010; Harris, 2018; Monarrez et al., 2019). Therefore, this study uses SEDA version 2.1, a publicly available national data set, to investigate whether school choice enrollment is associated with test score gaps at the district level in third through eighth grade, and whether racial/ethnic segregation mediates this association. Overall, there is great variability in associations between school choice and students' test performance in elementary and middle school, likely due to heterogeneity in school choice landscapes across different contexts. Methodological approaches to studies examining the impact of school choice on students' academic outcomes also vary widely, particularly in their treatment of comparison groups and their attention to omitted variable bias (e.g., Rapa et al., 2018). Here, we summarize the results of selected large-scale studies that address threats to internal validity through research design or analytic approach. A quasi-experimental examination of five urban school districts in four states found no overall effect of magnet schools in grades K–12 (Wang et al., 2017). Differences in academic outcomes between magnet and traditional public school students varied widely depending on a range of school-level factors, including student engagement, professional development, teacher support, and partnerships (Wang et al., 2017). Another study used value-added models with student fixed effects to examine effects of magnet schools in a large southwestern school district (Harris, 2019) and found that magnet students in first through eighth grade had the same or worse test scores compared with students in traditional public schools. Some of the effects were related to peer composition on dimensions of race, poverty, and academic track (Harris, 2019). Large-scale charter school studies also yield mixed findings at the local level due to contextual differences. For example, two causal studies leveraging charter middle school lotteries as instrumental variables, one examining 33 schools in 13 states (Clark et al., 2015) and another examining 28 schools across 15 states (Gleason et al., 2010), found no overall achievement effect for charter attendance; some charters' students exhibited better test scores and some worse, compared with other students who applied. These studies found variation in test scores related to urbanicity and socioeconomic status. When discussing district-level education equity, it is also essential to consider students who remain in traditional public schools in districts where choice is expanding. The indirect impacts of school choice on traditional public school students also vary depending on context. For example, evidence in a large southwestern district using an instrumental variables approach finds that charter expansion undermines the test performance of elementary students in traditional public schools (Imberman, 2011). However, a study in New York City employing a difference-in-difference approach found elementary students in traditional public schools experienced slight increases in test scores resulting from the presence of charters (Cordes, 2018). Consequently, there is evidence that school choice can be disadvantageous, advantageous, or unrelated to the test performance of students both in public schools of choice and in traditional public schools. The disparate impacts of school choice merit additional research to delineate how these effects map onto differences in racial/ethnic test score gaps across districts throughout the United States. It is also important to better understand the mechanisms that drive differences in associations between school choice enrollment and test score gaps. One potential mechanism is district segregation. By some measures, U.S. public schools are at least as segregated by race/ethnicity as they were in 1954, when Brown v. Board of Education deemed segregation unconstitutional (Donnor & Dixson, 2013). As of 2016, over 18% of schools were 90% to 100% non-White, up from 6% of schools in 1988 (Frankenberg et al., 2019). While much of this change is attributable to factors such as the increasing racial and ethnic heterogeneity of the U.S. population and exclusionary zoning, it is also predicted by the proliferation of school choice policies that can serve as catalysts for racial/ethnic segregation (Donnor & Dixson, 2013; Fiel, 2013; Rothwell, 2012). National studies of districts with charter schools find that charter students are more likely to be in segregated schools and that charter growth relates to increases in segregation (Monarrez et al., 2019; Vasquez Heilig et al., 2019). Additionally, even for charters that disproportionately enroll Black or Hispanic students, transfers of Black and Hispanic students from traditional public schools to charter schools can contribute to district-level segregation (Bifulco & Ladd, 2007; Garcia, 2008; Kotok et al., 2017). Studies find that charter expansion is especially predictive of White-Black segregation (Fiel, 2013; Frankenberg et al., 2010). There is less research linking magnet schools to increasing segregation at the district level, likely because many magnet programs were established to address federal desegregation mandates (Goldring & Swain, 2019). Magnet schools became an especially useful tool for school districts that had to rely on a metropolitan remedy due to Milliken v. Bradley; these were metropolitan districts with desegregation orders that did not have the option of interdistrict mandates despite surrounding suburban areas actively contributing to de facto segregation (Gordon, 1994). However, it is possible for a district to become more segregated, even as its magnet schools become more integrated (Harris, 2018). For example, in some urban, predominantly non-White school districts, magnet schools attract a majority of the White students, which fosters within-school integration in a few schools while amplifying segregation at the district level (Kimelberg & Billingham, 2013). Furthermore, while federally mandated desegregation orders and metropolitan remedies established an initial relationship between magnets and integration, once districts began to achieve unitary status, the role of magnets shifted away from integration (Goldring & Swain, 2019; Harris, 2019). In fact, Supreme Court rulings starting in the mid-1990s, and continuing with more recent cases such as Parents Involved in Community Schools v. Seattle, increased restrictions on the consideration of race/ethnicity in public school enrollment overall, particularly for districts with unitary status (Goldring & Smrekar, 2002; Goldring & Swain, 2019; Harris, 2019). Thus, many magnet schools are increasing their focus on higher achievement while decreasing integration efforts (Harris, 2019), and in some cities magnets are increasingly racially isolated (e.g., Grooms & Williams, 2015). Therefore, when examining associations between school choice and structural education inequity, attention to magnet schools is crucial. While much of school segregation is rooted in neighborhood segregation (Rothwell, 2012), school choice also independently contributes to segregation. In fact, many districts in the United States are becoming more racially/ethnically diverse while the schools in those districts are concurrently becoming more segregated (Coughlan, 2018; Mader et al., 2018; Monarrez, 2018; Siegel-Hawley, 2014). There is also evidence that as White students' neighborhoods become more racially/ethnically heterogenous, White students are more likely to apply to magnets and charters, leading to increased school segregation (Goldring & Swain, 2019; Renzulli & Evans, 2005). One reason for this may be that for some parents, the idea of sending their children to the traditional public schools deterred them from living in certain neighborhoods, and magnet or charter schools potentially remove that deterrent. This is supported by evidence that an influx of school choice is associated with rising gentrification (Pearman & Swain, 2017). Persistent links between school segregation and structural education inequity suggest that segregation resulting from the proliferation of school choice likely has implications for racial/ethnic test score gaps. School segregation by race/ethnicity is one of the largest correlates of district-level test score gaps (Reardon et al., 2019a, 2019b). Most evidence finds that school segregation is associated with lower resourced schools and inferior school quality (e.g., facilities, range of course offerings, access to counselors, student to teacher ratio, and teacher qualifications, absenteeism, and retention) for minoritized students, which affects educational outcomes into adulthood (Johnson, 2019). For example, for Black students in particular, a 15% increase in segregation for at least half of a student's schooling years is associated with a 7–percentage point decrease in likelihood of college attendance, a 7% reduction in earnings, and a 3.5–percentage point increase in likelihood of incarceration (Johnson, 2019). In general, school segregation concentrates social and economic disadvantage in the lowest resourced schools that are also majority Black and or Hispanic (Rothstein, 2015). Thus, when school choice segregates districts by race/ethnicity this is accompanied by socioeconomic segregation and coupled with an inequitable distribution of resources. For example, Bifulco et al. (2009) found that students with college educated parents were more likely to choose a magnet or charter if their traditionally assigned public school had a large proportion of students without college educated parents. Ni (2012) found that in predominantly urban and low-income areas, student transfers from traditional public schools to charters resulted in both the charter and traditional public school enrollment becoming stratified by race, socioeconomic, and special education status (Ni, 2012). In this case, charter school expansion segregated minoritized, low-income, and special needs students in the most underresourced traditional public schools (Ni, 2012). Hence, if the expansion of school choice segregates school districts in a way that further concentrates disadvantage in the most underresourced schools where the enrollment is majority Black and or Hispanic students, then White-Black and White-Hispanic test score gaps may widen (Bifulco & Ladd, 2007). There is also evidence that racial/ethnic school segregation has disproportionately negative impacts on the academic performance of minoritized students independently of socioeconomic status (e.g., Hanushek et al., 2009; Mickelson, 2001). Research examining math test score gaps over thirty years finds that despite increases in economic mobility for Hispanic and Black families, school segregation is also growing which leads to racial/ethnic isolation and results in significant increases in test score gaps (Berends & Peñaloza, 2010). Hence, segregation may also widen test score gaps due to processes related to racial/ethnic inequity beyond gaps in schools' socioeconomic resources, such as unequal social capital (Bankston & Caldas, 1996), teachers' racial biases (T. M. Scott et al., 2019), racist disciplinary practices (Skiba, 2015), and language barriers for Spanish-speaking students (Reardon & Galindo, 2009). In addition to the negative associations between segregation and education equity, there are positive benefits of integration. Studies find that integrated schools improve academic outcomes for Black students in particular, with positive or null effects for White students (Johnson, 2019; Linn & Welner, 2007). Beyond academic outcomes, integration also reduces White students' racial prejudice, increases the racial diversity of their social networks, and shapes students' political identities in adulthood (Billings et al., 2020; Johnson, 2019). Thus, not only is the exacerbation of segregation actively detrimental to education equity, but efforts intentionally focused on integration have the capacity to narrow disparities in academic outcomes across race and ethnicity while improving equity. However, there is also some evidence that school choice has the capacity to improve educational outcomes for minoritized students, despite increasing segregation, in instances where schools of choice have more resources than traditional public schools (Whitehurst et al., 2016). In other words, Black and Hispanic students in racially/ethnically homogenous schools of choice may fare better than they would in more heterogeneous traditional public schools if the schools of choice have greater resources and are of superior quality to the traditional public schools students would otherwise be enrolled in. If school choice is primarily working to improve the quality of education for Black and Hispanic students, then district-level White-Black and White-Hispanic test score gaps may narrow as a result of school choice expansion (Dobbie & Fryer, 2011; Whitehurst et al., 2016). Given these competing possibilities, it is important to examine links between school choice, segregation, and test score gaps to better understand how this widespread public education reform is associated with structural education inequity. The current study examines the relationship between district-level school choice and test score gaps in third through eighth grades for 4,613 school districts in the United States from 2009 to 2015 by addressing two main research questions: Research Question 1: What is the association between magnet and charter school enrollment and White-Black and White-Hispanic math and ELA test score gaps at the district level? Research Question 2: Are there indirect effects of magnet and charter school enrollment on White-Black and White-Hispanic math and ELA test score gaps at the district level that operate through White-Black and White-Hispanic segregation? Most of the data analyzed in this study come from SEDA (Reardon et al., 2018), a publicly available data set that includes national district-level data for grades three through eight from the 2008–2009 to 2014–2015 school years. SEDA includes data for 12,065 school districts in total which enroll roughly 35 million third through eighth graders. The cases included in SEDA represent 89.5% of all possible district-level cases across subject, grade, and year. The 10.5% of missing cases in the full SEDA are due to suppressed data for districts with less than 95% participation for standardized tests, suppressed individual estimates with a standard error greater than 2 SDs on the state-standardized scale, test score data not reported to EdFacts, and data identified as incorrect due to data entry errors. There are 4,613 school districts in SEDA with enough racial/ethnic heterogeneity to measure White-Black and or White-Hispanic math and or ELA test score gaps. These districts represent 38% of the districts included in SEDA, but they serve almost 29 million third through eighth graders. This is 83% of all public school students in third through eighth grade enrolled in districts included in SEDA and an even larger proportion of Black and Hispanic students. Districts are considered to have adequate racial/ethnic heterogeneity if they have at least 20 students of each racial/ethnic subgroup in at least one grade in third through eighth grade in at least one school year from 2008–2009 to 2014–2015 (e.g., for White-Black test score gaps, districts must have at least 20 Black students and 20 White students in at least one grade in at least one year). SEDA chose a 20 student minimum based on the disclosure risk agreement with the National Center for Education Statistics. Of the 4,613 districts that meet the criteria described above, 95% of observations have complete data for every variable analyzed in the study. To account for the 5% of missing data, the study used multiple imputation methods with chained equations to create 20 complete data sets in Stata 15.0 (Royston, 2004, 2005). A total of 2,171 of the districts in the analyses have enough heterogeneity to examine both White-Black and White-Hispanic math and or ELA test score gaps. An additional 743 districts only have enough heterogeneity to examine White-Black math and or ELA test score gaps (resulting in a total of 2,914 districts analyzed for the White-Black test score gaps (math and reading test score gaps have different sample sizes due to differences in available data per test per district)), and an additional 1,699 districts only have enough heterogeneity to examine White-Hispanic math and or ELA test score gaps (resulting in a total of 3,870 districts analyzed for the White-Hispanic test score gaps). Of the total 4,613 districts, about 31% have school choice in the form of magnets and or charters (Table 1), compared with only 15% of the 12,065 districts in SEDA overall. Of the 31%, 7% have both magnets and charters, 4% have magnets and no charters, and 20% have charters and no magnets. Table 1 highlights a range of differences in districts based on school choice, which extend beyond the presence or absence of magnets and charters. For example, 7% of districts without magnets and charters are urban compared with 51% of districts with both magnets and charters. Additionally, districts with both magnet and charter schools have over eight times the public school student enrollment as districts without magnets and charters. Districts with choice are also substantially less affluent, less White, and more racially and ethnically segregated on average; they also demonstrate lower test scores and larger test score gaps compared with districts without choice. Tables 2 and 3 provide additional descriptive information on the variables included in this study. SEDA obtained test scores from the U.S. Department of Education's (2017) EdFacts database and standardized them according to the National Assessment of Educational Progress scale in order to be comparable across districts and states. SEDA includes test score gaps for districts from 2008–2009 to 2014–2015. Within each district and each school year, the mean test score gaps are included for both math and ELA tests for each grade, from third through eighth. The analyses examine math and ELA test score gaps separately since they are descriptively different. For example, in these data, ELA test score gaps are generally higher than math test score gaps (see Tables 2 and 3). The White-Black test score gaps represent the difference between the mean White students' test scores and mean Black students' test scores for a given test subject, grade, school year, and school district. The same calculation is used for White-Hispanic test score gaps. The Black and White students' test scores in SEDA represent non-Hispanic students. SEDA measures the cumulative percentage of third through eighth graders in each district enrolled in charter schools. In practice, charters are not linked to geographic school districts, so SEDA identified the geographic school district where charters are physically located (only brick and mortar charters are included). In addition to analyzing SEDA data, this study independently sourced and merged magnet school data from the U.S. Department of Education's (2017) Common Core of Data (CCD). The CCD includes school-level data on magnet school enrollment which we aggregated to the district level to compute the cumulative percentage of third through eighth graders enrolled in magnet schools in each district. The CCD does not include complete administrative data on magnet schools in Massachusetts, New Jersey, New York, Ohio, and Vermont. So, we adjusted the classification of magnet schools in these five states in the CCD files based on a list of all the magnet schools in these states obtained by the first author from Magnet Schools of America (2018) based on a directory compiled by researchers at the University of North Carolina Charlotte. SEDA includes a measure of segregation based on Theil's (1972) information theory index. The White-Black and White-Hispanic information index variables are calculated by computing the mean deviation of a student's school's racial and ethnic composition from the school's district-wide racial and ethnic composition. The variable is calculated individually for each grade from third through eighth in each school year from 2009 to 2015. If the variable's value is close to one, this indicates high racial/ethnic segregation. This is a useful operationalization because the point of reference is the district-wide integration that is possible given the racial/ethnic composition of the school district as opposed to measures of segregation that simply look at students' level of exposure to students of other races and ethnicities. As a robustness check, however, we also ran the analyses using exposure indices which measure the average percent of White students in the average Black or Hispanic student's school and vice versa. This is an important check because if a predominantly White school district is adjacent to a predominantly Black school district, it is possible that both of these districts would score relatively low on the Theil index given their low capacity for within-district integration. Additionally, catchment areas of magnet and charter schools do not always correspond to the boundaries of school districts, so they may be working to segregate or integrate school districts beyond those in which they are physically located. The findings of indirect effects of the exposure indices are not included in the results because they largely replicate the size, direction, and significance of the analyses using the Theil index. The analyses control for several factors that are associated with district-level school choice and racial/ethnic test score gaps. Districts' mean test scores are included to control for baseline district test performance. Total district enrollment is included because school choice is more prevalent in larger districts (NAPCS, 2012; Polikoff & Hardaway, 2017). District urbanicity is reflected in a series of dummy variables for urban, suburban (reference group), rural, and town. District-level socioeconomic status is measured with a composite variable that includes a district's median income, proportion of adults with a bachelor's degree or higher, the poverty rate in households with at least one child aged 5 to 17 years, unemployment rate, proportion of households participating in the Supplemental Nutrition Assistance Program, and proportion of households headed by a single mother. District per pupil expenditures are also included (per pupil expenditures for 2014–2015 are not included in SEDA version 2.1, so the values for the 2014–2015 school year reflect the same values as the 2013–2014 school year). District income inequality is controlled for using a Gini coefficient, with values closer to zero indicating equal income distribution in a district and values closer to one reflecting inequality in the income distribution (Von Hippel & Powers, 2015). The racial composition of the school district is accounted for with the percent of students in each grade in each district who are Black and Hispanic. The total percent of English language learners in a district is included as well. For the analyses examining the White-Hispanic test score gaps, the percent of the Hispanic population in a district that is Central American Cuban, Mexican, Puerto Rican, and South American (these reflect the categories in SEDA) are included as covariates to account for the heterogeneity of the Hispanic population in the United States. The analyses also control for standard deviation differences in districts' White and Black parents' as well as White and Hispanic parents' income and educational attainment to account for the intersection of race/ethnicity and socioeconomic status. Importantly, racial/ethnic differences in free lunch rates (included in Table 1) are not included in the final analyses due to high intercorrelations with the racial/ethnic segregation variables; this is discussed further in the limitations. Finally, the analyses control for all grade levels using dummy variables for grades three through eight (reference group) and all school years using dummy variables for each school year from 2008–2009 to 2014–2015 (reference group). The first research question examines the association of charter and magnet school enrollment and the White-Black and White-Hispanic test score gaps at the district level. This association is tested using multilevel random intercept and longitudinal fixed effects modeling. We ran the models separately for each test score gap (White-Black math, White-Black ELA, White-Hispanic math, and White-Hispanic ELA). Our three-level random intercept models include a random intercept at the district and state levels. We conducted all of our analyses using Stata 15.0.[MATH](1) In Equation 1, Yijk represents the racial/ethnic test score gap in grade/year (i), nested in district (j), and in state (k). Here, πpjk (p is the within district parameter) represents the coefficients for predictors apijk that vary within district by grade-level and or school year (percent of students enrolled in charters, percent of students enrolled in magnets, urbanicity, mean district test performance, district size, per pupil expenditures, racial/ethnic composition, percent of English language learners, grade-level, and school year), and eijk is the within-district random effect.[MATH](2) In Equation 2, β0qk (q is the between district parameter) represents the coefficients for predictors Xqjk that only vary in the data between districts (socioeconomic status, socioeconomic inequality, percent of a district's Hispanic population representing different nationalities, and racial/ethnic differences in family income and parent education), and r0jk is the district-level random effect.[MATH](3) In Equation 3, u00k represents the state-level random effect to account for districts being nested within states. While there are no state-level predictors in the models, this level is necessary because school choice policies vary by state and this likely influences the associations in these analyses. For example, the 12 states that were awarded federal "Race to the Top" grants expanded support for charter schools (Boser, 2012). Conversely, some states have caps on the number of charters permitted statewide and as of 2016, seven states did not have laws allowing for charter schools at all (Center for Education Reform, 2018). A general concern of analyzing data at the school district level is omitted variable bias. There are remaining factors at the individual student, school, and district level that influence district-level racial/ethnic test score gaps and school choice that are not measured in these data. Obvious omitted variables include other forms of school choice such as vouchers and open enrollment. We address omitted variable bias with longitudinal fixed effects models that replicate the random intercept models, but also control for each variable's district mean for a given grade from 2008–2009 to 2014–2015. The benefit of this approach is that it removes variance explained by unmeasured time-invariant variables (Miller et al., 2016). So, each district serves as its own counterfactual over time by examining whether deviation from a district's mean magnet and charter enrollment predicts deviation from a district's mean White-Black or White-Hispanic test score gap. In these models, the covariates are also measured as deviations from their district means with the exception of invariant covariates including urbanicity, socioeconomic status, the Gini coefficient, White-Black/White-Hispanic differences in family income, White-Black/White-Hispanic differences in parent education, and composition of the Hispanic population (percent Central American, Cuban, Mexican, Puerto Rican, and South American). While many of these covariates may be time variant in reality, SEDA only includes one measure of these variables from 2009 to 2015. The longitudinal fixed effects models control for these time invariant variables because they may still predict the degree to which a district deviates from its mean test score gap over time. The second research question examines district-level White-Black and White-Hispanic segregation as pathways through which charter and magnet school enrollment relate to White-Black and White-Hispanic math and ELA test score gaps. Congruently to the analytic approach described for research question one, we ran the models separately for each of the four test score gaps. We calculated indirect effects by multiplying the adjusted effects of school choice enrollment on segregation with the adjusted effects of segregation on test score gaps; the significance of indirect effects was probed using Sobel standard errors (Preacher & Leonardelli, 2010). As a robustness check, we also estimated indirect effects in a structural equation modeling framework analyzed in Mplus 8.1. There was consistency across frameworks, so only the models run in Stata 15.0 are included in the results. Due to the high concentrations of zeroes for the percentage of magnet and charter enrollment in districts, we also ran the analyses with natural log-transformed school choice variables. These models yielded consistent results with slightly larger effect sizes. For ease of interpretation, we present the results with untransformed variables in the article and present the results with natural log-transformed variables as supplementary tables (see Supplemental Tables S1 and S2 in the online version of the journal). General trends indicate that even when accounting for factors such as district-level income inequality and racial/ethnic socioeconomic disparities, there are robust but small associations between higher charter enrollment and larger White-Black test score gaps and there is some evidence of associations between higher magnet enrollment and larger White-Hispanic gaps. The results also consistently support that segregation is a mechanism through which charter and, to a lesser degree, magnet enrollment predict larger racial/ethnic test score gaps. The three-level random intercept model results in Table 4 and Figure 1 show that for every 10 percentage points of students in a district enrolled in charters, a district's White-Black math test score gap is 0.3 points (0.03 SD units) larger and the White-Black ELA gap is 0.36 points (0.03 SD) larger. These results are replicated with stronger effect sizes in the longitudinal fixed effects models; math and ELA test score gaps are 0.06 and 0.05 SD larger for every additional 10 percentage points of students in a district enrolled in charter schools (Table 5, Figure 2). These associations are mediated by White-Black segregation. The random intercept models show that an increase of 10 percentage points of charter enrollment is associated with a 0.02 point (0.13 SD) increase in White-Black segregation, which is associated with a White-Black math gap that is 0.03 points (0.003 SD) larger (Table 6, Figure 3). This is even more pronounced for ELA where the indirect effect of White-Black segregation predicts a gap that is 0.08 points (0.01 SD) larger (Table 6, Figure 3). Similar to the direct associations, the indirect effects are stronger in the longitudinal fixed effects models. An increase of 10 percentage points in charter enrollment is associated with a 0.02 point (0.6 SD) increase in White-Black segregation, which is associated with White-Black math and ELA gaps that are 0.11 points (0.02 SD) and 0.15 points (0.03 SD) larger (Table 7, Figure 4). District-level magnet school enrollment does not significantly predict variance in White-Black test score gaps (Tables 4 and 5, Figures 1 and 2). However, there are small significant indirect effects of heightened magnet enrollment on larger math and ELA gaps that operate through White-Black segregation in both the random intercept and longitudinal fixed effects models (Tables 6 and 7, Figures 3 and 4). While charter enrollment predicts variance in White-Black test score gaps, it is magnet enrollment that drives the school choice effects in the White-Hispanic gaps. In the three-level random intercept models, for every additional 10 percentage points of students in a district enrolled in magnet schools, a district's White-Hispanic math gap is 0.15 points (0.02 SD) larger and the White-Hispanic ELA gap is 0.16 points (0.02 SD) larger (Table 8, Figure 1). However, these results were not replicated in the longitudinal fixed effects models (Table 9, Figure 2). There are small positive indirect effects of magnet school enrollment on White-Hispanic test score gaps operating through White-Hispanic segregation in both the random intercept and longitudinal fixed effects models (Tables 10 and 11, Figures 3 and 4). In the random intercept models, a 10–percentage point increase in magnet enrollment is associated with a 0.001 unit (0.02 SD) increase in White-Hispanic segregation, which significantly predicts a White-Hispanic math gap that is 0.002 points (0.0003 SD) larger, indicating a small significant indirect effect (Table 10, Figure 3). The effect was more pronounced for ELA where the indirect effect of magnet enrollment operating through White-Hispanic segregation predicts a gap that is 0.01 points (0.001 SD) larger (Table 10, Figure 3). There are no significant direct effects of district-level charter enrollment on White-Hispanic test score gaps (Tables 8 and 9, Figures 1 and 2). However, the results suggest there are larger indirect effects of charter enrollment than magnet enrollment on White-Hispanic test score gaps operating through White-Hispanic segregation (Tables 10 and 11, Figures 3 and 4). In other words, higher charter enrollment is related to greater White-Hispanic segregation than magnet enrollment which leads to larger indirect effects on White-Hispanic test score gaps. School choice is expanding rapidly in the United States while robust racial/ethnic segregation and test score gaps reflect staunch structural education inequity. Yet the variability of findings in school choice research conducted at the local level makes it difficult to determine best policy practices at the federal level. Consequently, analyses such as those presented in this study are necessary to understand whether school choice may be operating to exacerbate or attenuate structural education inequity on a macro scale. Given the proliferation of school choice, it is important to highlight that this study finds no support for the notion that charter or magnet enrollment narrows district-level race/ethnicity test score gaps. On the contrary, the results suggest that school choice enrollment is associated with slightly larger test score gaps at the district level even when controlling for other district characteristics, including socioeconomic status disparities by race/ethnicity. The associations between higher charter enrollment and larger White-Black test score gaps are robust in all model specifications, and associations between higher magnet enrollment and larger White-Hispanic test score gaps are significant only in the random intercept models. It is useful to consider both the random intercept and longitudinal fixed effects models due to the differences in between-district comparisons versus within-district comparisons that are driving the effects. While the longitudinal fixed effects models provide more compelling causal evidence, these models are only capable of highlighting associations in districts with variability from 2008–2009 to 2014–2015. In other words, if 100% of third through eighth graders in a district are enrolled in magnet schools every year from 2008–2009 to 2014–2015, that district's difference from mean value is zero in every wave, the same as it would be in a district with 0% magnet enrollment from 2008–2009 to 2014–2015. As a result, the longitudinal fixed effects estimates are conservative due to constrained variance. This may be one reason why the magnet effects were not replicated for the White-Hispanic test score gaps. This is further supported by the fact that magnet enrollment only grew about 20% from 2009 to 2015 while charter enrollment grew about 65% (NCES, 2018). Consequently, the fact that the longitudinal fixed effects models show even stronger effects for associations between higher charter enrollment and larger White-Black test score gaps at the district level makes a strong case for the robustness of this finding. The indirect effects analyses suggest that racial/ethnic segregation is one mechanism through which magnet and even more so charter enrollment may be exacerbating education inequity for both Black and Hispanic students. For example, in the longitudinal fixed effects models, White-Black segregation accounts for over half of the association between charter enrollment and the White-Black ELA test score gap. This finding is consistent with literature that demonstrates associations between increases in charter enrollment and racial/ethnic segregation with widening test score gaps (Bifulco & Ladd, 2007; Fiel, 2013; Frankenberg et al., 2010). The indirect effects of higher magnet enrollment and greater segregation are also consistent with research that demonstrates magnet schools can be associated with greater district-wide segregation (Harris, 2018). However, segregation is a much weaker mediator of associations between magnet enrollment and test score gaps than of charter enrollment and gaps. While there are theoretical similarities in how charters and magnets can potentially affect district-level segregation and test score gaps, the school types still have fundamental differences that may explain why the charter findings are stronger. For example, at the school level, charters are predominantly racially and ethnically segregated where magnets tend to be more integrated (Bifulco & Ladd, 2007; Harris, 2018). So, charters may be further segregating districts in addition to contributing to school-level segregation, while magnets may be further segregating districts but increasing opportunities for students in the district to receive integrated education, which may buffer some of the negative effects of district-level segregation. Additionally, as previously mentioned, charters and magnets have different enrollment, governing, accountability, and funding structures. Public school districts often choose how to allocate resources to magnets, but charters are often governed privately and can therefore drain resources from traditional public schools for charter schools that do not serve the most disadvantaged students in a district (Dee & Fu, 2004; Mickelson et al., 2018; Riel et al., 2018). In addition to differences between magnet and charter findings, the patterns of associations between school choice and test score gaps differ for White-Black and White-Hispanic gaps. This is consistent with general differences in test score gap trends across different minoritized groups. For example, the White-Hispanic test score gap is generally smaller than the White-Black gap (Tables 2 and 3). Additionally, an examination of test score gaps from kindergarten to fifth grade reveals that the White-Hispanic test score gap narrows while the White-Black test score gap widens during these grades, and socioeconomic status may explain more of the White-Hispanic gap than the White-Black gap (Reardon & Galindo, 2009). The finding that charter enrollment is associated with White-Black test score gaps and not White-Hispanic test score gaps (except through segregation) is also consistent with evidence that Black students are disproportionately enrolled in charter schools and are more likely to be in segregated charters than Hispanic students (Frankenberg & Lee, 2003). In other words, the association may be more salient for Black students because they are more likely to be exposed to and affected by the proliferation of charters. The association between magnet enrollment and White-Hispanic test score gaps is much weaker and less robust than the charter association with White-Black test score gaps. One potential explanation relates to Harris's (2019) finding that Hispanic students were less likely to be enrolled in magnets compared with White students. Thus, it is possible that in districts with large proportions of magnet schools and Hispanic students, Hispanic students are more likely to be in segregated schools than they would be otherwise. However, given the less robust evidence for this finding in this study, more research is necessary to determine the validity of this association. Overall, the effect sizes in this study are small and, unsurprisingly, the results suggest that factors such as district-level socioeconomic status, district income inequality, and racial/ethnic differences in parent education are much larger predictors of test score gaps than school choice enrollment. However, considering the millions of students attending schools of choice each year and the districts, states, and independent organizations opening schools of choice at an expeditious rate with, at most, the goal of reducing racial/ethnic disparities and, at least, without the goal of exacerbating inequities, even null or small effects supporting the latter should raise concern. Furthermore, small effect sizes are not unusual in macro-level school choice research due to great contextual variability. For example, one national study of charters found a 0.005 SD growth in math for charter students (Cremata et al., 2013 in Rapa et al., 2018). A White-Black math test score gap that is 0.06 SD larger per 10 percentage points of students in a district enrolled in charters (Figure 2) should be equally as compelling as a 0.005 SD growth in math when considering charter expansion. Additionally, the small effects in this study are consistent with prior macro-level research linking increases in charter enrollment to segregation that finds small effects in part due to the immense variability across districts (Monarrez et al., 2019). It is also important to reiterate that the lack of evidence that school choice enrollment is associated with less segregation and smaller test score gaps at the district level should be a concern. If the goal is to eliminate racial/ethnic disparities, school choice expansion may not achieve this goal without attending to other important processes, such as racial/ethnic segregation. In fact, given that much of school segregation is attributable to neighborhood segregation, school choice theoretically provides a unique opportunity to integrate schools by drawing on many catchment areas so that students' schools do not depend on the value of their parents' homes (Rothwell, 2012; Riel et al., 2018). Furthermore, previously mentioned evidence of the positive effects of integration certainly provides a compelling argument in favor of choice policies that result in integrated schooling opportunities for all students within and between districts (Johnson, 2019; Linn & Welner, 2007). Magnet schools, especially those that rely on interdistrict enrollment, are already capitalizing on this (Goldring & Swain, 2019). Yet, as previously mentioned, there is an increasing trend of school choice segregating districts even as catchment areas in the district become more racially/ethnically heterogenous (Coughlan, 2018; Mader et al., 2018; Monarrez, 2018; Siegel-Hawley, 2014). Thus, school choice, and charter school choice in particular, could better capitalize on the capacity for district integration, or at least improve efforts to prevent school choice from actively inhibiting district integration. There are many barriers to school integration such as a reduction in federal desegregation mandates, the legality of de facto segregation, and the increasing legal difficulties to establishing enrollment policies based on racial/ethnic composition (Goldring & Smrekar, 2002; Goldring & Swain, 2019; Johnson, 2019; National Coalition on School Diversity [NCSD], 2020a, 2020b; Reardon et al., 2012). However, there are current efforts in progress to address these difficulties. For example, the "Strength in Diversity Act" is a federal policy initiative to provide local funding for voluntary integration (NCSD, 2020b). States and districts can also adjust their implementation of existing federal policies to reduce the likelihood of school choice exacerbating educational inequities through segregation. For example, districts can work with states via their "Every Student Succeeds Act" (ESSA) improvement plans to receive and allocate funds to improve integration efforts through school choice (NCSD, 2020a). Title IV of ESSA offers assistance for magnets and charters that explicitly prioritizes both socioeconomic and racial diversity (NCSD, 2020a). Furthermore, there are opportunities to improve federal policy as it relates to school choice and education equity. For example, charter expansion is incentivized at the federal level (e.g., "Race to the Top") without any accountability structures for racial/ethnic equity (Boser, 2012; Frankenberg et al., 2019; J. Scott et al., 2020). Given the robust association between charter enrollment and district-level White-Black test score gaps, this is certainly an opportunity for intervention. Thus, despite small effect sizes and the fact that school choice policies do not solely have the capacity to eliminate test score gaps, especially without broader comprehensive social policies, it is still important to intervene where the proliferation of school choice impedes progress toward education equity. This secondary analysis has several limitations. The largest limitation is the inability to draw causal conclusions about whether school choice expansion is driving an increase in racial/ethnic test score gaps. The longitudinal fixed effects analyses provide compelling causal inference; however, they include time variant covariates operationalized with time invariant variables preventing a true longitudinal fixed effects analysis. Variable measurement also presents limitations to the types of questions the analyses can answer. For example, SEDA version 2.1 (Reardon et al., 2018) only includes aggregated charter enrollment for third through eighth grade, so this study cannot examine enrollment by grade. This is important to consider given differences in the academic and psychological development of third graders versus eighth graders. Additionally, about 45% of all magnet students and 17% of all charter students in the United States are in secondary/high school (NCES, 2018). Students in grades nine and above are not included in this study since those grades are not captured in SEDA, and it is possible the associations would look different in older grades. Thus, the findings do not generalize to associations between school choice and test score gaps outside grades three through eight. This study is also limited by the oversimplification of achievement disparities as White-Black and White-Hispanic test score gaps. There are significant intersections between race and ethnicity, and collapsing students into single racial or ethnic categories masks the more complex relationship linking race and ethnicity to academic achievement. Furthermore, there are inherent confounds with test scores as a marker of academic achievement, ranging from stereotype threat to racial/ethnic inequities in achievement-based tracking (Berlak, 2001). However, as previously mentioned, when there are differences between White students' average test scores and Black and Hispanic students' average test scores at the district level, this is an indicator of structural education inequity and inequitable access to opportunity, resources, and support at a macro level (Reardon et al., 2019b). Additionally, as previously mentioned, the analyses do not control for differences in free lunch rates in the average White student's school versus the average Black or Hispanic student's school in a district because this measure is correlated with the racial/ethnic segregation measures at 0.8 or higher. Therefore, despite rigorous controls for district-level socioeconomic factors and socioeconomic disparities by race/ethnicity, this study does not disentangle racial/ethnic segregation from socioeconomic segregation by race/ethnicity. There is strong evidence linking racial/ethnic differences in school poverty to the association between racial/ethnic segregation and test score gaps (Reardon et al., 2019b). Thus, it may be important for future research to examine these associations with socioeconomic test score gaps to provide a more comprehensive understanding of and possible solutions for associations between school choice and structural education inequity. Furthermore, in order to measure test score gaps, districts had to meet the criteria of having at least 20 students in at least one grade in at least one school year. So, extremely segregated districts (e.g., districts that are almost entirely composed of students of a single race/ethnicity) are not included in this study, and these districts are arguably the largest contributors to segregation and least amenable to policy solutions for integration (Holme & Finnigan, 2013). Additionally, when considering racial/ethnic segregation, it is important to highlight that district segregation is just one level at which segregation can inhibit students' academic outcomes. For example, even within integrated schools there is capacity for within school segregation through mechanisms, such as inequitable gifted and talented programs, that may undermine efforts of inter and intradistrict integration (Roda, 2015). Finally, this study exclusively considers brick and mortar magnet and charter schools and does not generalize to all school choice. Magnets and charters only comprise a piece, albeit a large piece, of the overall school choice landscape. Future analyses should also investigate cyber charters, open enrollment, vouchers, and private school choice to examine a more complete story about the relationship between school choice and education inequity. Moreover, as previously mentioned, this study finds null direct associations between charter enrollment and White-Hispanic test score gaps (except through segregation), and the associations with magnet enrollment and White-Hispanic gaps are small and do not hold across model specifications. Thus, more work is needed specifically on how school choice is affecting Hispanic/Latinx populations. Overall, given the limitations in this study, future research should explore more comprehensive operationalizations of education inequity, segregation, and school choice, as well as other potential mediators related to more proximal measures of children's schooling experiences such as instructional quality and disciplinary practices. It is also important to examine how these associations influence longer-term outcomes such as high school and college completion. Despite limitations, by analyzing data from SEDA, a publicly available national district-level data set, this study provides novel information on macro-level associations between school choice enrollment, racial/ethnic segregation, and education inequity across thousands of school districts in the United States. While individual districts and states have unique school choice policies with distinct impacts on education inequity, this study highlights that there are robust patterns nationally at the district-level. This study finds no evidence to suggest school choice enrollment reduces district-level test score gaps in third through eighth grade. In particular, this study finds greater charter enrollment is associated with larger White-Black test score gaps, and this relationship is mediated by White-Black segregation. These macro-level findings are especially important to consider when making federal policy decisions and determining federal incentives and guidelines for state- and district-level policies regarding school choice expansion.
10.3102_0002831221999782	American Educational Research Journal	http://journals.sagepub.com/doi/full/10.3102/0002831221999782	Off the Beaten Path: Can Statewide Articulation Support Students Transferring in Nonlinear Directions?	 Students who transfer between colleges risk losing credits and decreasing their chances of degree completion. Despite emerging evidence regarding the effectiveness of articulation agreements to address this challenge, it is unclear if these policies support nonlinear transfer pathways—including lateral transfer between 4-year colleges or reverse transfer to 2-year colleges. I use propensity score weighting to examine a statewide articulation agreement in Ohio that established universal credit acceptance for coursework affecting all transfers. Comparing students who completed universally transferrable courses with those who did not, I find no measurable difference in degree attainment among reverse transfers. But there is a positive association with bachelor's degree attainment among lateral transfers, which the findings suggest is related to academic major persistence.	 Students who transfer vertically from community colleges garner much of the attention regarding undergraduate student mobility in both research and policy circles; yet a considerable number of students who first enroll in 4-year colleges will also transfer—often decreasing their chances of completing a degree program by doing so. Mobility for such students may occur in one of two nonlinear directions: lateral transfer to another 4-year college or reverse transfer to a community college. Among undergraduates in 2011, 39% of those at 4-year colleges transferred within 6 years—an increase of approximately 6 percentage points from the 2006 cohort (Hossler et al., 2012; Shapiro et al., 2018). According to these trends, nonlinear student mobility is becoming far more frequent as students forgo the "traditional" pathway of college enrollment that begins and ends at the same institution. While vertical transfers move from community colleges as a necessity to attain a bachelor's degree, those who move from 4-year institutions may transfer for various reasons. Among the few studies investigating the determinants of nonlinear transfer, researchers have found that those who move in reverse are most likely to be academically underprepared and come from families in which the parents have low levels of educational attainment (Goldrick-Rab & Pfeffer, 2009; Kalogrides & Grodsky, 2011). In contrast, lateral transfers may be motivated to achieve better institutional fit according to social integration or a change in their academic program of study (Goldrick-Rab & Pfeffer, 2009; Ishitani & Flood, 2018; Li, 2010). Irrespective of their personal motivations, students often experience a number of risk-inducing experiences in the transfer process, particularly the loss of credits. The U.S. Government Accountability Office (2017) found that transfer students lose 43% of the credits earned at their initial institution, on average. But according to a report by the National Center for Education Statistics, the risk of credit loss may be far greater for nonlinear transfer students, who lose between 9.3 and 21 credits, on average, relative to vertical transfers, who only lose 8.2 credits (Simone, 2014). By losing credits, students may be required to enroll in additional terms to repeat similar coursework at their receiving college or university, thereby risking an extended time-to-degree or the probability of not finishing all together. Among first-time freshmen in 2003, the 6-year, bachelor's degree attainment rate for students who transferred laterally was 15 percentage points lower than the national average (National Center for Education Statistics, 2011)—a finding that is robust after accounting for other factors (Andrews et al., 2014; Li, 2010). Although there is little empirical evidence regarding the effects of transferring in reverse, these students are potentially disadvantaged by moving from a 4- to a 2-year college, which have worse degree attainment outcomes by comparison (Alfonso, 2006; Doyle, 2009; B. T. Long & Kurlaender, 2009; Sandy et al., 2006; Stephan et al., 2009). For these reasons, nonlinear transfer activity can lead to significant disadvantages that threaten degree progress. Many state governments have employed statewide articulation agreements to improve the transfer process and subsequent degree completion. The purpose of articulation agreements are to help facilitate a universal agreement of credit flow between public colleges and universities within a state— each with its own course numbering system, course content, academic standards, and degree requirements (Cohen et al., 2014). According to the Education Commission of the States (2016), 42 states have introduced these comprehensive transfer policies. But despite a growing literature regarding the effects of statewide articulation agreements on community college student outcomes (Anderson et al., 2006; Boatman & Soliz, 2018; Gross & Goldhaber, 2009; LaSota & Zumeta, 2016; Roksa & Keith, 2008; Stern, 2016), it remains unclear whether these policies can also support students who transfer from 4-year colleges. Given the magnitude of credit loss that lateral and reverse transfers often encounter, it is significant to examine whether articulation efforts may also support these students in the effort of completing their degree program—and if so, it is also important to further examine why these policies may be effective. On one hand, post-transfer success may depend on whether or not articulation agreements help preserve the transfer of credits from one's first institution to the second, which may reduce the time-to-degree by preventing the need to repeat coursework. On the other hand, articulation policies may also help facilitate a smoother transition in a student's academic major of interest by ensuring that credits are not simply accepted but also applied accordingly to meet their needed degree requirements. In this effort, I examine the effectiveness of an articulation reform effort in Ohio featuring a statewide course equivalency system, which established universal credit transferability across public institutions for coursework in specific academic majors. Using administrative data of Ohio lateral and reverse student transfers, I compare the outcomes of students who transferred after completing universally transferrable (UT) courses with those who did not. Specifically, I use propensity score weighting to reduce bias in estimates of this articulation approach on post-transfer success. The specific questions guiding this research are the following: Research Question 1: Did transfer students who completed UT courses have a higher probability of degree attainment compared with those who did not? Research Question 2: Did the completion of UT courses improve other post-transfer related outcomes? I find that among those who engaged in lateral transfer, students who completed UT courses at their first institution experienced improved post-transfer outcomes. There is a positive 4.5 percentage point difference in the probability of bachelor's degree attainment within 3 years among lateral transfers who completed some amount of transferrable coursework relative to those who did not. The results also suggest that those who completed multiple courses were far more likely to benefit, and the degree completion advantage may be related to improved academic momentum and major persistence. This research sheds new light on the effectiveness of a common yet underresearched policy approach addressing the issue of student mobility: a trend that has only increased over time and threatens progress to rates of degree attainment. As students move between institutions, the loss of credits poses tremendous risk to timely degree completion and may also exacerbate costs. In the wake of the crisis driven by the COVID-19 pandemic and subsequent economic downturn, enrollment trajectories for many students are likely to be interrupted in ways that may include matriculation at multiple institutions in a series of starts and stops. Although many policymakers agree that improving postsecondary success has become increasingly important for success in a shifting economy (Carnevale et al., 2012, Chapter 5; Grubb & Lazerson, 2005), solutions to improve timely degree completion rarely pertain to the issue of student mobility and the inefficiencies that students experience in the process of moving between institutions. But we can no longer afford to assume that students' enrollment patterns are linear or that college destinations are fixed. Through this line of inquiry, this study further advances the necessity to consider the impact of a specific approach to achieve statewide articulation and whether such efforts may advance post-transfer success for students transferring in nonlinear directions. Articulation agreements are conceptualized to improve post-transfer outcomes by helping students to maintain academic momentum. Initially, momentum was defined by the intensity of course-taking and progress early in a student's academic career (Adelman, 1999). Attewell et al. (2012) later theorized that the speed of progress, as indicated by credit accumulation, may positively affect degree completion by enabling greater academic and social integration and by improving the investment to persist, among other potential mechanisms. There is certainly a vast body of literature demonstrating that student outcomes are affected by early academic behaviors (Adelman, 1999, 2005, 2006; Bahr, 2010; Calcagno et al., 2007; Crosta, 2014; Evans, 2019; Jenkins & Cho, 2013; Roksa & Calcagno, 2010). Several studies have also found a positive association between degree attainment and academic momentum, particularly when defined by credit accumulation (Attewell et al., 2012; Attewell & Monaghan, 2016; Doyle, 2011). However, academic momentum may also be diminished in the transfer process due to the loss of credits. According to economic theory, students will continue their pursuit of higher education when the expected benefits of doing so outweigh the potential monetary and nonmonetary costs (G. S. Becker, 1993). In other words, persistence to degree attainment may be threatened by the immediate financial and time-related costs that accompany the loss of credits. Wang (2017) posits that environmental circumstances and barriers, such as the loss of credits may serve as countermomentum friction. By preserving credits in the transfer process, articulation policies may moderate potential frictions and simultaneously promote momentum to degree completion. Although students may lose credits for a number of reasons (i.e., changing their major program of study), articulation agreements may improve post-transfer outcomes particularly when credit loss occurs for one of two primary determinants: curricular misalignment and credit applicability. Concerning curricular misalignment, transfer students may experience problems with credit mobility if the coursework taken at their sending institution is not deemed acceptable by the receiving institution. For instance, college-level courses with similar objectives may not meet the same institutional standards, or they may fail to fulfill degree program requirements at a student's subsequent institution. According to the findings from two studies employing the Beginning Postsecondary Students Study (BPS), vertical transfers who lose credits due to curricular misalignment have a lower probability of degree completion, on average (Doyle, 2006; Monaghan & Attewell, 2015). Most recently, Monaghan and Attewell (2015) found that the odds of degree completion were 2.5 times greater for students who successfully transferred most or all of their credits compared with those who only transferred less than half of their credits. Furthermore, students' progress in their chosen degree program of study may also become threatened due to the applicability of earned credits. Compared with the complete loss of credits, a student's second institution may accept the credits for college-level coursework, but only as electives and not to fulfill specific program requirements. This form of credit loss is most common with coursework intended to meet academic major prerequisites that are required for upper-division study (Bailey et al., 2015). A handful of qualitative studies have illuminated this aspect of the credit loss phenomenon. In a study of 170 transfer students at Indiana University campuses, Kadlec and Gupta (2014) show that students commonly discovered that their credits were transferred only as electives. Recent evidence of transfer students in several states similarly suggests that transfer students may be most likely to lose credits for coursework intended to apply for a specific major (Hodara et al., 2016). Although the stated purpose of statewide articulation agreements is to help preserve the transfer of credits between institutions, much of the literature regarding these efforts have employed nationally representative data to consider their effects on rates of transfer. Using various cohorts of BPS, some studies have found that there is generally no difference in transfer rates between students in states with statewide articulation agreements compared with those in states without them (Anderson et al., 2006; Stern, 2016). But according to Roksa and Keith (2008), these agreements are not intended to improve rates of transfer from 2- to 4-year institutions; instead, they are intended to prevent the loss of credits and thereby improve bachelor's degree outcomes. In a regression analysis using data from NELS:88, the authors found no difference in post-transfer outcomes for students across states with and without statewide articulation (Roksa & Keith, 2008). Using a multilevel modeling approach with data from the BPS:96, Stern (2016) found that these agreements were associated with higher odds of bachelor's degree attainment, in contrast. Other scholars have investigated the effects of specific statewide articulation approaches as opposed to treating all types as essentially the same. Indeed, there is variation between states in the approaches used to achieve statewide articulation such as a transferrable core of general education courses, a transferrable associate degree, a common course numbering system, or some combination thereof (Smith, 2010). Using nationally representative data, a few studies have uncovered a small, positive relationship with the probability of transfer after accounting for variation across specific articulation approaches as well as differences across subgroups of students (Gross & Goldhaber, 2009; LaSota & Zumeta, 2016). In addition, two studies have found that transferring with an associate degree in an academic program (i.e., A.A. or A.S.) was related to an increased probability of bachelor's degree completion (Crook et al., 2012; Kopko & Crosta, 2015). The authors have speculated that the transferability of certain associate degree credentials was likely facilitated by an articulation agreement. Using data from the Integrated Postsecondary Data System, Spencer (2019) also investigated the effect of this articulation policy approach on rates of transferrable degree completion—finding that there is variation in the effectiveness of such policies across states. The author contends that post-transfer success is dependent on whether or not students complete the specific mechanism designated by a given policy that will facilitate the transfer of credits. Boatman and Soliz (2018) corroborate the aforementioned theories that transfer success is conditional on the completion of transfer mechanisms employed for specific articulation approaches. In their study, the authors evaluated the effectiveness of statewide articulation in Ohio by estimating the direct effects of the state's transfer module (TM): a transferrable core of general education credits that improves curricular alignment across public colleges. Students who complete the courses comprising the TM are thus ensured that most of their required lower-division semester hours would transfer universally to all public institutions (Ohio Board of Regents, 2010). Using propensity score weighting with administrative data of Ohio community college students, the authors found that those who completed the TM course requirements were more likely to attain an associate degree, transfer to a 4-year college, and preserve credits post-transfer. Hence, their findings provide direct evidence of how articulation policies may effectively promote academic momentum. But because the research regarding articulation policies have only focused on the outcomes of students who first enroll in community colleges, it remains unclear if the reach of statewide articulation may also extend to lateral or reverse transfers: students who encounter penalties in the process of moving between institutions similar to, if not worse than, vertical transfers. While most approaches to achieve statewide articulation have mainly emphasized supporting the vertical transfer function, a few approaches such as common core curricula and a common course numbering system may also facilitate nonlinear transfer pathways (Li, 2010). But should post-transfer success also be expected of students moving in other directions? Maintaining academic momentum could help increase the odds of degree attainment, particularly bachelor's degree attainment for those transferring laterally and an associate degree for those who transfer in reverse. The present study directly builds on the work of Boatman and Soliz (2018) by further examining articulation reform efforts in the state of Ohio. While the TM was implemented in 1990 as the primary transfer mechanism for Ohio's articulation policy, state policymakers introduced a series of initiatives in subsequent years to improve the transfer of credits. State officials were motivated to enact reforms in order to accommodate significant transfer activity between institutions in its vast public higher education system, which includes 23 community colleges and technical colleges, 14 4-year colleges and universities, and 24 regional branch campuses. In 2003, the state general assembly in Ohio enacted reforms of its statewide articulation agreement with House Bill 95. The policy updates sought to address persistent barriers to bachelor's degree attainment by improving access to transferable coursework. According to reports from the state's higher education agency, many students continued to lose credits for lower-division courses when moving between institutions following the TM introduction (Tafel, 2010). The articulation reform effort featured a statewide course equivalency system—similar to a common course numbering system—that was implemented to ensure the successful transfer of credits for all courses deemed transferable across all public institutions. To establish the universal agreement of credit transfer, the Ohio Board of Regents collaborated with more than 600 faculty members from the state's 2- and 4-year colleges to establish the new course equivalency system (Kisker et al., 2011). Tafel (2010) writes that each discipline-specific faculty panel defined new pathways across specific majors by identifying the learning objectives and competencies for coursework that would be deemed UT, and the panels instituted an approval process for courses according to these new standards. The transferrable courses were then featured in transfer maps called Transfer Assurance Guides (TAGs) made available for students to identify the coursework offered by each respective institution for specific majors in addition to general education course requirements. By providing access to "TAG'ed" courses that were UT, students could move across institutions with the credits preserved for coursework that would fulfill their academic major prerequisite requirements in addition to their general education coursework. Although pathways were eventually developed for more than 40 disciplinary pathways, TAG'ed coursework was first introduced through the new course equivalency system in the fall semester of 2005 for 25 fields of study (Kisker et al., 2011; Ohio Board of Regents, 2010). Because many of these courses were first approved for access at 4-year colleges and universities, students who would later transfer laterally or in reverse were well positioned to potentially benefit. To facilitate the investigation, I use longitudinal, administrative data of students enrolled throughout the Ohio public, higher education system from 2003 through 2010. Maintained by the Ohio Board of Regents, these data include comprehensive information from student transcripts including periods of enrollment, academic performance, degree attainment, and demographic information from questionnaires collected at the initial point of entry. These data, which are organized by student, enrolled term, and institution attended, are ideally suited for this empirical study concerning enrollment and course-taking behavior across years. Importantly, because these data include information for all students across Ohio's higher education system, I am able to follow students who transfer between public institutions. With these data, I generate two separate samples of transfer students who initially enrolled at 4-year colleges and universities: the lateral transfer sample includes students who eventually moved to another 4-year institution, and the other sample includes students who moved to a community college. Similar to other scholars, I define transfers as those who earned 10 or more credits at their first institution before enrolling at another, were not co-enrolled at multiple institutions concurrently, did not earn a degree before transferring, and did not return to their initial institution (Adelman, 2005; Melguizo et al., 2011). Because the study's main objective is to examine the effects of Ohio's 2005 articulation reform efforts on post-transfer outcomes, I also focus on specific transfer cohorts who (1) were enrolled at their initial institution for at least 1 year following the policy introduction and (2) have at least 2 years of post-transfer data available to observe degree attainment. This restriction limits the study to student cohorts who transferred from public, 4-year institutions in 2006, 2007, and 2008. Importantly, the number of post-transfer years observed varies for each cohort (i.e., the cohort of students who transferred in 2008 is observed for only 2 years), but by pooling across multiple cohorts, I am able to generate a robust sample size and improve statistical power. Although there are dissimilarities between students in the timing of their enrollment, I account for such differences with controls and cohort year fixed effects, as further discussed below. These restrictions produce a final analytic data set including a pooled sample of 11,949 lateral students who transferred between 2006 through 2008 and another pooled sample of 4,163 of students who reverse transferred during the same period. To answer the research questions, I estimate the average treatment effect of UT course-taking using an application of propensity score matching that combines inverse probability weighting with ordinary least squares regression. While randomization in an experimental study would be an ideal method to eliminate selection bias by comparing similar treated and nontreated groups, quasi-experiential designs are often a statistically appropriate alternative to estimate policy effects (Shadish et al., 2002). Selection bias is the primary threat to internal validity because it may overstate the effect of completing UT courses. In other words, because students have the agency to choose their own course of study, there may be considerable differences between students who transferred after completing UT courses versus those who did not. For instance, students who completed UT courses may have been more likely to plan their transfer efforts early on—allowing them to identify coursework in which the credits would transfer seamlessly to another institution. Using propensity score matching reduces selection bias by creating balance between the treated and nontreated conditions according to observed characteristics and is a widely used technique to estimate effects in the absence of randomized data (Caliendo & Kopeinig, 2008). A propensity score is the conditional probability of receiving the treatment given the case's observed background characteristics (P. R. Rosenbaum & Rubin, 1983). Accordingly, propensity scores generate a more observationally similar counterfactual of students who did not complete any UT coursework before transferring as a control group. Specifically, I employ multiple dichotomous indicators of UT course-taking that represent a varying number of courses completed. For the main results, I define the treatment group according to students who completed any number of UT courses. But because this classification of the treatment may inadvertently conceal differences in the effect for students who complete different amounts of transferrable coursework, I also generate additional variables to capture potential variation in the effect of UT course-taking. This approach will, therefore, facilitate an examination of heterogeneity in treatment effects following an approach employed by other scholars (Brand et al., 2014; de Novais & Spencer, 2019). Figure 1 presents the distribution of UT courses completed before transferring among the lateral and reverse transfer samples. The figure shows that zero is the most common frequency for the number of UT courses taken (or rather, not taken), comprising 31% of lateral transfers and 58% of the reverse transfer sample. Taking only one course is the second most common frequency (27% of lateral transfers, 26% of reverse transfers), and frequencies decline as the values increase successively. On average, lateral transfers completed 1.66 UT courses while reverse transfers completed 1.12 (see Table 1). Because there are considerable differences between lateral and reverse transfer students who complete higher numbers of UT courses, I employ several dichotomous indicators of UT course-taking to maintain some consistency in how the treatment is defined between the two samples. These treatment dummies indicate whether the student completed only one UT course, two courses, three courses, or four or more—each respectively coded as one, versus completing no UT courses, coded as zero. The propensity score methodology follows the approach employed by Boatman and Soliz (2018) in which I generate balance between the treated and nontreated students using inverse probability weighting. The weights produced by inverse probability weighting are given as 1/[MATH] for the treated students and 1/[MATH] for the untreated students, where [MATH] refers to the probability of completing UT courses (Murnane & Willett, 2011; P. R. Rosenbaum & Rubin, 1983; Wooldridge, 2010). Therefore, higher estimated propensities are assigned weights greater in magnitude and the subsequent weighting of the samples generates a more observationally similar control group of students who did not complete the transferrable coursework. The propensity scores used for this procedure are produced in multiple stages for each dichotomous indicator of UT course-taking. In the first stage, I begin by using logistic regression to estimate the probability of taking UT courses as follows:[MATH](1) where [MATH] is the treatment—completing UT coursework—for student i who first enrolled at college c in year t, and [MATH], a vector of time-invariant, student-level covariates that are exogenous to the period of transfer. These covariates include sociodemographic characteristics such as race, gender, age, and Pell Grant eligibility. Additionally, [MATH] is a vector of factors related to academic achievement including indicators for remedial coursework, whether or not they took the ACT exam for admission, and their first semester grade point average. Finally, [MATH] accounts for several enrollment-related factors, including part-time enrollment status, living status as an on-campus resident or not, and dummies of academic major disciplines (i.e., social sciences and humanities, education, engineering, etc.). The next variables in the equation are college and year fixed effects. The college fixed effects, [MATH], are introduced through a series of dummy variables representing the Ohio institutions where students were first enrolled. Because the sample includes students who enrolled at different times, I also include [MATH], which represents fixed effects for their first year of enrollment. I estimate the average treatment effect of UT course-taking on the degree completion outcomes using a "doubly-robust" estimator in which fixed effects regression models incorporate the inverse probability weights (Cerulli, 2015; Wooldridge, 2010). The primary benefit of this approach regards omitted variable bias. Once the weights are applied, there is greater balance between the treated and untreated groups according to observed factors pre-transfer, but there may be other potential confounding variables that may affect degree completion post-transfer, which can be accounted for using a regression covariate adjustment. Indeed, Freedman and Berk (2008) argue that "weighting may lead to a substantial reduction in bias" (p. 400). Employing linear probability models, the general fixed effects model can be formally expressed as follows:[MATH](2) where [MATH] represents the dependent variable of interest: a dichotomous indicator for degree completion—specifically, the bachelor's degree for lateral transfers and the associate degree for reverse transfers. The outcome measures are equal to one if a student is observed to attain one of these degrees, and they are coded as zero otherwise. Given the different years in which students may have transferred, I observe degree attainment 2 years as well as 3 years post-transfer; but due to data limitations, the 3-year measure is estimated using a reduced sample that excludes the 2008 cohort. Although it is standard to observe degree completion within at least 4 years, this is unnecessary for most students moving from 4-year colleges, who leave their first institutions after earning a considerable amount of the credits needed to complete a degree program (see Table 1). The model is fitted separately for the samples of lateral transfers and the reverse transfers using several definitions for [MATH] as previously stated. These treatment measures are each, respectively, coded as one for students who completed and passed the requisite number of courses, compared with those who completed no UT courses who are coded as zero. I identify which courses are transferrable using the Transfer Assurance Guide Reporting System from the Ohio Department of Higher Education. The system facilitates the matching of "TAG'ed" courses that are offered at Ohio colleges and universities with their corresponding Ohio Articulation Number, the date in which the equivalency became effective, and the specific major area of study for which the course is articulated to transfer. Thus, I am able to identify whether and, if so, when each student enrolled in a course approved for universal transfer. For all models, [MATH] is the coefficient of interest and describes the relationship between the degree attainment outcomes with the relevant number of UT courses completed. The vector [MATH] includes the same covariates as aforementioned in Equation (1), but in this instance, [MATH] represents indicators of pre-transfer academic achievement such as their total pre-transfer grade point average, total pre-transfer credit hours completed, and an indicator of whether remedial coursework was taken before transferring. Additionally, [MATH] represents fixed effects for the second institution (or rather, the college where students chose to transfer), and [MATH] represents the year that students enrolled at their second institution (i.e., 2006, 2007, or 2008). Therefore, inclusion of college fixed effects accounts for unobserved, time-invariant differences between the institutions where students chose to matriculate. Such unobserved characteristics may include a number of factors concerning college quality—most often defined by institutional selectivity—which the literature shows may affect persistence and degree attainment outcomes (Alon & Tienda, 2005; Goodman & Cohodes, 2014; M. C. Long, 2008, 2010; Melguizo, 2008). Year fixed effects also account for differences over time that may affect the outcomes for all students in Ohio. For instance, if another statewide policy initiative were introduced in a given year, the fixed effects would control for its potential impact. To examine potential explanations for the relationship between UT course-taking and degree completion, I answer the second research question by repeating the aforementioned procedure with conditioned samples of transfer students. First, I condition subsamples to only include those who attained their degree—thereby facilitating the ability to examine other indicators that may explain possible differences between treated and untreated groups. These outcomes include a measure for the number of terms enrolled—to determine the timing of degree completion—and I also use an indicator for the number of credits earned, which I use to determine excess credit accumulation. In this effort, I am able to examine whether the treatment improves the efficiency of the transfer process. I also hypothesize that if UT course-taking protects the loss of credits by ensuring that they are applied to fulfill prerequisite requirements (and not an elective requirement), treated students may be more likely to persist in their intended major. Therefore, I also employ conditioned subsamples of students who concentrated in a particular academic program of study. Because it is difficult to confirm each undergraduate's actual degree intentions, I define concentrators according to two standards: first, that they completed at least three or more baccalaureate-level courses in a given academic major before transferring (Jenkins & Cho, 2013; Jenkins & Weiss, 2011; Moore & Shulock, 2011), and second, that they declared the same discipline as their program of study before transferring. Specifically, the analysis only focuses on students with a concentration in business, education, and engineering majors. I restrict this investigation to students in these programs because, compared with coursework in other disciplines (i.e., humanities, life sciences, and social sciences), it is less likely that courses in a professional degree program would be taken to fulfill a distribution or elective requirement. As a result, estimates should capture post-transfer differences among a similarly motivated group. The specific outcomes of interest include indicators of academic major persistence, such as declaring the same major post-transfer, completing multiple upper-division courses in the same major, and completing a degree in the same major within 2 or 3 years. For this analysis, I compare students who completed any number of UT courses in the same subject as their chosen academic major with those who completed none. Table 1 summarizes the descriptive statistics for the two analytic samples of transfer students juxtaposed with the full sample of students who first enrolled in Ohio 4-year colleges between 2003 and 2007. The first column presents characteristics for all students, and successive columns describe the characteristics, behavior, and outcomes for the two samples of transfer students. Comparatively, lateral transfer students were more likely to be female, White, and living in an on-campus residence. In contrast, students who transferred in reverse were more likely to come from groups that are often disadvantaged in higher education: A larger proportion of these students identified as Black, Hispanic, or Native American; a larger proportion was eligible for the Pell Grant; and more of these students enrolled part-time and lived off-campus. Importantly, reverse transfers were less academically prepared compared with lateral transfer students. In the first semester of enrollment, a larger proportion of reverse transfers enrolled in remedial coursework, they were less likely to declare an academic major program of study, and a higher proportion of these students had a grade point average of 2.6 or lower. Prior to enrolling at a new institution, lateral transfers also accumulated more credits in total: 60 credits on average, which is nearly half of the credits needed for a bachelor's degree. This suggests that lateral transfers likely completed most general education requirements and were much further along in their academic progress when they decided to move to a new institution. Lateral transfers were also more likely to have a higher cumulative grade point average compared with reverse transfers. These differences between the characteristics of lateral and reverse transfers are consistent with the trends uncovered by other scholars in their analyses of nationally represented cohorts (Goldrick-Rab & Pfeffer, 2009; Kalogrides & Grodsky, 2011). For this reason, although the present study's findings are generalized for the Ohio students who transferred via one of the aforementioned pathways, the findings also have external validity beyond the state context. Table 2 presents effects of completing UT courses on bachelor's degree attainment within 2 and 3 years of transferring. The table disaggregates estimates of UT course-taking according to a comprehensive measure representing effects for students who completed any number of these courses, followed by estimates for more restricted versions of the treatment that represent effects for students who took one, two, or three courses, in addition to those who took four or more courses. As reported in Equation (2), the models include post-transfer cohort year fixed effects in addition to post-transfer institutional fixed effects. All models also control for a vector of covariates to reduce residual variance. Table 2 first displays results for lateral transfers in the first four columns and then results for reverse transfers in the last four columns. For each respective group, I also present the estimates first using the unweighted samples followed by the samples weighted by the propensity scores. Table 2 shows that, among reverse transfers, there is no measurable difference in degree attainment according to the number of UT courses taken. Although the estimates among reverse transfers is often positive for most definitions of the treatment, differences in the probability of associate degree completion between students who completed UT courses and those who did not were not statistically different from zero. Indeed, among all reverse transfers, the probability of attaining an associate degree is rare: As presented in Table 1, only 8.2% of reverse transfers are found to do so. In contrast, there is more success of the treatment among lateral transfers, but the magnitude of the estimated effect varies. There are consistently positive estimates of the association between completing any number of UT courses with degree attainment both within 2 years and 3 years. The estimates are largest in the unweighted samples and attenuate in the weighted samples—suggesting that omitted variable bias is reduced. The weighted results suggest that, on average, there is a positive 1.9 percentage point difference in the probability of attaining a bachelor's degree within 2 years between transfer students who completed any number of UT courses compared with those who did not, but the difference in degree completion within 3 years is 4.5 percentage points. The lateral transfer results appear to be concentrated among the sample of students who completed multiple UT courses. In the unweighted samples, there are consistently positive estimates of the association with degree attainment for most measures of the treatment with exception to those who completed only one course. In fact, weighted estimates suggest that there is a small, negative association between taking only one course and completing the bachelor's degree within 2 years. Across the other treatment measures, the associations between UT course-taking and degree completion within 2 years is not significant across all specifications in the weighted models. But when estimating degree completion within 3 years, the effect of taking two, three, and four or more UT courses is positive and statistically significant. Whereas, relative to those who did not complete UT courses, there is an 8-percentage point difference in 3-year degree attainment from those who completed four or more UT courses; but the difference was 3.7 percentage points and 4.6 percentage points among the treated groups who completed only two or three courses, respectively. There are important limitations to the propensity score analysis. Compared with other quasi-experimental approaches, which may exploit exogenous variation in the assignment to treatment and control conditions, propensity scores cannot similarly identify the causal effect of completing UT courses as easily. The primary limitation of the propensity score procedure concerns the assumption of conditional independence. In other words, the propensity score–generated control group should not differ from the treated group so that an unobserved, omitted variable would have a confounding effect on the outcomes; in this, the treatment assignment would be considered to be ignorable. Because this assumption is impossible to prove, the propensity score approach does not completely solve the problem of selection bias. Though I cannot test the assumption of conditional independence, I conduct a sensitivity analysis using Mantel-Haenszel bounds to assist in the effort of determining the degree that estimates of the UT course-taking effect are sensitive to an unobserved confounder. In this analysis, I estimate the magnitude that an observed factor must be to undermine the estimated effect of the treatment (DiPrete & Gangl, 2004; P. R. Rosenbaum, 2002). Specifically, I use Mantel-Haenszel bounds, a derivative of Rosenbaum bounds that are used for dichotomous outcomes using the "mhbounds" command in Stata (S. O. Becker & Caliendo, 2007). Table 3 presents the results from my sensitivity analysis. For parsimony, I only present the results for estimates of the treatment in which multiple UT courses were taken among the lateral transfer sample. In Table 3, I present a range of odds ratios representing gamma ([MATH]), or the odds of taking a UT course. Each range of gamma that is statistically insignificant represents the "kill zone," or rather, the point in which the odds of taking a UT course is attributed to an unobserved variable, U, which may have an undue effect on estimates of its impact (P. R. Rosenbaum, 2002). Table 3 compares the range of gammas for the effect of each classification of the treatment on 3-year bachelor's degree completion between gamma values represented as odds ratios between 1.00 and 3.00 in increments of 0.10. The results suggest that the magnitude of U would need to be considerably large in order to have an effect on UT estimates of bachelor's degree completion. For the effect of taking two UT courses, the odds of affecting the treatment would need to be more than double (OR = 2.7). But for the effect of taking three and four or more courses, the study is not sensitive to a bias that would more than triple the odds. Thus, relative to the magnitude of most covariates used to predict the treatment (see Appendix Table A1), the necessary size of a potential U in this sample is quite large. Although I am not able to demonstrate that conditional independence has been achieved, given the balance between the treated and untreated conditions, I am confident that the propensity score approach has helped reduce some degree of selection bias. Therefore, estimates of the UT course-taking effect when weighted by propensity scores are an improvement to the results using the unweighted sample, but the results should not be interpreted as causal estimates of the treatment effect. There are several plausible explanations for the UT course-taking effect, which I examine further using conditioned samples. First, I explore whether the positive association between UT course-taking and bachelor's degree attainment could be attributed to improvements in the efficiency of the transfer process. In this effort, Table 4 presents estimates of completing UT courses on measures of excess credit accumulation and the number of enrolled terms for a subsample of lateral transfers who attained a bachelor's degree. If UT course-taking protects the loss of credits because coursework is misaligned between institutions, it is plausible that treated students who completed their degree program would be likely to do so by accruing fewer excess credits. The results suggest, however, that there is no statistically significant difference in the number of credits earned. Nonetheless, among bachelor's degree earners, there is a difference in the number of enrolled terms between lateral transfers who completed more than one UT course and those who completed none. This finding suggests that completing multiple UT courses before transferring may have helped lateral transfers to finish in a shorter period of time. UT course-taking may also positively affect transfer students' post-transfer success if preventing challenges to credit applicability facilitates the persistence in a chosen academic program of study. In Table 5, I present the effects of completing any number of UT courses in each of three academic majors on four dichotomous measures of program persistence. Regarding the probability of enrolling in upper-division coursework for the same subject, the results show that there were considerable and statistically significant differences between students who took UT courses and those who did not—ranging from 23.5 to 29.4 percentage points. However, only lateral transfers who took UT courses in engineering were also more likely to declare a major in the same subject matter post-transfer. But most important, compared with non-UTC course takers, the probability of bachelor's degree attainment, within 2-years post-transfer, was approximately 10 to 20 percentage points higher for lateral transfers who completed UT courses in each respective field. These findings, therefore, suggest that the UT course-taking effect is likely to benefit students to move seamlessly through their intended program of study. Although there is a growing body of literature regarding the effectiveness of statewide articulation, past studies have not examined whether such agreements may improve the post-transfer success of students who transfer laterally or in reverse: nonlinear transfer directions that are both associated with considerable credit loss. Ohio is one of several states with an articulation approach that facilitates the universal transfer of credits for coursework that can be completed at any public institution. But do students who transfer from 4-year colleges have improved post-transfer success if they complete transferable courses before doing so? The evidence from this study suggests that the answer varies for students who transferred to another 4-year college compared with those who transferred to community colleges. I find that there was no relationship between UT course-taking with degree attainment among students who transferred in reverse. Notably, because reverse transfers are more likely to be academically low-achieving, the probability of degree attainment is small in general for this population of students. Descriptive evidence shows that few reverse transfers were able to complete an associate degree program. Structural barriers including access to adequate advising and degree pathways are common disadvantages for many students who enroll in the community college sector (J. E. Rosenbaum et al., 2006). For this reason, although moving to a community college is an accessible and affordable option, it may not provide a viable degree pathway for many students who initially enroll in 4-year colleges and universities. Certain limitations of the present study may also preclude a better understanding of the UT course-taking effect for reverse transfers. One important limitation of the study concerns the relatively brief number of years observed post-transfer, which ranges depending on the timing of transfer. Given the aforementioned academic and structural challenges, the time-to-degree for reverse transfers may take longer than expected. It is also difficult to ascertain the specific degree aspirations for students transferring in reverse. Given the multiple degree options for students who enroll in community colleges, it is unclear if these students are actually aspiring to an associate degree. Other scholars have noted that many students transfer in reverse namely to "drop-in" in order to complete a small number of credits before returning to complete a bachelor's degree (Adelman, 2005). Although I find no evidence that reverse transfers in my sample returned to 4-year colleges during the observed period, it is possible that they did so at a later period of time. In contrast, the findings pertaining to lateral transfers are particularly helpful to further advance the theoretical basis for evaluating articulation policies in future research. This study applied a framework that articulation policies moderate diminished academic momentum that occurs when students lose credits due to curricular misalignment and credit applicability. Although the study does not directly test for the effects of credit transfer—or rather, the loss of credits—the subgroup analysis enabled a way to better understand why articulation policies may be effective in promoting degree attainment. First, the findings suggest that UT course-taking has a positive association with degree attainment among lateral transfers, but estimates of this relationship vary according to the number of courses that were completed. I find that, on average, the probability of bachelor's degree completion within 3 years was 3.7 to 8 percentage points higher for students who completed two or more of these courses. The likelihood of attaining a degree was highest among those who completed the most transferrable coursework, but the relationship was not statistically significant among students who only completed one UT course. This finding aligns with the theory that articulation policies help maintain academic momentum post-transfer. But specifically, such approaches are only advantageous if they help preserve a critical mass of credits in the transfer process. National studies suggest that students who transfer laterally lose credits equivalent to three college-level courses, on average (Simone, 2014). Thus, preserving the credits for only one UT course may be insufficient to positively affect post-transfer outcomes. A student may potentially save more time and money if they can avoid retaking two or more courses, while the costs of retaking only one course may have fewer consequences. Second, findings from the subgroup analysis suggest that among lateral transfers who completed a bachelor's degree, those who took UT courses before transferring were enrolled for a fewer number of terms in total. Given the improved time-to-degree, students who completed UT coursework may have benefitted from the preservation of credits in the process of transferring between institutions. This may have occurred if the policy helped students finish their degree program without incurring a penalty of enrolling for additional terms and enduring the financial burden to do so. Therefore, completing UT courses may be particularly helpful to address the issues of curricular misalignment: ensuring that students would not need to retake coursework and consequently reducing the time-to-degree. Third, the findings suggest that lateral transfers who completed UT courses may have a higher probability of bachelor's degree attainment given a seamless transition into their initial academic major. Because transferring credits for coursework at the department level is often a critical barrier for transfers (Hodara et al., 2016), I surmise that the credits for UT coursework were more likely to be accepted for students to advance in their chosen program of study. The results demonstrate that students who completed transferable courses in their declared major had a higher probability of completing multiple upper-division courses in the same program and completing a bachelor's degree in that program. Considering the challenges of curricular applicability, academic major persistence is far more likely if UT courses were applied as meeting prerequisite requirements as opposed to counting merely as elective credit. This study also makes several important contributions to the literature. In particular, the study further advances the necessity to consider the impact of specific approaches to achieve statewide articulation and provides additional evidence in support of the notion that post-transfer success is conditional on whether or not students complete the specific mechanisms employed by such policies (Spencer, 2019). As demonstrated by a growing number of studies, students within states with these policies have better outcomes when they transfer with a transferrable associate degree (Crook et al., 2012; Kopko & Crosta, 2015) or transferrable coursework (Boatman & Soliz, 2018). The acquisition of designated transfer mechanisms is, therefore, essential to post-transfer success. Additionally, the results also demonstrate that articulation agreements can support students transferring in multiple directions, not only those transferring vertically from community colleges. This finding carries important practical significance given the growing population of students who are moving between institutions and the severity of credit loss that many of these students may suffer. While most approaches to achieve articulation are intended to explicitly support students transferring from community colleges, not all will accommodate those moving in nonlinear directions. For this reason, policymakers should carefully examine whether or not their state's current transfer initiatives adequately serve all forms of student mobility. It is especially important to further examine articulation approaches that may specifically support students transferring in reverse compared with those that only appear to help lateral and vertical transfers. Importantly, there are stark differences in the characteristics of students transferring in these directions. Concerning vertical transfer, there is a robust literature demonstrating that the probability of moving from a community college to a 4-year institution varies considerably by income and race/ethnicity, whereas groups from historically disadvantaged groups are found to be less likely to transfer (Bowen et al., 2009; Bradburn et al., 2001; Dougherty & Kienzl, 2006; Wang, 2012; Whitaker & Pascarella, 1994). As previously noted, there are similar sociodemographic disparities between the populations of students who engage in lateral versus reverse transfer. Given the apparent disparities in student mobility patterns, whether or not articulation agreements can improve degree completion inequities is a serious concern. Some scholars have argued that, unlike many policies for the K–12 education sector, statewide articulation agreements rarely emphasize accountability for improving the outcomes of students who have been historically disadvantaged in higher education (Chase, 2014). In addition to institutional accountability, perhaps students who transfer in reverse would benefit from access to advising at both their sending and receiving institutions. Since reverse transfer students are less likely to concentrate in a major before moving to another institution, guidance should begin with greater clarity concerning careers and optimal pathways to degree programs—including associate-level degrees—that would facilitate access to these options. Perhaps one policy consideration could include the expansion of current efforts under the Credit When It's Due initiative. Starting in 2012 as a partnership between national foundations and public institutions across 12 states, Credit When It's Due has been an influential force in designing reverse transfer credit articulation policies. However, the efforts under this initiative have mainly sought to encourage vertical transfers to retroactively earn an associate degree (Taylor & Giani, 2019). Similar efforts could also help students who first enrolled in a 4-year institution to seamlessly complete the requirements for an associate degree if they move to a community college. In this, states may significantly reduce the number of students who enroll in higher education without completing a credential. Policymakers must also reconsider the functionality and intentions of current policy efforts to achieve statewide articulation. Certainly, reverse transfers may be better supported by another type of articulation policy approach—perhaps one that emphasizes the completion of general education coursework like the Ohio Transfer Module. Yet as evidence from Boatman and Soliz (2018) and the present study show, far too few students appear to complete transferrable mechanisms regardless of the transfer pathway they experience. Therefore, reinforcing the transferability of credits that would meet requirements for all college students, regardless of the sector for their first institution, would greatly improve the challenge of credit loss.