{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3nrRKc0HIEV"
   },
   "source": [
    "#### Final Proposal EDF 6938: Natural Langauge Processing\n",
    "\n",
    "### Can a Human Scoring System Be Replaced by an Automated Scoring System?: A New Approach to Teacher Evaluation\n",
    "\n",
    "> #### Author: Hyojong Sohn\n",
    "> #### Date: December 6th, 2022\n",
    "> #### Email: hsohn@ufl.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXmw_S3MHpPW"
   },
   "source": [
    "#### 1. Introduction \n",
    "\n",
    "> #### Problem Statement\n",
    "> Teachers are considered the most crucial school factor for supporting students’ needs and improving their learning, and this is especially true for those students scoring below grade level on state and national assessments, including those with disabilities (Myers et al., 2022). Researchers have shown that students with disabilities consistently are the lowest performing subgroup of students on the National Assessment of Education Progress and most likely to have poor post school outcomes, thus, efforts to improve teacher quality for these students is essential to their success (Brownell et al., 2020; Jones et al., 2022). The Institute of Education Sciences (IES) has taken efforts to improve teaching quality by funding professional development (PD) research to augment teacher changes in evidence-based practices (EBPs; National Academies of Sciences, Engineering, and Medicine 2022). Classroom observations play an essential role in evaluating and improving teacher practice in PD research. However, observational research has consistently revealed several challenges due, in part, to the current human-based scoring system. One, human-rater scoring is a costly process; it heavily relies on human resources manually assessing teaching practices. To train raters, PD researchers may spend valuable time and resources on rater recruitment and training, regular calibration, and reassessment for quality control (Connor et al., 2014; Demszky et al., 2021). These efforts are often necessary to reduce such threats as rater bias and severity to the temporal stability of scores and fairness (Park et al., 2015; Wind & Guo, 2021). This cost of human raters decreases the accessibility of observational measures. Two, current human-mediated feedback systems, based on video-recorded observations in research, impede providing teachers with timely performance feedback. If researchers fail to provide teachers with immediate feedback, teachers have more time to repeat errors and less time to implement target practices (Taylor et al., 2022). Three, live observations are more common in typical school practice, thus, differences between the conditions of observations conducted in research and practice may diminish the generalizability of research results (Liu & Cohen, 2021). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr6LyhpnH1Km"
   },
   "source": [
    "#### 2. Related Work \n",
    "\n",
    "> To address challenges associated with human-mediated observation systems, recent advances in machine learning techniques provide data-driven solutions. In recent observational research, a convolutional neural networks model was able to recognize the classroom dialogue in seven categories (e.g., prior-known knowledge, agreement) with an acceptable level of accuracy (the overall precision: 0.688; Song et al., 2021). Liu and Cohen (2021) also found that a natural language processing algorithm that was embedded in the text-as-data methods successfully detected three latent traits (i.e., classroom management, interactive instruction, teacher-centered instruction) aligned with existing observation instruments, such as the Framework for Teaching (Danielson, 2013) and Protocol for Language Arts Teaching Observations (Grossman et al., 2014). In this study, we will examine the possibility of automated classification methods of transcribed lessons in general and special education settings. Our analytic framework attempts to provide teachers with timely performance feedback with automated coding. The following research questions guided this study:\n",
    "\n",
    "> 1.\tDoes the classification system achieve comparable accuracy to human-expert judgment?\n",
    "> 2.\tDoes the linguistic evidence that is identified as deterministic features by the classification algorithm associate with teachers’ latent abilities in effective special education instruction (e.g., explicit instruction, reading evidence-based practice)?\n",
    "> 3.\tWhat implications do the classification models have for effective special education practice?\n",
    "\n",
    "> #### Theoretical Framework\n",
    "> We adopted the computational psychometrics framework developed by von Davier and colleagues (2021; see Figure 1) to compare performance between human-coded and computer-coded labels. Their framework blends both top-down and bottom-up approaches to integrate traditional psychometric methods (e.g., classical test theory, factor analyses) and computational methods (e.g., machine learning algorithms) for observational data. First, in the top-down approach, we designed an observation protocol based on the principles of effective special education instruction (see Table 1) and generated the micro- and macro-level evidence using the protocol. In other words, human raters analyzed teacher cues at the micro level (e.g., “Let me show you how to decode this word,” “Find the main idea with your partner”) and labeled them into ten categories at the macro level (e.g., modeling, practice opportunities, feedback, decoding, prefix, suffix). Once human-coded measures are collected, the bottom-up approach is applied to generate computer-coded labels using supervised machine learning methods. Lastly, in the psychometric models, kappa scores will be used to compare the prediction accuracy of computer-coded labels and human-coded ones. Factor analyses will also be conducted to examine whether the constructs of computer-coded labels are associated with teachers’ latent instructional factors in effective special education instruction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HYXWwxRH7_E"
   },
   "source": [
    "#### 3. Methods \n",
    "\n",
    "> #### Overview of Research Design and Methodology\n",
    "> In Project Coordinate (PC), a Goal II professional development innovation funded by the IES, we developed the Project Coordinate Observation Protocol (PC-OP) to capture teachers’ use of evidence-based instruction in tiered instruction. This instrument has been used to assess well-researched principles of instruction that support the effective implementation of EBPs: explicit instruction (e.g., modeling, explanation, practice opportunities; Johnson et al., 2019), and responsiveness to student learning (e.g., response affirmation, constructive feedback; Connor et al., 2011; Doabler et al., 2015). It also assesses the amount of time teachers spend implementing evidence-based word study, morphological awareness, and summarization strategies (Katz & Carlisle, 2009). Human-coded measures on the PC-OP were previously validated in Kane (2006)’s measurement framework (Pua et al., 2020; Sohn et al., 2021). In this study, we will evaluate psychometric properties of computer-coded measures generated by automated classification models in the computational psychometrics framework. \n",
    "\n",
    "> #### Participants\n",
    ">  During the 2019-2020 school year, participants were all from Project Coordinate and were 4th grade teachers from 15 schools in the southwestern United States. Eight treatment schools included 14 general education teachers (GETs) and 8 special education teachers (SETs), and seven control schools included 14 GETs and 8 SETs. \n",
    "\n",
    "\n",
    "> #### Data \n",
    "> We collected 176 video-recorded pre-intervention lessons and 90 video-recorded post-intervention lessons. Among them, 12 videos will be randomly selected and transcribed. As one transcript (video) includes 100 to 300 cues, 12 transcripts would have a substantial amount of data set (at least 1200 cues) to adopt and conduct machine learning analyses.  \n",
    "\n",
    "> #### Data Analysis \n",
    "> As supervised machine learning requires labeled training data, it is considered more powerful than unsupervised machine learning, which deals with unstructured data (von Davier et al., 2021). Thus, we will adopt supervised machine learning methods, specifically classification models, to analyze our structured data with human-coded measures (i.e., the target outcomes). Before applying the machine learning methods, we will perform several preprocessing steps, such as tokenization, part-of-speech tagging, and stop-word filtering, to increase the accuracy of the classification performance. Then, we will conduct the following four stages of a basic machine learning analysis framework: data processing, model development, model learning, and model evaluation. First, in the data processing stage, we will examine the use of two vectorization approaches (i.e., count-based and contextual vectorization) to effectively capture the semantic representation of the teacher discourse. Second, in the model development and learning stages, we will use four classification models (i.e., logistic regression, Naïve Bayes, support vector machines, multi-layer perceptron [MLP]) using a one-vs-rest (i.e., one-vs-all) approach in order to deal with a multi-class classification problem given a finite number (n = 10) of label categories (Glare, 2020). Lastly, in the model evaluation stage, we will evaluate the performance accuracy of the four models with the commonly adopted classification performance measures (e.g., accuracy, f1-score, the area under the receiver operating characteristic curve [AUC-ROC], kappa scores) with the 5-fold cross-validation. This system will enable a computer to classify the teacher discourse by learning the implicit and explicit decisions made by human judgement, and automatically applying the rules to unseen data (e.g., the teacher discourse data that are not coded by human raters).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxhsjBDDIDIH"
   },
   "source": [
    "#### 4. Analysis Demonstration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQAGOovRHjvx"
   },
   "source": [
    "##### 4.1. Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gKqprgOWHCOG"
   },
   "outputs": [],
   "source": [
    "# Import all the library that is necessary for your analysis \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Classification Models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Classification performance measures\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score #for f1 score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#Generate the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Create a heat map\n",
    "import seaborn as sns\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odZPjAudIY3P"
   },
   "source": [
    "##### 4.2. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd5d-suXHgIs"
   },
   "outputs": [],
   "source": [
    "input_dir = './'\n",
    "filename = 'PC06-03-22.xlsx'\n",
    "\n",
    "dat = pd.read_excel(input_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(dir_='./'):\n",
    "    dat= pd.read_excel('PC06-03-22.xlsx')\n",
    "    dat = dat.fillna(0)\n",
    "\n",
    "    dat[['ESI1', 'ESI2', 'ESI3', 'ESI4', 'ESI5', 'ESI6',\n",
    "           'EBP4', 'EBP5', 'EBP6', 'EBP10']] = dat[['ESI1', 'ESI2', 'ESI3', 'ESI4', 'ESI5', 'ESI6',\n",
    "           'EBP4', 'EBP5', 'EBP6', 'EBP10']].replace({\"x\":1})\n",
    "\n",
    "    short_dat = dat.iloc[:399] \n",
    "\n",
    "    short_dat['Dialogue'] = [entry.lower() for entry in short_dat['Dialogue'].astype(str)]\n",
    "    short_dat['Dialogue']= [word_tokenize(entry) for entry in short_dat['Dialogue']]\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(short_dat['Dialogue']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english'):# and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        short_dat.loc[index,'text_final'] = str(Final_words)\n",
    "    return short_dat, dat.iloc[:399]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_dat, og_dat = import_data()\n",
    "short_dat = short_dat.iloc[1:] #remove the second row in the excel file\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "Tfidf_vect = TfidfVectorizer(max_features=500)\n",
    "Tfidf_vect.fit(short_dat.text_final)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "X = short_dat.text_final.values\n",
    "y = short_dat['ESI3'].values  #item that I would like to analyze\n",
    "y = list(y)\n",
    "\n",
    "lr_acc = []\n",
    "nb_acc = []\n",
    "svm_acc = []\n",
    "nn_acc = []\n",
    "\n",
    "lr_f1 = []\n",
    "nb_f1 = []\n",
    "svm_f1 = []\n",
    "nn_f1 =[]\n",
    "\n",
    "lr_auc = []\n",
    "nb_auc = []\n",
    "svm_auc = []\n",
    "nn_auc =[] \n",
    "\n",
    "lr_kappa = []\n",
    "nb_kappa = []\n",
    "svm_kappa = []\n",
    "nn_kappa =[] \n",
    "\n",
    "result_df =[]\n",
    "\n",
    "for train, test in skf.split(X, y):\n",
    "    y = np.array(y)\n",
    "    Train_X, Test_X = X[train], X[test]\n",
    "    Train_Y, Test_Y = y[train], y[test]\n",
    "    \n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "\n",
    "    # fit the training dataset on the logistic regression classifier\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(Train_X_Tfidf,Train_Y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_lr = lr.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(\"Logistic Regression Accuracy Score -> \",\n",
    "          accuracy_score(predictions_lr, Test_Y))\n",
    "    print(\"LR F1 score -> \", f1_score(predictions_lr, Test_Y))\n",
    "    print(\"LR AUC-ROC score -> \", roc_auc_score(Test_Y, predictions_lr))\n",
    "    print(\"LR Kappa score -> \", cohen_kappa_score(predictions_lr, Test_Y))\n",
    "        \n",
    "    lr_acc.append(accuracy_score(predictions_lr, Test_Y))\n",
    "    lr_f1.append(f1_score(predictions_lr, Test_Y))\n",
    "    lr_auc.append(roc_auc_score(Test_Y, predictions_lr))\n",
    "    lr_kappa.append(cohen_kappa_score(predictions_lr, Test_Y))\n",
    "    \n",
    "    # fit the training dataset on the NB classifier\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(\"Naive Bayes Accuracy Score -> \",\n",
    "          accuracy_score(predictions_NB, Test_Y))\n",
    "    print(\"NB F1 score -> \", f1_score(predictions_NB, Test_Y))\n",
    "    print(\"NB AUC-ROC score -> \", roc_auc_score(Test_Y, predictions_NB))\n",
    "    print(\"NB Kappa score -> \", cohen_kappa_score(Test_Y, predictions_NB))\n",
    "    \n",
    "    nb_acc.append(accuracy_score(predictions_NB, Test_Y))\n",
    "    nb_f1.append(f1_score(predictions_NB, Test_Y))\n",
    "    nb_auc.append(roc_auc_score(Test_Y, predictions_NB))\n",
    "    nb_kappa.append(cohen_kappa_score(predictions_NB, Test_Y))\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=2, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(\"SVM Accuracy Score -> \",\n",
    "          accuracy_score(predictions_SVM, Test_Y))\n",
    "    print(\"SVM F1 score -> \", f1_score(predictions_SVM, Test_Y))\n",
    "    print(\"SVM AUC-ROC score -> \", roc_auc_score(Test_Y, predictions_SVM))\n",
    "    print(\"SVM Kappa score -> \", cohen_kappa_score(Test_Y, predictions_SVM))\n",
    "    \n",
    "    svm_acc.append(accuracy_score(predictions_SVM, Test_Y))\n",
    "    svm_f1.append(f1_score(predictions_SVM, Test_Y))\n",
    "    svm_auc.append(roc_auc_score(Test_Y, predictions_SVM))\n",
    "    svm_kappa.append(cohen_kappa_score(predictions_SVM, Test_Y))\n",
    "    \n",
    "    # Classifier - Algorithm - multiperceptron neural network classifier (MLP)\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    nn = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(3, 2), random_state=1)\n",
    "    nn.fit(Train_X_Tfidf,Train_Y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_nn = nn.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(\"NN Accuracy Score -> \",\n",
    "          accuracy_score(predictions_nn, Test_Y))\n",
    "    print(\"NN F1 score -> \", f1_score(predictions_nn, Test_Y))\n",
    "    print(\"NN AUC-ROC score -> \", roc_auc_score(Test_Y, predictions_nn))\n",
    "    print(\"NN Kappa score -> \", cohen_kappa_score(Test_Y, predictions_nn))\n",
    "    \n",
    "    nn_acc.append(accuracy_score(predictions_nn, Test_Y))\n",
    "    nn_f1.append(f1_score(predictions_nn, Test_Y))\n",
    "    nn_auc.append(roc_auc_score(Test_Y, predictions_nn))\n",
    "    nn_kappa.append(cohen_kappa_score(predictions_nn, Test_Y))\n",
    "    \n",
    "\n",
    "    compare = og_dat.iloc[test]\n",
    "    compare['prediction_nn'] = predictions_nn # you can change this to check other models \n",
    "    result_df.append(compare)\n",
    "    \n",
    "print('\\n Average Prediction Accuracy for LR Accuracy: ', np.average(lr_acc))\n",
    "print('\\n Average Prediction Accuracy for LR F1 score: ', np.average(lr_f1))\n",
    "print('\\n Average Prediction Accuracy for LR AUC-ROC score: ', np.average(lr_auc))\n",
    "print('\\n Average Prediction Accuracy for LR Kappa score: ', np.average(lr_kappa))\n",
    "    \n",
    "print('\\n Average Prediction Accuracy for NB Accuracy: ', np.average(nb_acc))\n",
    "print('\\n Average Prediction Accuracy for NB F1 score: ', np.average(nb_f1))\n",
    "print('\\n Average Prediction Accuracy for NB AUC-ROC score: ', np.average(nb_auc))\n",
    "print('\\n Average Prediction Accuracy for NB Kappa score: ', np.average(nb_kappa))\n",
    "\n",
    "print('\\n Average Prediction Accuracy for SVM Accuracy: ', np.average(svm_acc))\n",
    "print('\\n Average Prediction Accuracy for SVM F1 score: ', np.average(svm_f1))\n",
    "print('\\n Average Prediction Accuracy for SVM AUC-ROC score: ', np.average(svm_auc))\n",
    "print('\\n Average Prediction Accuracy for SVM Kappa score: ', np.average(svm_kappa))\n",
    "\n",
    "print('\\n Average Prediction Accuracy for NN Accuracy: ', np.average(nn_acc))\n",
    "print('\\n Average Prediction Accuracy for NN F1 score: ', np.average(nn_f1))\n",
    "print('\\n Average Prediction Accuracy for NN AUC-ROC score: ', np.average(nn_auc))\n",
    "print('\\n Average Prediction Accuracy for NN Kappa score: ', np.average(nn_kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the confusion matrix\n",
    "cf_matrix = confusion_matrix(predictions_nn, Test_Y)\n",
    "\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = og_dat.iloc[test]\n",
    "compare['prediction_nn'] = predictions_nn # Change this to check other models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare[(compare['ESI4'] != compare.prediction_nn)] # Change ESI5 to check other indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting Data Out of Python\n",
    "compare[(compare['ESI4'] != compare.prediction_nn)].to_excel('./Excel.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('ESI3 Modeling\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Human-coded Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['0','1'])\n",
    "ax.yaxis.set_ticklabels(['0','1'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1e7Gt-BIhWL"
   },
   "source": [
    "#### 4. Results \n",
    "\n",
    "> We demonstrated the capacity of the classification models. Our preliminary analyses consisted of three transcribed lessons. The results showed that the MLP model showed relatively better classification performance across four models in predicting the decoding strategy category. The values for accuracy, F1 score, and AUC-ROC in the MLP model were 89.95, 73.13, and 82.08, respectively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHRLxsDAIlCr"
   },
   "source": [
    "#### 5. Conclusion and Discussion\n",
    "\n",
    "> Our findings support the possibility of one of the supervised algorithms, classification, to automatically score the teacher discourse. However, if the human-labeled training data includes any bias, the automated scoring system based on supervised learning could also be biased. To address this limitation, studies that evaluate fairness evidence need to be followed up.  \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
