{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Proposal EDF 6938: Natural Langauge Processing\n",
        "\n",
        "### Stude Edge: Student Feedback and Recommendation Rates\n",
        "> #### Author: Erin Mowry\n",
        "> #### Date: Dec. 6th, 2022\n",
        "> #### Email: enmowry@ufl.edu\n"
      ],
      "metadata": {
        "id": "m3nrRKc0HIEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Introduction \n",
        "\n",
        "> Study Edge is a tutoring company based in Gainesville, Fl. They provide review content for a variety of University of Florida classes, including live review sessions, study hours, and 1-on-1 tutoring. \n",
        ">\n",
        "> Like many other businesses, Study Edge sends out a survey to its customers at the end of every semester. Some of the information this survey collects includes open-ended feedback related to the quality of course materials and products. It also ask students to rate how likely they are to recommend the Study Edge product to a friend. This number is important because word of mouth is the main mode of advertisement for the company. \n",
        ">\n",
        "> This year, Study Edge is preparing to receive additional funding from its new parent company. This leads to the question of which problems should be tackled with these funds. Logically, the problems which should be addressed first are those which cause customers to have a low recommendation rate for the Study Edge product, since this costs the company new customers.\n",
        ">\n",
        ">The goal of this project is to identify the most salient problems leading to low recommendation rates. To accomplish this goal, I will use a Naive Bayes classifier to identify the top features associated with three different sets of customer feedback, grouped by recommendation rate. These features can then be used to guide spending decisions. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MXmw_S3MHpPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Methods \n",
        "\n",
        "> **I. Data Collection and Cleaning**\n",
        ">\n",
        "> The data were sent to me in an excel sheet containing the contents of google surveys sent every semester from Fall 2013 to Spring 2022. Each semester had its own sheet in the document, but some of the recent years were duplicated on other sheets. Although each sheet had the same heading, the responses in some cells clearly did not match the heading. \n",
        "> \n",
        "> In order to clean the data, the first thing I did was remove columns from each sheet that were responses to multiple choice questions, or some unidentifiable open-ended question that was not relevant to this project. This left me with colummns for the date of the survey response, the course code, the recommendation rating, and two to three columns of open-ended responses. I renamed the columns of the open-ended responses to \"Website/App Improvements,\" \"Non-App Improvements,\" and \"Other Comments.\" I then manually shifted comments into the appropriate column if they were not already aligned. I also removed the column containing course code because it was not relevant. \n",
        ">\n",
        "> Lastly, I had to manually go through and remove responses that were either entirely blank or duplicates of other responses. Most of the duplicates I removed were of longer responses that were easily identifiable as someone submitting the survey twice on accident. Others were duplicated one word responses, and I realized too late that I couldn't be confident whether these were duplicates from the same student or just many students saying that \"Nothing\" could be improved. In repeating this analysis, I would need to re-do this data cleaning process a bit more thoroughly. \n",
        ">\n",
        "> I ended up with n = 2377 survey responses to be used. \n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> \n",
        "> **II. Pre-processing**\n",
        ">\n",
        "> Upon uploading the excel sheet into Colaboratory, I had to remove a few random columns that came in with the document. I also had to fill blank cells with an empty string. The other initial task was to create a new column called 'feedback,' which combined the text inside the \"Website/App Improvements\" column and the \"Non-App Improvements.\" This was the larger text that I proceeded to normalize.\n",
        ">\n",
        "> My text normalization was based on the methods Dr. Shin provided in class. In the cleaning method, punctuation as well as contractions were removed. However, I did not remove digits as I thought references to time might be relevant in this context. Additionally, when I actually called the method on my feedback data, punctuation was not removed. I duplicated the punctuation removal section of the code as its own method and called it again, and the second time it worked. \n",
        "> \n",
        "> The other methods I used to pre-process the data were word tokenization, lemmatization, and stopwords. I used the NLTK resources for both the word tokenizer and lemamtizer. I also used the NLTK English stopwords set. However, after reviewing what the processed data looked like with stopwords removed, I decided that some of the default stopwords were actually relevant to my context and removed them from the list. The default stopwords that I kept in my feedback data are: 'more', 'less', 'should', 'not', and 'most.'\n",
        ">\n",
        "> After applying each of the methods described above to the feedback data, I was left with a new column of processed data, called 'p_feedback.' At this point, I created a column for the correct labels of each text. Each text with a recommendation rate of 10 was labeled as \"Perfect\" (n = 1520). Text with a recommendation rate of 8 or 9 was labeled as \"High\" (n = 612). All texts with a recommendation rate of 7 or below were labeled as \"Low\" (n = 245). Lastly, I rejoined the tokens in the 'p_feedback' column into one string of feedback. \n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ">**III. Text Vectorization**\n",
        ">\n",
        "> I chose to use an n-gram BOW vectorization method from SKLearn. With this tool, I would be able to test a range of n-grams in my model to see which had the best fit and most interpretable results. The n-gram range (a, b) works by creating all possible features of size a, a+1,..., b-1, b. Part of my data analysis was to determine which n-gram range was ideal for training the model. At the start of the project, BOW was the text vectorization method I felt most compfortable implementing independently. Additionally, the independence assumption of BOW vectorization seemed relevant given that my text was composed of answers to two different, somewhat unrelated survey questions. \n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> **IV. Model Selection**\n",
        ">\n",
        "> I selected my n-gram range after comparing the recall and f1-score of the model using various ranges. The ranges I initially tested were: (1,1), (1,2), (2,2), (1,3), (2,4), (2,5), and (1,5). In selecting the desired range, I wanted to prioritize better results on the Low data group, even at the expense of the other two groups. After considering these factors, and the interpetability of various n-gram sized features, I chose to use the n-gram range (2,4). The confusion matrix from the initial test of this range is below. \n",
        "\n",
        "Sklearn's score on testing data : 0.43487394957983194\n",
        "\n",
        "Classification report for testing data : \n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "        High       0.29      0.41      0.34       123\n",
        "         Low       0.17      0.41      0.24        49\n",
        "     Perfect       0.73      0.45      0.56       304\n",
        "\n",
        "    accuracy                           0.43       476\n",
        "\n",
        ">In most of the other models, values for recall and f1-score of the Low data were below 0.20. While the (2,5) range had comparable effectiveness to this model, I chose to use the (2,4) range for analysis in order to minimize the number of features being evaluated. \n",
        ">\n",
        ">\n",
        "> After completing the first round of data anlaysis with the n-gram range (2,4), I realized that the top weighted bi-gram features were not all interpretable. For example, I could not be sure whether \"practice problems\" refers to a need for *more* problems or *better* problems. \n",
        ">\n",
        ">    *Top 10 features in (2,4) model, Low group*\n",
        ">\n",
        ">         -7.9020\texam review    \n",
        ">         -8.0198\tpractice problem\n",
        ">         -8.0843\tstudy edge     \n",
        ">         -8.7129\treview video   \n",
        ">         -8.8465\tstudy hour     \n",
        ">         -9.0006\treview session \n",
        ">         -9.0006\tmore practice  \n",
        ">         -9.0006\tmore candy     \n",
        ">         -9.0006\tformula sheet  \n",
        ">         -9.0006\tend packet     \n",
        "   \n",
        ">\n",
        "> I did some more testing with n-gram ranges whose smallest feature was a tri-gram. I decided to proceed using the range (3,4) because the confusion matrix was very similar to the results I saw for the (2,4) range but this time the results would be more interpretable. For comparison, the confusion matrix for the model using the (3,4) range is below.\n",
        ">\n",
        "Sklearn's score on testing data : 0.4117647058823529\n",
        ">\n",
        "Classification report for testing data : \n",
        ">\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "        High       0.26      0.36      0.30       123\n",
        "         Low       0.16      0.35      0.22        49\n",
        "     Perfect       0.68      0.44      0.54       304\n",
        "\n",
        "    accuracy                           0.41       476\n",
        "\n",
        ">\n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> **V. Language Model**\n",
        ">\n",
        "> The base task of this project was to classify testing data by recommendation rate. Since the data is labeled categorically, I used a Multinomial Naive Bayes model to train the data. The model I used was the SKLearn NB model presented in class by Dr. Shin. I did not make any modifications to the model itself. In splitting my data into training and testing sets, I did add a parameter to have the samples stratified by label, as the data set was very skewed. \n",
        ">\n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> **VI. Analysis**\n",
        ">\n",
        "> The purpose of this project was to identify the features with the heighest weighted features for the Low recommendation rate data. Hypothetically, this information can be used to inform actions taken to improve the Study Edge product. To identify the most important features, I used the log proability of features provided by the SKLearn NB classifier. Once I have the list of features, all I need to do is identify the topics that seem to be causing low recommendation rates and pass them along to the company to act on. \n",
        ">\n",
        "> For this portion of the code, my limited knowledge of Python was making it difficult to print out the information I needed. I was grateful to find a method on Stack Overflow that magically called the data in the right way. The original source of the code I used can be found here: https://stackoverflow.com/questions/29867367/sklearn-multinomial-nb-most-informative-features\n",
        "\n"
      ],
      "metadata": {
        "id": "9HYXWwxRH7_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Analysis Demonstration "
      ],
      "metadata": {
        "id": "BxhsjBDDIDIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.1. Dependencies "
      ],
      "metadata": {
        "id": "vQAGOovRHjvx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKqprgOWHCOG"
      },
      "outputs": [],
      "source": [
        "# Import all the library that is necessary for your analysis \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import nltk \n",
        "nltk.download(['punkt', 'wordnet', 'omw-1.4', 'stopwords'])\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2. Code"
      ],
      "metadata": {
        "id": "odZPjAudIY3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Your code for the analysis will be provided here \n",
        "\n",
        "#import data\n",
        "filename = 'SE_feedback.xlsx'\n",
        "data = pd.read_excel(filename)\n",
        "\n",
        "data = data.drop(['Unnamed: 5'], axis = 1)\n",
        "data = data.drop(['n = 2377'], axis = 1)\n",
        "data = data.replace(np.nan, '')\n",
        "\n",
        "#create new column of combined data\n",
        "data['feedback'] = data['Website/App Improvements'] + ' ' + data['Non-App Improvements']\n",
        "\n",
        "#preprocess data using methods from class\n",
        "#and then apply them to a duplicate of feedback column so can restart if anything is weird.\n",
        "def cleaning(self):  #clean whole comment first\n",
        "\n",
        "    string = self\n",
        "    string  = string.lower() # step 1. lowercase\n",
        "\n",
        "    punc = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' \n",
        "    string  = string.strip(punc) \n",
        "\n",
        "    string  = string.replace(\"can't\", 'cannot') \n",
        "    string = string.replace(\"n't\", ' not')\n",
        "    string  = string.replace(\"'ll\", ' will')\n",
        "    string  = string.replace(\"'m\", ' am')\n",
        "    string = string.replace(\"he's\", \"he is\")\n",
        "    string = string.replace(\"it's\", 'it is')\n",
        "    #did not remove digits bc communicate relevant information in some comments\n",
        "    return string \n",
        "\n",
        "def tokenize(self):  #word tokenize second\n",
        "    import nltk\n",
        "    return nltk.word_tokenize(self)\n",
        "\n",
        "def stopwords(self):  #stop words 3rd, to minimize num words through lemmatizer\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    stop_words = set(stopwords.words('english')) - {'more', 'less', 'should', 'not', 'most', } \n",
        "    word_tokens = self\n",
        "    filtered_feedback = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  \n",
        "    filtered_feedback = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_feedback.append(w)\n",
        "    return filtered_feedback\n",
        "\n",
        "def lemmatize(self):  #lemmatize fourth bc requires tokens\n",
        "    import nltk\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in self]\n",
        "\n",
        "#Applying preprocess methods here\n",
        "data['p_feedback'] = data['feedback'].apply(cleaning) #processed feedback is copy of feedback initially\n",
        "data['p_feedback'] = data['p_feedback'].apply(tokenize) \n",
        "data['p_feedback'] = data['p_feedback'].apply(stopwords)\n",
        "data['p_feedback'] = data['p_feedback'].apply(lemmatize)\n",
        "\n",
        "\n",
        "#still has punc, so try to remove again lol\n",
        "def rem_punc(self):\n",
        "    punc = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    filtered_feedback = []\n",
        "    for w in self:\n",
        "        if w not in punc:\n",
        "            filtered_feedback.append(w)\n",
        "    return filtered_feedback\n",
        "\n",
        "data['p_feedback'] = data['p_feedback'].apply(rem_punc)\n",
        "\n",
        "#add labels\n",
        "data['labels'] = ''\n",
        "for i in range(len(data)):\n",
        "  if(data['Recommendation'].values[i] == 10):\n",
        "    data['labels'][i] = 'High' #'Perfect'\n",
        "  elif(data['Recommendation'].values[i] == 8 or data['Recommendation'].values[i] == 9):\n",
        "    data['labels'][i] = 'High'\n",
        "  else:\n",
        "    data['labels'][i] = 'Low'\n",
        "\n",
        "#rejoin to one string\n",
        "def rejoin(self):\n",
        "    return ' '.join(self)\n",
        "\n",
        "data['p_feedback'] = data['p_feedback'].apply(rejoin)\n",
        "\n",
        "#n-gram bow to vectorize \n",
        "bow = CountVectorizer(ngram_range=(3,4))\n",
        "X = bow.fit_transform(data['p_feedback'])\n",
        "\n",
        "y = data['labels']\n",
        "\n",
        "#Let's randomly shuffle and use 80% of the data as training and rest of it as testing \n",
        "#also tell it to stratify\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "\n",
        "clf_nb = MultinomialNB() # this is your new classifier \n",
        "clf_nb.fit(X_train, y_train) #let's fit the model \n",
        "y_hat = clf_nb.predict(X_test) #predit y_hat \n",
        "\n",
        "#precision, f-measure, recall\n",
        "sklearn_score_test= clf_nb.score(X_test,y_test)\n",
        "print(\"Sklearn's score on testing data :\",sklearn_score_test)\n",
        "\n",
        "print(\"Classification report for testing data : \")\n",
        "print(classification_report(y_test, y_hat))\n",
        "\n",
        "#results of how test data was categorized\n",
        "print('The testing labels were:')\n",
        "print(list(y_test))\n",
        "print('The predicted labels were:')\n",
        "print(list(y_hat))\n",
        "\n",
        "#n-gram probabilities\n",
        "print(list(bow.get_feature_names_out()))\n",
        "\n",
        "print(list(clf_nb.classes_))\n",
        "\n",
        "#The method below was found on Stack Overflow at https://stackoverflow.com/questions/29867367/sklearn-multinomial-nb-most-informative-features\n",
        "#Original method was only able to access one group, so I added class_num parameter to be able to access coefficients for each group of data\n",
        "#Magically this all worked and I dare not try to recreate it :)\n",
        "#This shows 100 highest and lowest weighted features side by side\n",
        "def show_most_informative_features(vectorizer, clf, class_num, n=100):\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "  coefs_with_fns = sorted(zip(clf.coef_[class_num], feature_names))\n",
        "  top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
        "  for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
        "      print('\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s' % (coef_1, fn_1, coef_2, fn_2))\n",
        "\n",
        "print('Top 100 features for High recommendation rate')\n",
        "show_most_informative_features(bow, clf_nb, 0)\n",
        "print('Top 100 features for Low recommendation rate')\n",
        "show_most_informative_features(bow, clf_nb, 1)\n",
        "print('Top 100 features for Percfect recommendation rate')\n",
        "show_most_informative_features(bow, clf_nb, 2)\n",
        "####################################################"
      ],
      "metadata": {
        "id": "dd5d-suXHgIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Results \n",
        "\n",
        "> Model: n-gram range (3,4)\n",
        ">\n",
        ">**Confusion Matrix**\n",
        ">\n",
        ">Sklearn's score on testing data : 0.41596638655462187\n",
        ">\n",
        ">Classification report for testing data : \n",
        ">\n",
        ">               precision recall  f1-score   support\n",
        "    High            0.25      0.34      0.29       123\n",
        "    Low             0.17      0.35      0.23        49\n",
        "    Perfect         0.67      0.46      0.55       304\n",
        "    accuracy                            0.42       476\n",
        ">\n",
        "> There is nothing stellar about the efficacy of this model, but I am okay with that and expected to see results like this. The student feedback is very similar regardless of the recommendation rate. However, the students with low recommendation rates are more likely to be customers outside of Study Edge's main market (i.e., students not in greek life) and thus it is very important to address the issues most important to this group if the company wants to grow. \n",
        "> \n",
        ">\n",
        "> **Top 10 features for Low recommendation rate**\n",
        ">\n",
        ">        -8.8117\tmore practice problem\n",
        ">        -8.8117\tend packet question\n",
        ">        -9.0348\tmore study hour\n",
        ">        -9.3225\tworksheet homework question\n",
        ">        -9.3225\ttest phenomenal integration quiz\n",
        ">        -9.3225\tstreaming buggy material integration\n",
        ">        -9.3225\treserve seat review\n",
        ">        -9.3225\tquestion practice problem\n",
        ">        -9.3225\tput another website\n",
        ">        -9.3225\tproblem could look more\n",
        ">\n",
        "> **Top 10 features for High recommendation rate**\n",
        ">\n",
        ">         -7.7661\tmore practice problem\n",
        ">         -8.2769\tmore study hour\n",
        ">         -8.8647\tgive more token\n",
        ">         -9.0878\tweekly exam review\n",
        ">         -9.0878\tstudy hour not \n",
        ">         -9.0878\tshort concept video\n",
        ">         -9.0878\tlive review schedule\n",
        ">         -9.3755\twould nice practice question\n",
        ">         -9.3755\twould nice practice\n",
        ">         -9.3755\tworth 12 token need\n",
        ">\n",
        "> **Top 10 features for Perfect recommendation rate**\n",
        "> \n",
        ">         -7.1764\tmore practice problem\n",
        ">         -7.6583\tmore study hour\n",
        ">         -8.7569\tstudy edge great\n",
        ">         -8.7569\tnot think anything\n",
        ">         -8.9110\twatch video without\n",
        ">         -8.9110\tvideo le token \n",
        ">         -8.9110\ttoken per month\n",
        ">         -8.9110\texam review video\n",
        ">         -8.9110\tend packet problem\n",
        ">         -9.0933\tthink study edge\n",
        ">\n",
        ">\n",
        "> These results show that the top features in the Low recommendation rate set of feedback are largely related to practice problems. I did not expect feedback related to practice problems to be so closley related to a low recommendation rate of the Study Edge product. I originally expected that review schedules and video access/token payment system would be the most important features. While this is not the case for the Low data set, these topics do appear in the top features for the High and Perfect recommendation groups. \n",
        "> "
      ],
      "metadata": {
        "id": "b1e7Gt-BIhWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Conclusion and Discussion\n",
        "\n",
        "> The results of this analysis indicate the largest factor contributing to customers giving a low recommendation rate for Study Edge is dissatisfaction with practice problems. Practice problems are written by tutors, remote tutors, or class assistants. Most practice problems also have video solutions for students to view. If more practice problems need to be created, or a large number of practice problems need to be revised, it is reasonable to expect that additional labor is needed to complete that project. So, I might suggest to Study Edge that they use the additional funding they are receiving to hire additional staff to address this problem. \n",
        ">\n",
        "> Over the course of this project, I have noticed a few issues that I would want to revise before continuing. Firstly, almost half of the survey data I used is from 2013-2015, and I know that Study Edge and its products have changed significantly since then. When I look at the top 100 features for each data set, I can see these irrelevant features being highly weighted. In the future, I would choose to work only with the most recent data, from 2019 onwards. \n",
        "> \n",
        "> Secondly, now that I have spent some time working with this model and identifying top features, I'm not sure that it is the most helpful analysis of the data. The top features are sometimes being pulled from a single comment, and that's not what I intended. I was hoping top features would be more repetitive in the data. I wonder if it would be better to look for larger clusters in the text as a whole, since there was so much overlap in the feedback between groups anyway. \n",
        ">\n",
        ">\n",
        ">\n",
        ">\n"
      ],
      "metadata": {
        "id": "gHRLxsDAIlCr"
      }
    }
  ]
}