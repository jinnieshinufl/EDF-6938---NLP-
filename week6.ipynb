{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "530144c4",
      "metadata": {
        "id": "530144c4"
      },
      "source": [
        "---\n",
        "<h1><center>  lab 6 : Text Feature Engineering </center>\n",
        "    \n",
        "<img src=\"data:image/png;base64,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\" width=\"350\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c7066e",
      "metadata": {
        "id": "c3c7066e"
      },
      "source": [
        "> `Created by: Jinnie Shin (jinnie.shin@ualberta.ca)`\\\n",
        "> `Date: `\n",
        "---\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQmNf86oJnfhpkPA9LnrFnAbfwF2VywPYpB_w&usqp=CAU\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
        "\n",
        " ### Required Packages or Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02eac3c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02eac3c7",
        "outputId": "f87023c6-47b9-4e66-9973-3b15faaab237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting syllables\n",
            "  Downloading syllables-1.0.7-py3-none-any.whl (15 kB)\n",
            "Collecting cmudict<2.0.0,>=1.0.11 (from syllables)\n",
            "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.3/939.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0 (from syllables)\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Collecting importlib-resources<6.0.0,>=5.10.1 (from cmudict<2.0.0,>=1.0.11->syllables)\n",
            "  Downloading importlib_resources-5.13.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.16.2)\n",
            "Installing collected packages: importlib-resources, importlib-metadata, cmudict, syllables\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 6.0.1\n",
            "    Uninstalling importlib-resources-6.0.1:\n",
            "      Successfully uninstalled importlib-resources-6.0.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.8.0\n",
            "    Uninstalling importlib-metadata-6.8.0:\n",
            "      Successfully uninstalled importlib-metadata-6.8.0\n",
            "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 importlib-resources-5.13.0 syllables-1.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install syllables #run this if you run into an error (download syllables)\n",
        "import syllables\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98aabe31",
      "metadata": {
        "id": "98aabe31"
      },
      "source": [
        "\n",
        "## **REVIEW**: Dataset\n",
        "\n",
        "> Using the text_normalizer function we created last time, we will import `essay set 5`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d75c156",
      "metadata": {
        "id": "3d75c156"
      },
      "source": [
        "#### Essay Set 8\n",
        "`Prompt`: We all understand the benefits of laughter. For example, someone once said, \"Laughter is the shortest distance between two people.\" Many other people believe that laughter is an important part of any relationship. Tell a true story in which laughter was one element or part.\n",
        "\n",
        "\n",
        "| Type of response            | Persuasive/Narrative/Expository |\n",
        "|-----------------------------|---------------------------|\n",
        "| Grade level                 | `10`                       |\n",
        "| Total sample size           | `918`                |\n",
        "| Average length of responses | `650 words`                  |\n",
        "| Score range                 | `0-30`                       |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8288c594",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8288c594",
        "outputId": "0caad474-7538-4a82-e873-6c5c82e5d2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Softball has to be one of the single most greatest sports alive; playing softball in college has always been a goal of mine. I love the dirt that sticks to your face, the sweat dripping from your forehead, and the gallons and gallons of water you poor all over yourself to keep cool in the blistering @CAPS2. Although I love softball I feel that the memories you acquire from the times you have with your teammates, are the things you remember the most through out your softball career. I have always had great memories through softball many laughs, tears, and frustrations so when I had the opportunity to play on a top notch team I looked forward to all the many more memories I would have.  Hood @CAPS1 @CAPS2 was my team name, I had played almost four years with this local team. Many of these girls were like sisters to me they had treated me amazingly through out my years playing with them. I felt like I had my set team, I was going to play with these girls all though high school and we would become even closer. As my fifth year now approached I was looking forward to it, but when my dad came up with the idea that maybe I play for a team not as locally, that idea was also very enticing. I now faced a big dilemma either stay with the team I knew so well and continue to play with them until high school ended; or play with a team I knew nothing about, play with girls that might not like me, girls that were so much better then me, and looked down upon me. As I looked at the pros and cons of both teams I decided no; I would stay with the team that I knew so well and I would stay comfortable at where I was at. At that point softball was just around the corner and I began to reanalyze everything. Did I really want to stay and be comfortable? Or maybe challenge myself to become a better player; maybe i could work hard enough, and have college scouts take a look at me and hopefully have me play for them later. I then decided I would take a chance and try for this team.  As my dad and I drove to the field where I would try out for this team, my stomach was in knots. I wanted this team to like me, I was playing all the possible ways in my head I could greet them so i didn't sound snobby or a grump. The time moved quickly and I wished it would slow; my heart felt like it was beating a million times per second. I was so nervous my mouth began to feel sticky and have an odd taste to it, soon after my mouth became bone dry no taste, no flavor, just a sticky mess. The car then was stationary my dad looked at me and just said \"ready or not here we come\". Nervous began to take over my body my arms were shaking uncontrollably and I was second guessing every move I made, I even had troubles removing my seat belt.  As I approached the field it felt like time was in slow motion, walking by I saw all the girls turn there heads to see what new fresh meat had arrived. My first thought was to run; that was it I didn't want to go, I was done I wanted home I was ready to start my year with the @CAPS2, I just couldn't take it. Then as I walked onto the field they all grew a smile. Each and everyone of them told me their names, there favorite color, and a small fun fact about themselves. I found myself laughing through out my whole try out, there were no awkward moments and I enjoyed my time there. I couldn't wait to go back, I now had made new friends, friends I knew I would grow close with. My year with this team was amazing I have friendships with these girls that I never would have discovered if I hadn't gone. Those girls are like family, and the coaches even more so. Laughter played a very big role in the decision I made to play with this team, if I had just gone and tried out, and had I not had fun and laughed, I would have decided to play with the @CAPS2. I have had great memories with the @CAPS2, but also softball is all about going out discovering new friendships; therefore, enjoying the sport as well as the people.\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_excel('training_set_rel3.xlsx')\n",
        "data = data[data.essay_set==8]\n",
        "print(data.iloc[1].essay)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fb145d",
      "metadata": {
        "id": "66fb145d"
      },
      "source": [
        "\n",
        "## 1. Extracting Descriptive Indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d8f0e0",
      "metadata": {
        "id": "70d8f0e0"
      },
      "outputs": [],
      "source": [
        "def descriptives(text):\n",
        "    ## the total number of sentences\n",
        "\n",
        "    ############################ MINI TASK ###################################\n",
        "\n",
        "    #1. Total number of sentences\n",
        "    #sentences = ```Tokenize the text into sentences, text is a single essay``\n",
        "\n",
        "\n",
        "    ##########################################################################\n",
        "    n_sentences = len(sentences)\n",
        "\n",
        "      ## the total number of words (tokens)\n",
        "    ############################ MINI TASK ###################################\n",
        "\n",
        "    #2. Total number of words\n",
        "    #words = ```Tokenize the text into tokens (words), text is a single essay``\n",
        "\n",
        "\n",
        "    ##########################################################################\n",
        "    n_words = len(words)\n",
        "\n",
        "      ## the total number of unique vocabulary\n",
        "    unique_words = set(words)\n",
        "    n_unique_words = len(unique_words)\n",
        "\n",
        "      ## the total number of stop words\n",
        "    import nltk\n",
        "    stop_words = list(set(nltk.corpus.stopwords.words('english')))\n",
        "    n_stopwords = len([i for i in words if i in stop_words])\n",
        "\n",
        "      ## the total number of punctuation marks\n",
        "    n_punc = len([i for i in words if i in \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"])\n",
        "\n",
        "      ## total number of syllables\n",
        "    n_syllables = np.sum([syllables.estimate(i) for i in words])\n",
        "\n",
        "    ############################ MINI TASK ###################################\n",
        "\n",
        "    #1. Total number of unique lemmas\n",
        "\n",
        "    #n_lemmas=\n",
        "    #2. Total number of unique stems\n",
        "\n",
        "\n",
        "    #n_stems=\n",
        "    ##########################################################################\n",
        "\n",
        "    return n_sentences, n_words, n_unique_words, n_stopwords, n_punc, n_syllables #, n_lemmas, n_stems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430d1c2f",
      "metadata": {
        "id": "430d1c2f"
      },
      "source": [
        "> *Let's explore!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d9a80b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "96d9a80b",
        "outputId": "9a73650a-3864-4953-b4ec-1aac9bdd296c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f155364840f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0messay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdescriptives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "text = data.iloc[0].essay\n",
        "\n",
        "descriptives(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a131acb",
      "metadata": {
        "id": "4a131acb"
      },
      "source": [
        "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
        "\n",
        "> ## Task 1: Descriptive Indices\n",
        "\n",
        "> Q1. Let's apply the `descriptives` function to the entire dataframe (`data.essay`) and save them into a new column (`features`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c2cbdd",
      "metadata": {
        "id": "79c2cbdd"
      },
      "outputs": [],
      "source": [
        "################# YOUR CODE HERE ############\n",
        "data['features'] = data.essay.apply(descriptives)\n",
        "data.features\n",
        "############################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ca0d4a",
      "metadata": {
        "id": "a1ca0d4a"
      },
      "source": [
        "## 2. Readability Indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848f8b91",
      "metadata": {
        "id": "848f8b91"
      },
      "outputs": [],
      "source": [
        "class readability():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_sent, self.n_word, self.n_unique, self.n_stop, self.n_punc, self.n_syll, _, _ = descriptives(text)\n",
        "\n",
        "    def fr_score(self, text):\n",
        "        score = 206.835-1.015*(self.n_word/self.n_sent) - 84.6*(self.n_syll/self.n_word)\n",
        "        return score\n",
        "\n",
        "    def gf_score(self, text):\n",
        "        score = 0.4*((self.n_word/self.n_sent)+ 100*(self.n_word))\n",
        "        return score\n",
        "\n",
        "    def smog(self, text):\n",
        "        if self.n_sent < 30:\n",
        "            score = 1.0430*np.sqrt(self.n_syll*(30/self.n_sent))+3.1291\n",
        "            return score\n",
        "        else:\n",
        "            return print('total number of sentences are less than 30')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e9f770",
      "metadata": {
        "id": "d1e9f770",
        "outputId": "c2be4601-a0b9-4f8b-94d0-a8b230edc710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "86.72442806694306\n",
            "33408.5641025641\n",
            "total number of sentences are less than 30\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "analyzer = readability()\n",
        "print(analyzer.fr_score(text))\n",
        "print(analyzer.gf_score(text))\n",
        "print(analyzer.smog(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776af2b1",
      "metadata": {
        "id": "776af2b1"
      },
      "source": [
        "## 4. LFTK: Handcrafted Features in Computational Linguistics\n",
        "> More information about LFTK is available at: https://github.com/brucewlee/lftk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bd8fc663",
      "metadata": {
        "id": "bd8fc663",
        "outputId": "16ab93de-a795-4e62-cad5-db39628a2c57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lftk\n",
            "  Downloading lftk-1.0.9.tar.gz (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lftk) (1.5.3)\n",
            "Collecting ndjson (from lftk)\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from lftk) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lftk) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lftk) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->lftk) (1.23.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lftk) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->lftk) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->lftk) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->lftk) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->lftk) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->lftk) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->lftk) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->lftk) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->lftk) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy->lftk) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->lftk) (2.1.3)\n",
            "Building wheels for collected packages: lftk\n",
            "  Building wheel for lftk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lftk: filename=lftk-1.0.9-py3-none-any.whl size=2359765 sha256=05a40550024f73a6c997d6c728ae7433c233151a8cc7c4905001875cd368f56d\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/19/8a82ad750c6321373b8557b6cf4f35241ac04a9aefd8a489a7\n",
            "Successfully built lftk\n",
            "Installing collected packages: ndjson, lftk\n",
            "Successfully installed lftk-1.0.9 ndjson-0.3.1\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-10-03 12:30:18.022329: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-03 12:30:19.129435: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install lftk\n",
        "\n",
        "!pip install spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Script (from LFTK)"
      ],
      "metadata": {
        "id": "rf42CYGPhJkB"
      },
      "id": "rf42CYGPhJkB"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import lftk\n",
        "\n",
        "# load a trained pipeline of your choice from spacy\n",
        "# remember we already downloaed \"en_core_web_sm\" pipeline above?\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# create a spaCy doc object\n",
        "doc = nlp(\"I love research but my professor is strange.\")\n",
        "\n",
        "# initiate LFTK extractor by passing in doc\n",
        "# you can pass in a list of multiple docs\n",
        "LFTK = lftk.Extractor(docs = doc)\n",
        "\n",
        "# optionally, you can customize how LFTK extractor calculates handcrafted linguistic features\n",
        "# for example, include stop word? include puncutaion? maximum decimal digits?\n",
        "LFTK.customize(stop_words=True, punctuations=False, round_decimal=3)\n",
        "\n",
        "# now, extract the handcrafted linguistic features that you need\n",
        "# refer to them as feature keys\n",
        "extracted_features = LFTK.extract(features = [\"a_word_ps\", \"a_kup_pw\", \"n_noun\"])\n",
        "\n",
        "# {'a_word_ps': 8.0, 'a_kup_pw': 5.754, 'n_noun': 2}\n",
        "print(extracted_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrijMp4Hg41i",
        "outputId": "258530fb-7a27-473d-fae3-f5ac4d9b54e0"
      },
      "id": "YrijMp4Hg41i",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a_word_ps': 8.0, 'a_kup_pw': 5.754, 'n_noun': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's explore"
      ],
      "metadata": {
        "id": "4BDC1dRFhUpq"
      },
      "id": "4BDC1dRFhUpq"
    },
    {
      "cell_type": "code",
      "source": [
        "############################ MINI TASK ###################################\n",
        "\n",
        "#1. Randomly sample 10 response from out dataset (only the raw responses)\n",
        "\n",
        "#2. Apply the LTFK functions to extract word difficulty related features\n",
        "lftk.search_features(family = \"worddiff\")\n",
        "\n",
        "#3. [ADVANCED] Identify the features that are most highly associated with the essay score\n",
        "\n",
        "\n",
        "##########################################################################\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gze_gQJ0hNUF",
        "outputId": "74850aeb-ec9d-42a4-96aa-b92909b7d6e6"
      },
      "id": "gze_gQJ0hNUF",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'key': 't_kup',\n",
              "  'name': 'total_kuperman_age_of_acquistion_of_words',\n",
              "  'formulation': 'foundation',\n",
              "  'domain': 'lexico-semantics',\n",
              "  'family': 'worddiff',\n",
              "  'language': 'en'},\n",
              " {'key': 't_bry',\n",
              "  'name': 'total_brysbaert_age_of_acquistion_of_words',\n",
              "  'formulation': 'foundation',\n",
              "  'domain': 'lexico-semantics',\n",
              "  'family': 'worddiff',\n",
              "  'language': 'en'},\n",
              " {'key': 't_subtlex_us_zipf',\n",
              "  'name': 'total_subtlex_us_zipf_of_words',\n",
              "  'formulation': 'foundation',\n",
              "  'domain': 'lexico-semantics',\n",
              "  'family': 'worddiff',\n",
              "  'language': 'en'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ixp1kmvrh3Od"
      },
      "id": "Ixp1kmvrh3Od",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}