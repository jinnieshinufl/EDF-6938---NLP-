{"cells":[{"cell_type":"markdown","id":"530144c4","metadata":{"id":"530144c4"},"source":["<h1><center>  lab 8 : ML Overview: Supervised Learning algorithms </center>\n","    \n","<img src=\"https://files.realpython.com/media/NLP-for-Beginners-Pythons-Natural-Language-Toolkit-NLTK_Watermarked.16a787c1e9c6.jpg\" width=\"400\">\n"]},{"cell_type":"markdown","id":"c3c7066e","metadata":{"id":"c3c7066e"},"source":["```Created by Jinnie Shin (jinnie.shin@coe.ufl.edu)```\\\n","```Date: ```\n","\n","<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQmNf86oJnfhpkPA9LnrFnAbfwF2VywPYpB_w&usqp=CAU\" align=\"left\" width=\"70\" height=\"70\" align=\"left\"> \n","\n"," ### Required Packages or Dependencies"]},{"cell_type":"code","execution_count":null,"id":"02eac3c7","metadata":{"id":"02eac3c7"},"outputs":[],"source":["#!pip install { } ! in case you run into the `package not avaialble` error\n","import pandas as pd \n","import numpy as np \n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"98aabe31","metadata":{"id":"98aabe31"},"source":["\n","## **REVIEW**: Dataset\n","\n","> We will use the coh-metrix indices introduced in Week 6, `features.xlsx`"]},{"cell_type":"code","execution_count":null,"id":"c324cee8","metadata":{"id":"c324cee8"},"outputs":[],"source":["data= pd.read_excel('./data/features.xlsx')\n","\n","############################### MINI TASKS ####################################\n","# Q1. The total number of rows?\n","\n","# Q2. How many coh-metrix features? \n","# (excluding, `TextID`, `domain1_score`, `domain2_score`, `essay_id`, and `essay_set`)\n","\n","###############################################################################\n","\n","X = data.drop(columns=['TextID','domain1_score', 'domain2_score', 'essay_id', 'essay_set'])\n","y = data.domain1_score\n"]},{"cell_type":"markdown","id":"ee132932","metadata":{"id":"ee132932"},"source":["## 1. Regression and Classification Problems\n","\n","> Our task is to predict the `domain1_score` column using the given coh-metrix features. \n","> We will implement and use the two algorithms, linear and logistic regression, as our main prediction/classification models. Before we construct the algorithms next week, we will take a look at how the model weights are learned using **the gradient descent algorithms**. \n","\n","### 1.1 Gradient Descent \n","<img src=\"https://miro.medium.com/proxy/1*fBxEzbzP1KkqR7PTexJZdw.png\" width=\"250\">\n","\n","> The objective of the learning algorithm is to determine the best possible values for the parameters (`w` and `b`), such that the overall loss (squared error loss) of the model is minimized as much as possible. \\\n","> Let's solve this regression problem: `y = 4.0+(3.0洧논0)+(1.0洧논1)+(3.0洧논2)+(0.5洧논3)+(1.5洧논4)`"]},{"cell_type":"code","execution_count":null,"id":"2e5ad077","metadata":{"id":"2e5ad077"},"outputs":[],"source":["x0 = 3.0 + np.random.standard_normal(num_samples)\n","x1 = 1.0 + np.random.standard_normal(num_samples)\n","x2 = -8.0 + np.random.standard_normal(num_samples)\n","x3 = -2.0 + np.random.standard_normal(num_samples)\n","x4 = 0.5 + np.random.standard_normal(num_samples)\n","y = 4.0 + 3.0 * x0 + 1.0 * x1 + 3.0 * x2 + 0.5 * x3 + 1.5 * x4 + np.random.standard_normal(num_samples)\n","\n","X = np.column_stack((x0, x1, x2, x3, x4))\n","Y = y "]},{"cell_type":"markdown","id":"938556da","metadata":{"id":"938556da"},"source":["#### 1.1.1 Batch Gradient Descent (BGD)\n","> Partial derivates of `b` and `w` in linear regression with the squared loss is: \n","<img src=\"https://eli.thegreenplace.net/images/math/aef02f077919896478d0456619f934dcc5809142.png\" width=\"250\">\n"]},{"cell_type":"code","execution_count":null,"id":"3453809b","metadata":{"id":"3453809b"},"outputs":[],"source":["def BGD(X, Y, b, w, alpha=0.005): # alpha is a learning rate, we will set it as 0.005 for now\n","   \n","    num_feat = X.shape[1]\n","    \n","    num_sample = X.shape[0] # This indicates the total number of data points (rows)\n","\n","    b_grad = 0 #Intercept \n","    \n","    w_grad = np.zeros(num_feat) # weight vector \n","    \n","    for i in range(num_sample): # BGD first calculates the `b_grad` or `w_grad` \n","                                # from the total sample N\n","        y = Y[i] # one sample, y\n","        x = X[i] # one sample, x \n","        b_grad += -(2./float(num_sample)) * (y - (b + w.dot(x)))\n","\n","        for j in range(num_feat):\n","            x_ij = x[j]\n","            w_grad[j] += -(2./float(num_sample)) * x_ij * (y - (b + w.dot(x)))\n","\n","    b_new = b - alpha * b_grad\n","    w_new = np.array([w[i] - alpha * w_grad[i] for i in range(num_feat)])\n","    return b_new, w_new"]},{"cell_type":"code","execution_count":null,"id":"dccd0db9","metadata":{"id":"dccd0db9"},"outputs":[],"source":["def BGD_train(X, Y, alpha=0.005):\n","    b = 0\n","    w = np.zeros(X.shape[1])\n","    print('===== Start Training ====')\n","    for i in range(10000):\n","        b_new, w_new = BGD(X, Y, b, w, alpha=alpha)\n","        b = b_new\n","        w = w_new\n","        if i % 1000 == 0:\n","            print('{}: b = {}, w = {}'.format(i, np.round(b_new, 2), np.round(w_new, 2)))\n","\n","    print('final: b = {}, w = {}'.format(np.round(b, 2), np.round(w, 2)))\n","    return b, w"]},{"cell_type":"markdown","id":"93d82dc5","metadata":{"id":"93d82dc5"},"source":["> *Let's explore!*"]},{"cell_type":"code","execution_count":null,"id":"29b5e930","metadata":{"id":"29b5e930","outputId":"2d3b22b2-74c3-4403-8734-0e02a164ff30"},"outputs":[{"name":"stdout","output_type":"stream","text":["===== Start Training ====\n","0: b = -0.1, w = [-0.27 -0.11  0.87  0.22 -0.04]\n","1000: b = 0.29, w = [3.21 1.12 2.67 0.33 1.5 ]\n","2000: b = 0.69, w = [3.19 1.11 2.71 0.34 1.5 ]\n","3000: b = 1.05, w = [3.18 1.1  2.75 0.36 1.51]\n","4000: b = 1.37, w = [3.17 1.09 2.78 0.37 1.51]\n","5000: b = 1.67, w = [3.16 1.08 2.81 0.38 1.52]\n","6000: b = 1.93, w = [3.15 1.08 2.84 0.39 1.52]\n","7000: b = 2.18, w = [3.14 1.07 2.86 0.4  1.52]\n","8000: b = 2.39, w = [3.14 1.06 2.88 0.4  1.52]\n","9000: b = 2.59, w = [3.13 1.06 2.9  0.41 1.53]\n","final: b = 2.77, w = [3.13 1.05 2.92 0.42 1.53]\n"]},{"data":{"text/plain":["(2.766278150852741,\n"," array([3.12549199, 1.05189554, 2.91675839, 0.41842552, 1.52997421]))"]},"execution_count":390,"metadata":{},"output_type":"execute_result"}],"source":["BGD_train(X, Y)"]},{"cell_type":"markdown","id":"80fb9bc0","metadata":{"id":"80fb9bc0"},"source":["#### 1.1.1 Stochastic Gradient Descent (SGD)\n","> Shuffles the data and randomly sample one data point to update the gradient"]},{"cell_type":"code","execution_count":null,"id":"55c260d1","metadata":{"id":"55c260d1"},"outputs":[],"source":["def SGD(x, y, b, w, num_feat, num_sample, alpha=0.005):\n","    \n","    b_grad = -(2./float(num_sample)) * (y - (b + w.dot(x)))\n","    w_grad = np.zeros(num_feat)\n","    \n","    for i in range(num_feat):\n","        w_grad[i] += -(2./float(num_sample)) * x[i] * (y - (b + w.dot(x)))\n","\n","    b_new = b - alpha * b_grad\n","    w_new = np.array([w[i] - alpha * w_grad[i] for i in range(num_feat)])\n","    return b_new, w_new"]},{"cell_type":"code","execution_count":null,"id":"8fae0cd2","metadata":{"id":"8fae0cd2"},"outputs":[],"source":["def SGD_train(X, Y, alpha =0.005):\n","    \n","    import random \n","\n","    b = 0\n","    w = np.zeros(X.shape[1])\n","\n","    num_sample = X.shape[0] \n","    num_feat = X.shape[1]\n","\n","    for i in range(5000):\n","        indices = list(range(num_sample))\n","        random.shuffle(indices)\n","\n","    for j in indices:\n","        b_new, w_new = SGD(X[j], Y[j], b, w, num_feat, num_sample,  alpha=alpha)\n","        b = b_new\n","        w = w_new\n","\n","    if i % 1000 == 0:\n","        print('{}: b = {}, w = {}'.format(i, np.round(b_new, 2), np.round(w_new, 2)))\n","\n","    print('final: b = {}, w = {}'.format(np.round(b,2), np.round(w, 2)))\n","    "]},{"cell_type":"markdown","id":"00c905de","metadata":{"id":"00c905de"},"source":["> *Let's explore!*"]},{"cell_type":"code","execution_count":null,"id":"c3d839f9","metadata":{"id":"c3d839f9"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"da6db121","metadata":{"id":"da6db121"},"source":["<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n","\n"," ## Task 1: Training & Testing data\n",">  Q1. In order to analyze large dataset efficiently, we will use the package `scikit-learn` to implement regression models. \n",">> **Step 1**: Download the package `!pip install sklearn` \\\n",">> **Step 2**: Import models ` from sklearn.linear_model import LinearRegression`\\\n",">> **Step 3**: Call the module `lr = LinearRegression()` \\\n",">> **Step 4**: Fit the dataset using `lr.fit({input}, {output})` and check the intercept and the coefficients using `lr.intercept_` and `lr.coef_`\n","\n","> More information about the package is available at: https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n","\n","> Q2. Compare the results with our findings. "]},{"cell_type":"code","execution_count":1,"id":"b7f0c1da","metadata":{"id":"b7f0c1da","executionInfo":{"status":"ok","timestamp":1667308595310,"user_tz":240,"elapsed":1073,"user":{"displayName":"Jinnie Shin","userId":"13700560240711162462"}}},"outputs":[],"source":["################################### YOUR CODE HERE #############################\n","from sklearn.linear_model import LinearRegression\n","\n","\n","\n","###############################################################################"]},{"cell_type":"markdown","source":["<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n","\n"," ## Task 2: Training & Testing data using `Linear Regression`\n",">  Q3. Let's use the `data` and fit a `linear regression` model (DV = `domain1_score`. \n","\n","> Q4. Evaluate the R2-score from `from sklearn.metrics import r2_score`"],"metadata":{"id":"_5_5AQI_VjV-"},"id":"_5_5AQI_VjV-"},{"cell_type":"code","source":["data= pd.read_excel('./data/features.xlsx')\n","X = data.drop(columns=['TextID','domain1_score', 'domain2_score', 'essay_id', 'essay_set'])\n","y = data.domain1_score\n","\n","################################### YOUR CODE HERE #############################\n","from sklearn.linear_model import LinearRegression\n","\n","\n","\n","###############################################################################"],"metadata":{"id":"PQAsFEEUVjnu"},"id":"PQAsFEEUVjnu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n","\n"," ## Task 3: Training & Testing data with `Logistic Regression`\n",">  Q3. Let's use the `data` and fit a `logistic regression` model (DV = `domain1_score`. (Hint: You should create a ____ output). Use `from sklearn.linear_model import LogisticRegression` \n","\n","> Q4. Evaluate the accuracy from `from sklearn.metrics import accuracy_score`"],"metadata":{"id":"NHd5dD96WA0o"},"id":"NHd5dD96WA0o"},{"cell_type":"code","source":["data= pd.read_excel('./data/features.xlsx')\n","X = data.drop(columns=['TextID','domain1_score', 'domain2_score', 'essay_id', 'essay_set'])\n","y = data.domain1_score\n","\n","################################### YOUR CODE HERE #############################\n","from sklearn.linear_model import LogisticRegression \n","\n","\n","\n","###############################################################################"],"metadata":{"id":"qRelzBd1WX_M"},"id":"qRelzBd1WX_M","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}