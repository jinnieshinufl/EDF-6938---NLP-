{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530144c4",
   "metadata": {},
   "source": [
    "<h1><center>  lab 8 : ML Overview: Supervised Learning algorithms </center>\n",
    "    \n",
    "<img src=\"https://files.realpython.com/media/NLP-for-Beginners-Pythons-Natural-Language-Toolkit-NLTK_Watermarked.16a787c1e9c6.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7066e",
   "metadata": {},
   "source": [
    "```Created by Jinnie Shin (jinnie.shin@coe.ufl.edu)```\\\n",
    "```Date: ```\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQmNf86oJnfhpkPA9LnrFnAbfwF2VywPYpB_w&usqp=CAU\" align=\"left\" width=\"70\" height=\"70\" align=\"left\"> \n",
    "\n",
    " ### Required Packages or Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "02eac3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install { } ! in case you run into the `package not avaialble` error\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aabe31",
   "metadata": {},
   "source": [
    "\n",
    "## **REVIEW**: Dataset\n",
    "\n",
    "> We will use the coh-metrix indices introduced in Week 6, `features.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c324cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_excel('./data/features.xlsx')\n",
    "\n",
    "############################### MINI TASKS ####################################\n",
    "# Q1. The total number of rows?\n",
    "\n",
    "# Q2. How many coh-metrix features? \n",
    "# (excluding, `TextID`, `domain1_score`, `domain2_score`, `essay_id`, and `essay_set`)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "X = data.drop(columns=['TextID','domain1_score', 'domain2_score', 'essay_id', 'essay_set'])\n",
    "y = data.domain1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee132932",
   "metadata": {},
   "source": [
    "## 1. Regression and Classification Problems\n",
    "\n",
    "> Our task is to predict the `domain1_score` column using the given coh-metrix features. \n",
    "> We will implement and use the two algorithms, linear and logistic regression, as our main prediction/classification models. Before we construct the algorithms next week, we will take a look at how the model weights are learned using **the gradient descent algorithms**. \n",
    "\n",
    "### 1.1 Gradient Descent \n",
    "<img src=\"https://miro.medium.com/proxy/1*fBxEzbzP1KkqR7PTexJZdw.png\" width=\"250\">\n",
    "\n",
    "> The objective of the learning algorithm is to determine the best possible values for the parameters (`w` and `b`), such that the overall loss (squared error loss) of the model is minimized as much as possible. \\\n",
    "> Let's solve this regression problem: `y = 4.0+(3.0洧논0)+(1.0洧논1)+(3.0洧논2)+(0.5洧논3)+(1.5洧논4)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "2e5ad077",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 3.0 + np.random.standard_normal(num_samples)\n",
    "x1 = 1.0 + np.random.standard_normal(num_samples)\n",
    "x2 = -8.0 + np.random.standard_normal(num_samples)\n",
    "x3 = -2.0 + np.random.standard_normal(num_samples)\n",
    "x4 = 0.5 + np.random.standard_normal(num_samples)\n",
    "y = 4.0 + 3.0 * x0 + 1.0 * x1 + 3.0 * x2 + 0.5 * x3 + 1.5 * x4 + np.random.standard_normal(num_samples)\n",
    "\n",
    "X = np.column_stack((x0, x1, x2, x3, x4))\n",
    "Y = y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938556da",
   "metadata": {},
   "source": [
    "#### 1.1.1 Batch Gradient Descent (BGD)\n",
    "> Partial derivates of `b` and `w` in linear regression with the squared loss is: \n",
    "<img src=\"https://eli.thegreenplace.net/images/math/aef02f077919896478d0456619f934dcc5809142.png\" width=\"250\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "3453809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD(X, Y, b, w, alpha=0.005): # alpha is a learning rate, we will set it as 0.005 for now\n",
    "   \n",
    "    num_feat = X.shape[1]\n",
    "    \n",
    "    num_sample = X.shape[0] # This indicates the total number of data points (rows)\n",
    "\n",
    "    b_grad = 0 #Intercept \n",
    "    \n",
    "    w_grad = np.zeros(num_feat) # weight vector \n",
    "    \n",
    "    for i in range(num_sample): # BGD first calculates the `b_grad` or `w_grad` \n",
    "                                # from the total sample N\n",
    "        y = Y[i] # one sample, y\n",
    "        x = X[i] # one sample, x \n",
    "        b_grad += -(2./float(num_sample)) * (y - (b + w.dot(x)))\n",
    "\n",
    "        for j in range(num_feat):\n",
    "            x_ij = x[j]\n",
    "            w_grad[j] += -(2./float(num_sample)) * x_ij * (y - (b + w.dot(x)))\n",
    "\n",
    "    b_new = b - alpha * b_grad\n",
    "    w_new = np.array([w[i] - alpha * w_grad[i] for i in range(num_feat)])\n",
    "    return b_new, w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "dccd0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD_train(X, Y, alpha=0.005):\n",
    "    b = 0\n",
    "    w = np.zeros(X.shape[1])\n",
    "    print('===== Start Training ====')\n",
    "    for i in range(10000):\n",
    "        b_new, w_new = BGD(X, Y, b, w, alpha=alpha)\n",
    "        b = b_new\n",
    "        w = w_new\n",
    "        if i % 1000 == 0:\n",
    "            print('{}: b = {}, w = {}'.format(i, np.round(b_new, 2), np.round(w_new, 2)))\n",
    "\n",
    "    print('final: b = {}, w = {}'.format(np.round(b, 2), np.round(w, 2)))\n",
    "    return b, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d82dc5",
   "metadata": {},
   "source": [
    "> *Let's explore!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "29b5e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Start Training ====\n",
      "0: b = -0.1, w = [-0.27 -0.11  0.87  0.22 -0.04]\n",
      "1000: b = 0.29, w = [3.21 1.12 2.67 0.33 1.5 ]\n",
      "2000: b = 0.69, w = [3.19 1.11 2.71 0.34 1.5 ]\n",
      "3000: b = 1.05, w = [3.18 1.1  2.75 0.36 1.51]\n",
      "4000: b = 1.37, w = [3.17 1.09 2.78 0.37 1.51]\n",
      "5000: b = 1.67, w = [3.16 1.08 2.81 0.38 1.52]\n",
      "6000: b = 1.93, w = [3.15 1.08 2.84 0.39 1.52]\n",
      "7000: b = 2.18, w = [3.14 1.07 2.86 0.4  1.52]\n",
      "8000: b = 2.39, w = [3.14 1.06 2.88 0.4  1.52]\n",
      "9000: b = 2.59, w = [3.13 1.06 2.9  0.41 1.53]\n",
      "final: b = 2.77, w = [3.13 1.05 2.92 0.42 1.53]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.766278150852741,\n",
       " array([3.12549199, 1.05189554, 2.91675839, 0.41842552, 1.52997421]))"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BGD_train(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb9bc0",
   "metadata": {},
   "source": [
    "#### 1.1.1 Stochastic Gradient Descent (SGD)\n",
    "> Shuffles the data and randomly sample one data point to update the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "55c260d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(x, y, b, w, num_feat, num_sample, alpha=0.005):\n",
    "    \n",
    "    b_grad = -(2./float(num_sample)) * (y - (b + w.dot(x)))\n",
    "    w_grad = np.zeros(num_feat)\n",
    "    \n",
    "    for i in range(num_feat):\n",
    "        w_grad[i] += -(2./float(num_sample)) * x[i] * (y - (b + w.dot(x)))\n",
    "\n",
    "    b_new = b - alpha * b_grad\n",
    "    w_new = np.array([w[i] - alpha * w_grad[i] for i in range(num_feat)])\n",
    "    return b_new, w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "8fae0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_train(X, Y, alpha =0.005):\n",
    "    \n",
    "    import random \n",
    "\n",
    "    b = 0\n",
    "    w = np.zeros(X.shape[1])\n",
    "\n",
    "    num_sample = X.shape[0] \n",
    "    num_feat = X.shape[1]\n",
    "\n",
    "    for i in range(5000):\n",
    "        indices = list(range(num_sample))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "    for j in indices:\n",
    "        b_new, w_new = SGD(X[j], Y[j], b, w, num_feat, num_sample,  alpha=alpha)\n",
    "        b = b_new\n",
    "        w = w_new\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print('{}: b = {}, w = {}'.format(i, np.round(b_new, 2), np.round(w_new, 2)))\n",
    "\n",
    "    print('final: b = {}, w = {}'.format(np.round(b,2), np.round(w, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c905de",
   "metadata": {},
   "source": [
    "> *Let's explore!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d839f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da6db121",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
    "\n",
    " ## Task 1: Training & Testing data\n",
    ">  Q1. In order to analyze large dataset efficiently, we will use the package `scikit-learn` to implement regression models. \n",
    ">> **Step 1**: Download the package `!pip install sklearn` \\\n",
    ">> **Step 2**: Import models ` from sklearn.linear_model import LinearRegression`\\\n",
    ">> **Step 3**: Call the module `lr = LinearRegression()` \\\n",
    ">> **Step 4**: Fit the dataset using `lr.fit({input}, {output})` and check the intercept and the coefficients using `lr.intercept_` and `lr.coef_`\n",
    "\n",
    "> More information about the package is available at: https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n",
    "\n",
    "> Q2. Compare the results with our findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "b7f0c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.434067716690402\n",
      "[3.07146115 1.00311739 3.08024028 0.47881489 1.55235906]\n"
     ]
    }
   ],
   "source": [
    "################################### YOUR CODE HERE #############################\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print(lr.intercept_)\n",
    "print(lr.coef_)\n",
    "\n",
    "###############################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
